[{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Janet M Thornton","Title":"50 Years of Protein Structures & Structural Bioinformatics","Abstract":"The last 50 years have seen a revolution in our understanding of proteins and how they work in 3D. This has been enabled by the development of many new technologies in producing proteins, crystallisation with robots, the synchrotrons to collect very high resolution data, structure determination by NMR, the more recent developments in Cryo-electron microscopy and tomography. These experimental developments have been matched by the development of sophisticated computational tools and databases using powerful computers, to help in determining structures and also in curating, analysing, comparing and predicting their structures._x000D_\n_x000D_\n In this talk I will focus on our collective progress in understanding more about these molecules of life, from the handful or structures determined in 1974 to our current knowledge of the complex world of proteins. I will conclude by describing some of our own recent work on exploring enzyme catalysis._x000D_\n_x000D_\n I will highlight:_x000D_\nÂ·      Our current knowledge of the universe of protein structures_x000D_\nÂ·      The development of tools for annotating structures_x000D_\nÂ·      wwPDB & PDBe; EMDB & EMPIAR, AFDB_x000D_\nÂ·      Protein structure prediction & AI_x000D_\nÂ·      Computational Enzymology_x000D_\nÂ·      The impact & the future?"},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Samuel Sledzieski","Title":"Democratizing Protein Language Models with Parameter-Efficient Fine-Tuning","Abstract":"Proteomics has been revolutionized by large protein language models (PLMs), which learn unsupervised representations from large corpora of sequences. These models are typically fine-tuned in a supervised setting to adapt the model to specific downstream tasks. However, the computational and memory footprint of fine-tuning large PLMs presents a barrier for many research groups with limited computational resources. Natural language processing has seen a similar explosion in the size of models, where these challenges have been addressed by methods for parameter-efficient fine-tuning (PEFT). In this work, we introduce this paradigm to proteomics through leveraging the parameter-efficient method LoRA and training new models for two important tasks: predicting protein-protein interactions (PPIs) and predicting the symmetry of homooligomer quaternary structures. We show that these approaches are competitive with traditional fine-tuning while requiring reduced memory and substantially fewer parameters. We additionally show that for the PPI prediction task, training only the classification head also remains competitive with full fine-tuning, using five orders of magnitude fewer parameters, and that each of these methods outperform state-of-the-art PPI prediction methods with substantially reduced compute. We further perform a comprehensive evaluation of the hyperparameter space, demonstrate that PEFT of PLMs is robust to variations in these hyperparameters, and elucidate where best practices for PEFT in proteomics differ from those in natural language processing. All our model adaptation and evaluation code is available open-source at https:\/\/github.com\/microsoft\/peft_proteomics. Thus, we provide a blueprint to democratize the power of protein language model adaptation to groups with limited computational resources."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Debswapna Bhattacharya","Title":"EquiPNAS: improved proteinâ€“nucleic acid binding site prediction using protein-language-model-informed equivariant deep graph neural networks","Abstract":"Protein language models (pLMs) trained on a large corpus of protein sequences have shown unprecedented scalability and broad generalizability in a wide range of predictive modeling tasks, but their power has not yet been harnessed for predicting proteinâ€“nucleic acid binding sites, critical for characterizing the interactions between proteins and nucleic acids. Here, we present EquiPNAS, a new pLM-informed E(3) equivariant deep graph neural network framework for improved proteinâ€“nucleic acid binding site prediction. By combining the strengths of pLM and symmetry-aware deep graph learning, EquiPNAS consistently outperforms the state-of-the-art methods for both proteinâ€“DNA and proteinâ€“RNA binding site prediction on multiple datasets across a diverse set of predictive modeling scenarios ranging from using experimental input to AlphaFold2 predictions. Our ablation study reveals that the pLM embeddings used in EquiPNAS are sufficiently powerful to dramatically reduce the dependence on the availability of evolutionary information without compromising on accuracy, and that the symmetry-aware nature of the E(3) equivariant graph-based neural architecture offers remarkable robustness and performance resilience. EquiPNAS is freely available at https:\/\/github.com\/Bhattacharya-Lab\/EquiPNAS."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"Live Stream","Speaker":"Shuo Zhang","Title":"Accurate High-throughput Cryptic Binding Site Prediction Using Protein Language Model","Abstract":"Identification of cryptic binding sites of proteins is an important but challenging task for understanding the function of proteins and screening potential drugs for proteins currently considered undruggable. Existing methods usually require 3D protein structures from resource-intensive molecular dynamics (MD) simulations or are too slow to be adopted in high-throughput compound screening. To tackle these limitations, we propose LaMPSite, which only takes protein sequences and ligand molecular graphs as input for cryptic binding site predictions. Without any 3D coordinate information of proteins, our proposed model is not only 100 to 1000 times faster than baseline methods that require 3D protein structures from time-consuming MD simulations or generative binding complex structures but also more accurate than them. Given the efficiency and accuracy of LaMPSite, it is promising to be applied to drug discovery."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Rohit Singh","Title":"Contrastive learning in protein language space predicts interactions between drugs and protein targets","Abstract":"Experimental screening of potential drug molecules against protein targets is a key bottleneck in the drug discovery pipeline. Fast and accurate computational prediction of drug-target interactions (DTIs) could significantly accelerate this process. However, current sequence-based DTI prediction methods struggle to achieve broad generalization and high specificity while remaining computationally efficient. We develop ConPLex, a deep learning model that successfully leverages the advances in pretrained protein language models (\"\"PLex\"\") and employs a protein-anchored contrastive coembedding (\"\"Con\"\") to outperform state-of-the-art approaches. ConPLex makes predictions of binding based on the distance between learned representations, achieving high accuracy, broad adaptivity to unseen data, and specificity against decoy compounds. Experimental validation yielded a 63% hit rate, including four hits with subnanomolar affinity and a novel strongly-binding EPHB1 inhibitor (KD = 1.3 nM). ConPLex is extremely fast, capable of making 100 million predictions per day on a single GPU, enabling predictions at the scale of massive compound libraries and the human proteome. The contrastive approach and the shared embedding space also provide interpretability, allowing visualization of drug-target relationships and functional characterization of cell-surface proteins. ConPLex has the potential to efficiently guide and prioritize candidates for experimental screening, unlocking significant value in the drug discovery process._x000D_\n_x000D_\nAvailability: https:\/\/conplex.csail.mit.edu\/_x000D_\n_x000D_\nSource code: https:\/\/github.com\/samsledje\/ConPLex_x000D_\n_x000D_\nPaper:  Singh, Sledzieski, Bryson, Cowen, & Berger. PNAS, 120(24) (2023)._x000D_\nhttps:\/\/www.pnas.org\/doi\/full\/10.1073\/pnas.2220778120"},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Thomas Descoteaux","Title":"NRGDock: An open-source software for ultra-massive high-throughput virtual screening","Abstract":"Here we present NRGDock, an easy-to-use docking software based on Python requiring less than 0.5 CPU second per molecule. With this speed, a modern laptop can dock 1 000 000 molecules in 24 hours. Its scoring function is based on that of FlexAID and an exhaustive search procedure. NRGDock has been benchmarked against the widely used DUD-E benchmarking dataset and obtained median enrichment factors similar to AutoDock Vina and Glide. Furthermore, NRGDock performs well on protein structures generated by AlphaFold, where residue positioning may not be modelled precisely. To validate the performance of NRGDock in high throughput virtual screening, testing was conducted on 102 DUD-E targets against 48.3 million compounds from the Enamine Real Diversity Subset (ERDS) for a total of 4.9 billion docking simulations. A clear separation in scores was observed with true binders getting significantly better scores than the ERDS molecules. Lastly, we used the protein kinase PIM-1 associated with triple-negative breast cancer and the related kinases PIM-2 and PIM-3 against the ERDS library. We show that dissimilar top-scoring compounds can be identified unique for each related target."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Zuolong Zhang","Title":"Enhancing Generalizability and Performance in Drug-Target Interaction Identification by Integrating Pharmacophore and Pre-trained Models","Abstract":"In drug discovery, it is crucial to assess the drug-target binding affinity. Although molecular docking is widely used, computational efficiency limits its application in large-scale virtual screening. Deep learning-based methods learn virtual scoring functions from labeled datasets and can quickly predict affinity. However, there are three limitations. First, existing methods only consider the atom-bond graph or one-dimensional sequence representations of compounds, ignoring the information about functional groups (pharmacophores) with specific biological activities. Second, relying on limited labeled datasets fails to learn comprehensive embedding representations of compounds and proteins, resulting in poor generalization performance in complex scenarios. Third, existing feature fusion methods cannot adequately capture contextual interaction information. Therefore, we propose a novel drug-target binding affinity prediction method named HeteroDTA. Specifically, a multi-view compound feature extraction module is constructed to model the atom-bond graph and pharmacophore graph. The residue concat graph and protein sequence are also utilized to model protein structure and function. Moreover, to enhance the generalization capability and reduce the dependence on task-specific labeled data, pre-trained models are utilized to initialize the atomic features of the compounds and the embedding representations of the protein sequence. A context-aware nonlinear feature fusion method is also proposed to learn interaction patterns between compounds and proteins. Experimental results on public benchmark datasets show that HeteroDTA significantly outperforms existing methods. In addition, HeteroDTA shows excellent generalization performance in cold-start experiments and superiority in the representation learning ability of drug-target pairs. Finally, the effectiveness of HeteroDTA is demonstrated in a real-world drug discovery study."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Petras Kundrotas","Title":"DOCKGROUND: a new release of the long-standing resource for studying protein recognition","Abstract":"Artificial intelligence (AI) has transformed the field of computational structural biology. Modeled structures of globular proteins now are accurate enough for computer-aided drug design. Structural prediction of protein-protein (PP) complexes (protein docking) has also been significantly advanced. Still, there is a constant need for re-training of the complex network models on newer data. Technical progress rapidly accelerates accumulation of data by various experimental techniques. Thus, static datasets quickly become obsolete. So far, significant efforts in generating reliable, up-to-date datasets have been focusing on individual macromolecules, while their assemblies have attracted less attention, mainly due to the complexity of the task. Here, we present a full revamp of our well-established DOCKGROUND resource for studying protein recognition (http:\/\/dockground.compbio.ku.edu). The resource contains comprehensive sets of data needed for the development and testing of protein docking techniques, including AI-based methods: bound and unbound (experimentally determined and simulated) structures of PP complexes, model-model complexes, docking decoys of experimentally determined and modeled proteins, and templates for comparative protein docking. The core dataset of bound PP structures, from which other sets are derived, is automatically updated on a weekly basis. We also implemented a new DOCKGROUND interactive interface that allows generating custom non-redundant datasets using various parameters and provides structure visualization. The DOCKGROUND resource also incorporates docking model quality assessment tool CAPRI-Q, which utilizes CAPRI criteria and other quality metrics such as DockQ, TM-score and l-DDT."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Marian Novotny","Title":"On finding the right match â€“ a structural perspective","Abstract":"Proteins can assume a number of 3D structural conformations during their lifetime and many of them can undergo a substantial conformational change that might be crucial for their function, e.g., during ligand binding._x000D_\nMany machine learning methods that are utilising 3D structural information are often trained on just a single structure of the protein. The single structure, however, does not have to represent the protein fully and it can even be misleading. For example,training a ligand-binding site prediction method on a conformation that is already binding a ligand (holo structure), while the prediction makes more sense for a conformation without a bound ligand (apo structure)._x000D_\nTo help avoid potential biases in building datasets, we have developed a tool called AHoJ (www.apoholo.cz) to identify apo-holo structure pairs for user-defined binding sites and post-translational modifications. We have also developed AHoJ-DB (www.apoholo.cz\/db), a database of apo-holo structure pairs for biologically relevant ligands as defined in the BioLiP2 database. Both services have easy-to-use interfaces and provide metrics of the similarity of binding sites between apo and holo structures, which can be used for further downstream analysis or development of derived datasets. An analysis of AHoJ-DB shows that apo structures are not available for more than 50% of the experimentally described binding sites. We used AHoJ-DB to build CryptoBench, a dataset of cryptic binding sites, which consists of 1437 apo structures and is the most extensive collection of its kind to date."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Valentin Lombard","Title":"Explaining Conformational Diversity in Protein Families through Molecular Motion","Abstract":"Proteins play a central role in biological processes, and understanding their conformational variability is crucial for unraveling their functional mechanisms. Recent advancements in high-throughput technologies have enhanced our knowledge of protein structures, yet predicting their multiple conformational states and motions remains challenging. This study introduces Dimensionality Analysis for protein Conformational Exploration (DANCE) for a systematic and comprehensive description of protein families conformational variability. DANCE accommodates both experimental and predicted structures. It is suitable for analyzing anything from single proteins to superfamilies. Employing it, we clustered all experimentally resolved protein structures available in the Protein Data Bank into conformational collections and characterized them as sets of linear motions. The resource facilitates access and exploitation of the multiple states adopted by a protein and its homologs. Beyond descriptive analysis, we assessed classical dimensionality reduction techniques for sampling unseen states on a representative benchmark. This work improves our understanding of how proteins deform to perform their functions and opens ways to a standardized evaluation of methods designed to sample and generate protein conformations._x000D_\nIn brief, the main contributions of our work are the following: 1. A pipeline was constructed for systematic analysis of protein conformational variability,_x000D_\n2. Datasets of protein ensembles and extracted linear motions have been made publicly accessible, _x000D_\n3. The ability of classical manifold learning methods, including PCA and kPCA, to_x000D_\ncapture the diversity of protein conformational states was evaluated."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Myungjin Lee","Title":"Pathway of transition for HIV-1 envelope trimer from prefusion-closed to CD4-bound open through an occluded-intermediate state","Abstract":"HIV entry into host cells is initiated by the engagement of the gp120 subunit of the HIV-1 envelope (Env) trimer with the cellular receptor CD4. This interaction induces substantial structural changes in the HIV-1 Env trimer. Although there is existing static structural information for both the prefusion-closed and the CD4-bound prefusion open trimer, the complete transition pathway between these static states (such as transition structures) remains uncharacterized. In this study, we investigated the transition of a fully and site specifically glycosylated HIV-1 Env trimer between prefusion-closed and CD4-bound open conformations using a special molecular dynamics simulation technique â€“ collective MD simulation (coMD). Here, we identified a transition intermediate â€“ the occluded intermediate state. Previously reported antibodies Ab1303, Ab1573, b12, and DH851.3 recognized this intermediate. Additionally, we validated the result by experiments single-molecule FÃ¶rster resonance energy transfer analysis, confirming that each of these four antibodies induces and stabilizes this distinct intermediate state of Env on the virus, replacing the CD4-bound open state. Overall, our findings using coMD simulation delineate a transition pathway between prefusion-closed and CD4-bound open conformations, unveiling the occluded intermediate as a prevalent intermediate state."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Aleksey Porollo","Title":"Analysis and prediction of RuBisCO kinetics using deep learning","Abstract":"This study focuses on enhancing the efficiency of Calvin cycle by targeting the kinetic parameters of its key enzyme, Ribulose-1,5-bisphosphate carboxylase\/oxygenase (RuBisCO). RuBisCO's slow catalytic rate (Kcat) and its specificity for COâ‚‚ over Oâ‚‚ (Sc\/o) substantially limit photosynthetic efficiency, particularly under high COâ‚‚ levels and light intensities. To address this, we analyzed 175 RuBisCO complexes with experimentally measured kinetic parameters using the protein language model ProtT5 for sequence embeddings. These embeddings were then processed through various machine learning models - Ridge regression, LASSO regression, SVM, and Random Forest regression - to predict Kcat and Sc\/o. The Ridge regression models performed best, achieving a Pearson correlation coefficient of 0.611 and RÂ² of 0.359 for Kcat, and 0.814 and RÂ² of 0.663 for Sc\/o, utilizing leave-one-out cross-validation. Further, we applied these models to predict kinetic parameters for 56,379 non-annotated RuBisCO sequences. Top performing sequences from both experimentally annotated and predicted datasets underwent in silico mutagenesis using a genetic algorithm. This mutagenesis targeted either any sequence position or specifically those lining the active site cavity, excluding the catalytic sites. Conducted over 10 iterations in 5 independent runs with 5000 mutants each, this approach yielded a maximum predicted Kcat of 12 sâ»Â¹ and 10 sâ»Â¹ from full sequence and cavity-targeted mutagenesis, respectively, a 2-fold improvement over natural enzymes. Our results highlight the potential of using computational tools and genetic algorithms for the rational design of RuBisCO, aiming to improve photosynthetic efficiency and agricultural productivity while contributing to climate change mitigation and renewable energy development."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Gabriel Galdino","Title":"Understanding and predicting ligand efficacy in the mu-opioid receptor through quantitative dynamical analysis of complex structures","Abstract":"GPCRs are a family of membrane proteins that regulate many biological processes and are attractive targets for drug development, representing approximately 1\/3 of global marketed drugs. We docked a set of ligands with known Emax for GTP-gammaS binding to a crystal structure of the active Mu (MOR) and Kappa (KOR) Opioid Receptors. Using a coarse-grained approach, we applied normal mode analysis to calculate Dynamical Signatures of different ligand\/GPCR complexes, identifying local and global changes in flexibility of different residues upon ligand binding. We used LASSO multiple linear regression to determine crucial residues in contact with the set of ligands and to obtain predictors of the efficacy of new drug candidates as agonists, antagonists, or partial agonists._x000D_\nWe obtained a roc AUC> 0.85 when analysing the performance of the model as a binary classifier. By analyzing the coefficients of these predictors, we identified positions of high importance to the receptor activation, such as L85 (Ballesteros-Weinstein position 1.47), that have mutations that are reported to affect morphine response in MOR, and positions with no known mutations reported such as K305 (6.58) for MOR. Our study provides insights into the dynamics and structural features of ligand binding to GPCRs and represents a new tool for predicting the efficacy of new drug candidates that can be coupled to high-throughput screening."},{"Track":"3DSIG","Room":"520a","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Aydin Wells","Title":"Dynamic network analysis of protein structural change","Abstract":"A proteinâ€™s sequence folds into a 3D structure, which directs what other proteins it may interact with to carry out cellular function. Hence, analyses of protein structures are critical for understanding protein functions. Because functions of many proteins remain unknown, computational approaches for linking proteinsâ€™ structures to functions are necessary._x000D_\n_x000D_\nOur lab previously used network-based methods to model protein structures as protein structure networks (PSNs). Graph-based analyses of these PSNs proved to be superior to using state-of-the-art sequence and non-network-based 3D structural approaches in task of protein structure classification (PSC). However, traditional PSN approaches (including ours) modeled whole, native protein 3D structures as static PSNs that overlook the protein folding dynamics. To overcome this, we recently proposed a dynamic PSN idea. Unfortunately, there is lack of data on 3D sub-structural configurations (or intermediates) of a protein as it undergoes folding to attain its native structure. So, we had to resort to modeling native structures of proteins as dynamic PSNs. Nonetheless, even this yielded significant improvements in the PSC task over modeling the native structures as static PSNs._x000D_\n_x000D_\nMost recently, as an even better proxy to studying protein folding dynamics than our recent PSC study, we have identified large enough experimental data that captures how the structure of a protein dynamically changes before vs. after the protein is bound to a ligand. We aim to examine how well the dynamic PSN analyses of this data will be able to explain seven different types of protein structural changes observed in the data."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"08:40-09:20","Format":"In Person","Speaker":"Yana Bromberg","Title":"Metallic origins of life","Abstract":"How did life appear on our planet? Alexander Oparinâ€™s 1924 theory of abiotic evolution of carbon-based molecules in a primordial soup suggests a means to the end. However, the evolutionary path beyond formation of individual molecules remains one of the most profoundly unanswered questions in biology. Biologically catalyzed redox reactions, i.e.  proton-coupled electron transfer, drive the energy requirements of all life on Earth, implying that they must have been among the first functionalities acquired by early life. _x000D_\nWe aimed to explore the patterns of evolution of redox-driving proteins, i.e. oxidoreductases. The billions of yearsâ€™ worth of divergence among existing oxidoreductases renders sequence similarity metrics inapplicable. Thus, we incorporated structure into our explorations. We found that the peptide structures that bind transition metals, ubiquitous in redox, have similar topologies across the full diversity of existing metal-binding proteins. The similarity between these peptides strongly suggests that metal binding had a small number of common origins. Moreover, folds central to our network of similarities came primarily from oxidoreductases, further confirming the idea that ancestral peptides facilitated electron transfer reactions. We further note that most (>85%) of the experimentally determined protein structures incorporate similar folds, suggesting that metal-binding may have given rise to much more functionality. Finally, our results suggest that the earliest, biologically-functional peptides were likely available prior to the assembly of the first fully functional protein domains over 3.8 billion years ago."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Jianlin Cheng","Title":"De Novo Atomic Protein Structure Modeling for Cryo-EM Density Maps Using 3D Transformer and Hidden Markov Model","Abstract":"Accurately building three-dimensional (3D) atomic structures from 3D cryo-electron microscopy (cryo-EM) density maps is a crucial step in the cryo-EM-based determination of the structures of protein complexes. Despite improvements in the resolution of 3D cryo-EM density maps, the de novo conversion of density maps into 3D atomic structures for protein complexes that do not have accurate homologous or predicted structures to be used as templates remains a significant challenge. Here, we introduce Cryo2Struct, a fully automated ab initio cryo-EM structure modeling method that utilizes a 3D transformer to identify atoms and amino acid types in cryo-EM density maps first, and then employs a novel Hidden Markov Model (HMM) to connect predicted atoms to build backbone structures of proteins. Tested on a standard test dataset of 128 cryo-EM density maps with varying resolutions (2.08 - 5.6  ÌŠA) and different numbers of residues (448 - 8,416), Cryo2Struct built substantially more accurate and complete protein structural models than the widely used ab initio method - Phenix in terms of multiple evaluation metrics. Moreover, on a new test dataset of 500 recently released density maps with varying resolutions (1.9 - 4.0  ÌŠA) and different numbers of residues (234 - 8,828), its performance of building atomic structural models is rather robust against changes in the resolution of density maps and the size of protein structures."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"Live Stream","Speaker":"Han Huang","Title":"RiboDiffusion: Tertiary Structure-based RNA Inverse Folding with Generative Diffusion Models","Abstract":"RNA design shows growing applications in synthetic biology and therapeutics, driven by the crucial role of RNA in various biological processes. A fundamental challenge is to find functional RNA sequences that satisfy given structural constraints, known as the inverse folding problem. Computational approaches have emerged to address this problem based on secondary structures. However, designing RNA sequences directly from 3D structures is still challenging, due to the scarcity of data, the non-unique structure-sequence mapping, and the flexibility of RNA conformation. In this study, we propose RiboDiffusion, a generative diffusion model for RNA inverse folding that can learn the conditional distribution of RNA sequences given 3D backbone structures. Our model consists of a graph neural network-based structure module and a Transformer-based sequence module, which iteratively transforms random sequences into desired sequences. By tuning the sampling weight, our model allows for a trade-off between sequence recovery and diversity to explore more candidates. We split test sets based on RNA clustering with different cut-offs for sequence or structure similarity. Our model outperforms baselines in sequence recovery, with an average relative improvement of 11% for sequence similarity splits and 16% for structure similarity splits. Moreover, RiboDiffusion performs consistently well across various RNA length categories and RNA types. We also apply in-silico folding to validate whether the generated sequences can fold into the given 3D RNA backbones. Our method could be a powerful tool for RNA design that explores the vast sequence space and finds novel solutions to 3D structural constraints."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-10:50","Format":"In Person","Speaker":"Philippe Youkharibache","Title":"Positional Protein Bioinformatics: A universal residue numbering scheme for the Immunoglobulin (Ig) fold enables its systemic detection in the protein universe.","Abstract":"The Immunoglobulin fold (Ig-fold) is the most populous fold in the human proteome, found in proteins from all domains of life, with current (under)estimates ranging from 2 to 3% of protein coding regions. The ability of Ig-domains to reliably fold and self-assemble through highly specific interfaces represents a remarkable property of these domains that makes them key elements of molecular interaction systems: the immune system, the nervous system, the muscular system and the vascular system. We define a universal sequence numbering scheme, called â€œIgStRAnDâ€ (Immunoglobulin Strand Residue Anchor Dependent), to represent all domains sharing the Ig-fold. IgStrand numbering enables comparative structural, functional, and evolutionary analyses through positional comparisons between any Ig-domain variant across the universe of Ig-domains.  It enables the systematic study of the Ig-proteome and associated Ig-Ig interactomes and sheds light on the robust Ig protein folding algorithm used by nature to form beta sandwich supersecondary structures, responsible for what may be convergent evolution for many of the more than 300 superfamilies sharing the fold. The numbering scheme is at the heart of an algorithm implemented in the interactive structural analysis software iCn3D to systematically recognize Ig-domains, to annotate them, and to perform detailed comparisons in sequence, topology, and structure, regardless of their tertiary plasticity and quaternary organizations. We performed a (preliminary) survey of the human proteome of over 80,000 protein structures leading to a surprisingly higher number of proteins having co-opted Ig-, Ig-like and Ig-extended domains than was estimated in the original human genome survey."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"10:50-11:10","Format":"In Person","Speaker":"Dongjun Guo","Title":"ImmunoMatch: Illuminating the design of antibody heavy and light chain pairs using deep learning approaches and structure analysis","Abstract":"Antibodies are composed of heavy (H) and light (L) chains. Sequence variations of H and L chains therefore combinatorially contribute to a diverse antibody â€œrepertoireâ€ for eliciting responses against a variety of antigens. How H chain chooses its L chain partner is still under debate. Little attention has been paid to the exact amino acid preferences and their relative importance in the H-L protein interface. Our results illustrate molecular rules governing antibody H-L chain pairing preferences._x000D_\nHere we present ImmunoMatch, a heavy-light chain pairing prediction tool taking advantage of recently published antibody language models. We capitalise on the increase in single-cell, paired H-L antibody repertoire data, and build the model to distinguish cognate H-L pairs from random synthetic pairs, with the AUC achieved 0.75. We assembled an antibody structure database (VCAb: https:\/\/fraternalilab.cs.ucl.ac.uk\/VCAb\/) for external validation and further structural interpretation of the pairing prediction. We show that our model, trained on human antibody repertoire, performs well on human and humanized antibody, while the performance dropped in detecting the cognitive pairs from mouse and chimera antibody. We took one therapeutic antibody (trastuzumab) for further analysis by searching through the potential mutation space using ImmunoMatch and extracting attention matrix and found positions which can increase\/decrease the H-L pairing likelihood clustering around CDR loops and H-L interface. These results highlight the necessity of considering the entire antibody sequence in antibody design by pre-excluding unlikely H-L combinations in the pipeline for better developability."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"11:10-11:30","Format":"In Person","Speaker":"Miriam Poley-Gil","Title":"Exploring the biophysical boundaries of protein families with deep learning methods","Abstract":"Recently, Deep Learning models have revolutionised the Molecular Biology field allowing us to explore the intricate interplay between protein sequence, structure and function faster. To understand what they are capturing and generating we have combined state-of-the-art protein models for inverse folding (such as ProstT5[1] and ProteinMPNN[2]) and for sequence generation (such as ProtGPT2[3] and ZymCTRL[4]) with biophysical analyses (Figure 1). _x000D_\nWe have studied conservation patterns of local energetic frustration in artificial datasets to shed light on the evolutionary processes leading to the diversification of some protein families, under the assumption that proteins are optimised for folding and stability, but also evolutionarily selected to function. We have developed a tool called FrustraEvo[5] that measures such conservation within and between protein families (available in full on the server https:\/\/frustraevo.qb.fcen.uba.ar\/)._x000D_\nWe found that most of the highly frustrated native residues are related to functional aspects. These functional residues are mostly recovered by sequence generation models, suggesting that there are alternative ways to design proteins instead of the way explored by evolution. In the case of catalytic sites, they are also recovered by inverse folding models. We therefore point out a selective memory concerning functionality (primary level of memory (local)). However, ProteinMPNN, also recovers the main network of frustrated contacts of the functional domains even suggesting a tertiary level of memory (contacts). Thus, our approach promises to effectively unravel the intricacies of protein family boundaries and explore design options for understanding protein evolution."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"11:30-11:40","Format":"In Person","Speaker":null,"Title":"Can proteins be represented through secondary structures?","Abstract":"Recent advancements in protein classification, driven by Foldseek for tertiary structure-based searches, raise the question of whether a simplified secondary structure format is enough for classification and functional inference, eliminating the need for confident tertiary structure determination. This paper explores this debate using a sequence format where 'H' denotes helices, 'S' represents strands, and 'L' signifies loops\/turns for each amino acid's secondary structure. Through an all-versus-all comparison using CATH and SCOPe datasets, the approach, though slightly less accurate than tertiary structure-based classification, advocates for a simple, informative representation of proteins, maintaining 90%-93% of tertiary structure performance. This invites the development of a search engine for all secondary structure sequences, facilitating a simple, efficient, and rapid protein search with minimized information requirements."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Thomas Litfin","Title":"SPfast: Highly efficient protein structure alignment with segment-level representations and block-sparse optimization","Abstract":"Recent advances in protein structure modelling have increased the availability of high-quality protein structures at an unprecedented scale. Newly available structure libraries represent an exciting opportunity for discovery-based research. However, the explosion of protein structure data has exposed scaling deficiencies in the bioinformatics toolset which limit their utility for downstream analyses. These scaling problems will only be further exacerbated as modelling projects expand to noncanonical isoforms, dynamic trajectories, de novo designs etc. foldseek has introduced a structure state alphabet to mitigate this computational burden. However, the increased speed is accompanied by trade-offs in search sensitivity due to sacrificing information about global topology. In this work we describe a fully geometric protein structure search engine, SPfast, which leverages a coarse grained, hierarchical representation and an efficient block-sparse optimization heuristic to greatly accelerate pairwise protein structure alignment and enable practical analysis of large-scale structure libraries. Combining SPfast with a newly parameterized SPscore maintains state-of-the art performance for database search, more accurately reproduces pairwise evolutionary alignments and increases throughput by 100x compared with traditional methods."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Alexander Monzon","Title":"STRPsearch: fast detection of structured tandem repeat proteins","Abstract":"State-of-the-art prediction methods are generating millions of publicly available protein structures. Structured Tandem Repeats Proteins (STRPs) constitute a subclass of tandem repeats characterized by repetitive structural motifs. STRPs exhibit distinct propensities for secondary structure and form regular tertiary structures, often comprising large molecular assemblies. They can perform important and diverse biological functions due to their highly degenerated sequences, which maintain a similar structure while displaying a variable number of repeat units. This suggests a disconnection between structural size and protein function. However, automatic detection of STRPs remains challenging with current state-of-the-art tools due to their lack of accuracy and long execution times, hindering their application on large datasets. In most cases, manual curation is the most accurate method for detecting and classifying them, making it impossible to inspect millions of structures._x000D_\nWe present STRPsearch, a novel computational tool for rapid identification, classification, and mapping of STRPs. Leveraging the manually curated entries in RepeatsDB as the known conformational space of the STRPs, STRPsearch utilizes the latest advancements in structural alignment techniques for a fast and accurate detection of repeated structural motifs in protein structures, followed by an innovative approach to map units and insertions through the generation of TM-score graphs. STRPsearch can serve researchers in structural bioinformatics and protein science as an efficient and practical tool for analysis and detection of STRPs."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Nicola Bordin","Title":"The Encyclopedia of Domains","Abstract":"The Encyclopaedia of Domains (TED) is a comprehensive classification of all globular protein structure domains in AlphaFold Database v4. Harnessing state-of-the-art deep learning methods for domain detection, structure comparison and fold detection, TED segments and classifies domains across AFDB, identifying over 370 million distinct domains, surpassing sequence-based resources by over 100 million domains. Nearly 90% of these domains exhibit similarities with known superfamilies in CATH, expanding the resource by over 600-fold. The remaining domains that do not have relatives in any PDB-based resources unveiled over 7 thousand new folds, some of which have interesting and beautiful symmetries. We also find some fascinating new architectures._x000D_\n _x000D_\nTED uncovers over 10,000 previously undetected structural interactions between superfamilies and extends domain coverage to over 1 million taxa, enhancing research for organisms which previously had low to non-existent structural coverage. TED data will be made available in 3D-Beacons as well as a dedicated resource, significantly enriching CATH superfamilies."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":null,"Title":"EXPLORING THE IMPACT OF STRESS-RESISTANT MUTATIONS IN PLANTS  USING ALPHAFOLD2 PROTEIN STRUCTURES","Abstract":"Plants face significant challenges arising from environmental stress factors like pathogens, salt, drought, extreme temperatures, and flooding. To counter these challenges, plants have evolved a wide array of adaptive mechanisms. In this study, we leverage the state-of-the-art AlphaFold Protein Structure Database to untangle the complex interplay between stress-resistant mutations found in plants through the Genome-Wide Association Studies (GWAS) Atlas and their corresponding protein structures. Plant structures from rice, maize, and soybean were chopped into 195 distinct structural domains using our in-house CATH-Assign protocol and 197 missense mutations associated with stress resistance were pinpointed. These mutations were analyzed for their impact using Grantham scores, MutPred2, and SIFT, with a keen emphasis on their proximity to functional sites. Our investigation revealed that 95 mutations led to non-conservative amino acid changes, with a majority of these mutations (90%) found to be non-pathogenic, linked to favorable gain-of-function adaptations. This sheds light on intricate plant stress-adaptive mechanisms. Furthermore, our study delved into the structural nuances, showcasing mutations' proximity to various functionally important sites such as 56 FunSites (our in-house predicted evolutionary conserved functional sites), 26 ligand-binding regions, 28 protein-protein interfaces, and more. Moreover, Alphafold Multimer was used to specifically look at the shortlisted protein domains and their interactions with stress mediators. By employing AlphaFold's high-quality protein structures, this pioneering study not only enhances our understanding of plant stress resilience at a molecular level but also paves the way for future strategies aimed at sustaining plant survival and productivity in challenging environments."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Yunzhuo Zhou","Title":"DDMut-PPI: predicting effects of mutations on protein-protein interactions using graph-based deep learning","Abstract":"Protein-protein interactions (PPIs) play a vital role in cellular functions and are essential for therapeutic development and understanding diseases. Traditional methods for exploring the effects of mutations on PPIs face challenges related to experimental complexity, cost, and scalability. While computational methods provide a quicker alternative, they often struggle to balance efficiency and precision in their predictions. In response, we present DDMut-PPI, a deep learning model that efficiently and accurately predicts changes in PPI binding free energy upon single and multiple point mutations. Building on the robust siamese network architecture with graph-based signatures from our prior work, DDMut, the DDMut-PPI model was enhanced with a graph convolutional network to better capture the importance of residues at the interface based on a 2D interaction graph. We used residue-specific embeddings from ProtT5 protein language model as node features, and a variety of molecular interactions as edge features. By integrating evolutionary context with spatial information, this framework enables DDMut-PPI to achieve a robust Pearson correlation of up to 0.67 (RMSE: 1.51â€‰kcal\/mol) in our non-redundant evaluations, outperforming most existing methods. Importantly, by utilising both forward and hypothetical reverse mutations to account for model anti-symmetry, the model demonstrated consistent performance across mutations that increase or decrease binding affinity. We believe DDMut-PPI would be a valuable resource for researchers and clinicians looking to explore the complex dynamics of protein interactions and their implications for health and disease. DDMut-PPI is freely available as a user-friendly web server and an API at https:\/\/biosig.lab.uq.edu.au\/ddmut_ppi."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Qichang Zhao","Title":"DDAffinity: Predicting the changes in binding affinity of multiple point mutations using protein three-dimensional structure","Abstract":"Motivation: Mutations are the crucial driving force for biological evolution as they can disrupt protein stability and protein-protein interactions which have notable impacts on protein structure, function, and expression. And the progressive accumulation of multiple point mutations would lead to cancer. However, existing computational methods for protein mutation effects prediction are generally limited to single point mutations with global dependencies, and do not systematically take into account the local and global synergistic epistasis inherent in multiple point mutations._x000D_\nResults: To this end, we propose a novel spatial and sequential message passing neural network, named DDAffinity, to predict the changes in binding affinity caused by multiple point mutations based on protein three-dimensional (3D) structures. Specifically, instead of being on the whole protein, we perform message passing on the k-nearest neighbour residue graphs to extract pocket features of the protein 3D structures. Furthermore, to learn global topological features, a two-step additive Gaussian noising strategy during training is applied to blur out local details of protein geometry. We evaluate DDAffinity on benchmark datasets and external validation datasets. Overall, the predictive performance of DDAffinity is significantly improved compared with state-of-the-art baselines on multiple point mutations, including end-to-end and pre-training based methods. The ablation studies indicate the reasonable design of all components of DDAffinity. In addition, applications in non-redundant blind testing, predicting mutation effects of SARS-CoV-2 RBD variants, and optimizing human antibody against SARS-CoV-2 illustrate the_x000D_\neffectiveness of DDAffinity. Availability and implementation: DDAffinity is available at https:\/\/github.com\/ak422\/DDAffinity."},{"Track":"3DSIG","Room":"520a","Weekday":"Tuesday","Date":"16 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Yingying Zhang","Title":"A multiscale functional map of somatic mutations in cancer integrating protein structure and network topology","Abstract":"A major goal of cancer biology is to understand the mechanisms underlying tumorigenesis driven by somatically acquired mutations. Two distinct types of computational methodologies have emerged: one focuses on analyzing clustering of mutations within protein sequences and 3D structures, while the other characterizes mutations by leveraging the topology of protein-protein interaction network. Their insights are largely non-overlapping, offering complementary strengths. Here, we established a unified, end-to-end 3D structurally-informed protein interaction network propagation framework, NetFlow3D, that systematically maps the multiscale mechanistic effects of somatic mutations in cancer. The establishment of NetFlow3D hinges upon the Human Protein Structurome, a comprehensive repository we compiled that incorporates the 3D structures of every single protein as well as the binding interfaces of all known protein interactions in humans. NetFlow3D leverages the Structurome to integrate information across atomic, residue, protein and network levels: It conducts 3D clustering of mutations across atomic and residue levels on protein structures to identify potential driver mutations. It then anisotropically propagates their impacts across the protein interaction network,  with propagation guided by the specific 3D structural interfaces involved, to identify significantly interconnected network â€œmodulesâ€, thereby uncovering key biological processes underlying disease etiology. Applied to 1,038,899 somatic protein-altering mutations in 9,946 TCGA tumors across 33 cancer types, NetFlow3D identified 12,378 significant 3D clusters throughout the Human Protein Structurome, of which ~54% would not have been found if using only experimentally-determined structures. It then identified 28 significantly interconnected modules that encompass ~8-fold more proteins than applying standard network analyses."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Laura Arbour","Title":"The Silent Genomes Project: Building the path to equitable genomic care for Indigenous patients, one variant at a time.","Abstract":"There is broad concern that genomic technologies may not reach those with the greatest health disparities. There are numerous reasons why there are barriers to specialty care including genetic\/genomic diagnosis for Indigenous people of Canada. Of relevance is the lack of Indigenous reference data in public databases limiting interpretation of genetic\/genomic sequencing results. However, Indigenous involvement and on-going Indigenous governance of data has the potential to change the current trajectory, improving access to diagnosis for Indigenous patients with genetic conditions. With Indigenous partners, colleagues and community members, the Silent Genomes Project is building an Indigenous background variant library (IBVL). With Indigenous oversight, an approved web-based variant release mechanism (requiring registration and agreement to certain conditions) is currently being tested for the first phase of the IBVL derived from samples from consenting First Nations communities in Canada-with official release in the months to come. This presentation will provide an overview of the Silent Genomes Project, focusing on the development of the IBVL and its governance model."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Beatriz Lujan Toro","Title":"From Sequences to Reports: A Controlled Approach to Pipeline Validation in Cancer Genomics","Abstract":"The Genomics program at the Ontario Institute for Cancer Research (OICR) specializes in providing genome sequencing and analysis services, offering whole-genome transcriptome (WGTS), plasma whole genome (pWGS), and targeted panels. This is supported by assay specific analysis pipelines which are maintained under version control in github (https:\/\/github.com\/oicr-gsi). These assays are CAP\/ACD-accredited, CLIA-certified, and ISO 15189-compliant and therefore any pipeline change must be evaluated for accuracy and version controlled. Djerba, an open-source software tool developed by our group, generates reports tailored for clinical diagnostics and is used to evaluate the reproducibility of results and the impact of pipeline updates._x000D_\nAs software is updated and alternative analysis tools are identified, our pipelines are subject to change. To facilitate updates without compromising the accuracy of our reporting, we have developed a validation framework which is part of our standard operating procedures. This methodology involves the parallel execution of benchmarking samples, chosen to reflect the variety of biomarkers being reported, through both the production and revised pipelines within a distinct staging environment. Subsequent comparison of outputs, facilitated by our clinical reporting software Djerba, allows the proper evaluation of results and determines the equivalence of clinical reports by a trained genome interpreter. By rigorously testing the entire pipeline from raw sequences to variant calls, we can evaluate the suitability and accuracy of pipeline updates. This approach guarantees the reliability of our analytical processes, fulfills the requirements of our accreditation, and underscores our commitment to providing cutting-edge bioinformatics solutions tailored for cancer research and patient care."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Erika Frangione","Title":"The Canadian Genomic Data Commons (CGDC): A Platform for National Genomic Data Sharing","Abstract":"Background: Clinical data generated from genome sequencing (GS) is an ever-growing and valuable resource. To date however, this data has been largely inaccessible to clinical and research communities due to data storage costs. The CGDC will address this by establishing a digital high performance computing (HPC) infrastructure comprised of two novel databases and a suite of bioinformatics tools to put Canadian labs on a centralized platform for sharing and accessing genomic data. Three core facilities in a federated and secure data sharing ecosystem will be developed: 1. The Canadian Open Genetics Repository (COGR); 2. Canadian genome aggregation database (gnomAD-Canada); and, 3. Tools for RD researchers._x000D_\n_x000D_\nMethods: 1) COGR will be developed as a genomic database for standardizing and sharing genetic interpretations and phenotypic information as reported by Canadian diagnostic laboratories. It will develop custom workflows for automated and real-time sharing of interpretations, and will utilize consensus-building for discrepancy reporting. 2) A Canadian instance of the gnomAD browser (gnomAD-Canada) will harmonize data from 50,000 genomes derived from large-scale GS projects to generate aggregated allele frequencies. 3) A Canadian version of the seqr software will be launched for gene-discovery on up to 10,000 exomes and genomes from RD cohorts._x000D_\n_x000D_\nConclusion: The CGDC will bring together a team of global leaders in bioinformatics and database development to advance biomedical research in Canada. It will improve the sharing of genomic data nationally through the creation of core facilities that provide standardized variant interpretations and frequency estimates for the study of gene-disease relationships."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Abdul Rahman Diab","Title":"Binomify: Unified normalization of ChIP-seq data through negative binomial regression","Abstract":"ChIP-seq data is crucial for understanding the mechanisms of gene regulation, and there is a need for a statistically-principled normalization method that facilitates the quantitative comparison of experiments which were conducted under different technical conditions. We propose Binomify, a normalization approach for ChIP-seq data which uses a neural network to predict the parameters of a negative binomial distribution conditioned on a set of experimental covariates, including position-specific control signal and GC content, the antibody used during immunoprecipitation, the machine used to perform the sequencing step, and the total sequencing depth. _x000D_\n_x000D_\nBinomify first predicts a distribution over read counts at each position in the genome given the associated covariates, then computes the quantile of the observed read count within the predicted distribution; this quantile roughly indicates how â€œsurprisingâ€ the observed signal is, given the covariates. Finally, Binomify uses the computed quantile in each bin to match a target distribution using quantile normalization, producing the final normalized signals which can be used for downstream tasks._x000D_\n_x000D_\nEvaluations on 14 ENTEx H3K27me3 experiments indicate that the normalized signals produced by the model are more predictive of gene expression levels than the observed read counts when using linear regression (mean R2=0.12 using normalized signals vs. mean R2=0.09 using observed reads), and are comparable predictors of externally-called peaks (0.97 AUROC using both signal types). Our results suggest that using a normalization method which explicitly accounts for technical noise can empower simple downstream methods to make more accurate predictions using ChIP-seq data."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-15:00","Format":"In Person","Speaker":null,"Title":"About Ag and AI","Abstract":null},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Pierre-Étienne Jacques","Title":"UseGalaxy Canada now in production","Abstract":"We are pleased to announce the release of the UseGalaxy Canada initiative (https:\/\/starthere.usegalaxy.ca). In collaboration with the international consortium of large Galaxy instances led by the US, Europe and Australia (https:\/\/galaxyproject.org\/use\/) and after more than 10 years of experience with Galaxy through the development, implementation and maintenance of the GenAP platform, we set up over the last year a new stable instance of this well-established free open-source system to analyze, visualize, and share data and workflows. This platform, using CILogon for the user authentication through their institution and located on the Beluga cloud from the Digital Research Alliance of Canada, is implementing a common core set of tools and reference genomes."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Deshan Perera","Title":"Apollo: A comprehensive GPU-powered Within-host Viral simulator with tissue and cellular hierarchies for studying viral evolutionary and infection dynamics.","Abstract":"The advent of high-throughput sequencing technologies coupled with breakthroughs in third-generation sequencing has allowed new exploration into within-host viral dynamics. However, a simulation platform to analyze this new world of within host\/tissue\/cell viral populations does not exist. We present a solution. Apollo is a state-of-the-art within-host viral simulator developed to comprehensively model viral transmission, replication dynamics, natural selection, and host behaviors such as Lost to Follow Up, across population, host, tissue, and cell levels. Leveraging CATE (https:\/\/doi.org\/10.1111\/2041-210X.14168), our proven large-scale GPU CUDA-powered parallel processing architecture, Apollo achieves unprecedented speeds and hardware efficiency. Apollo is built on the standard Wright Fisher (WF) evolutionary model, but, thanks to its scriptable parameter structure, users are able to design simulations that mimic real world dynamics that expand beyond the WF model. Through rigorous testing, we have been able to demonstrate Apolloâ€™s accuracy and resource efficiency. We present a complete simulation of an HIV epidemic with within-tissue factors, recombination, and mutation mechanisms that characterizes HIV viral evolution and dynamics both within hosts and across host levels. The simulations correspond\/align with clinical findings and enhances the real-world data by providing further insight into the pedigree of viral variants and within-host quasispecies dynamics. Apollo represents a significant advancement in structured viral evolution and offers a powerful new tool for studying complex viral dynamics aimed to inform individual therapeutics and public health interventions."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"J Maxwell Douglas","Title":"Utanos: A general-purpose shallow whole genome sequencing analysis toolkit identifies interpretable copy number signatures","Abstract":"Whole genome sequencing (WGS) is a powerful method for monitoring mutations in cancer genomes. However, the deep sequencing needed in a traditional WGS pipeline is costly, making it infeasible in many translational and population-level studies. In order to strike a balance between depth and breadth, shallow\/low-pass WGS methods have been applied to large-sample studies and have successfully identified copy number (CN) variation across hundreds of samples._x000D_\nEven so, should biological insights be desired beyond relative CN calling, comprehensive downstream analysis is needed. With this need in mind, we developed a general-purpose R-package for analysis workflows after relative copy-number calling - UTANOS, short for UTilities for the ANalysis Of Shallow WGS. Utanos provides data-driven quality control routines, absolute CN scaling, cross-cohort CN diversity profiling, CN feature extraction\/factorization and annotation, and Homologous Recombination Deficiency (HRD) status prediction._x000D_\nWe applied Utanos to sWGS data profiled in multiple cancer genome studies, including two subtypes of endometrial carcinoma and High-Grade Serous Ovarian Cancer, to extract cancer- and individual-specific CN signatures and their activities genome-wide. Across cancer types\/subtypes, Utanos indentified comparable CN signature sets and highlighted both ubiquitous and unique genomic features and locations. Further, we tested Utanos on down-sampled deep WGS (PCAWG), thus simulating sWGS data using a well-characterized cohort, and confirmed that Utanos effectively captured CN signatures manifested in the cohort. Finally, Utanos scales well to cohort-level analysis with hundreds of concurrent samples comfortably operable on a laptop."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:20","Format":"In Person","Speaker":"Vincent Ferretti","Title":"The Quebec Genomic Data Center","Abstract":"The Quebec Genomic Data Center (CQDG), launched in April 2024, is a collaborative research infrastructure project whose mission is to enable the development of precision medicine and artificial intelligence by promoting the harmonization and sharing of genomic data produced by clinical and research activities in Quebec. The main objective of the CQDG will be to provide a secure bioinformatics platform for hosting and harmonizing data produced by genomic studies and to disseminate it through a rigorous data access process that ensures the privacy of participants. The CQDG will integrate thousands of clinical whole exomes and genomes annually from consenting patients across Quebec with diverse conditions including rare diseases and cancer. The CQDG data infrastructure is built on open-source principles and leverages our extensive experience from past and ongoing international collaborations, including participation in projects such as the International Cancer Genome Consortium (ICGC), the NCI Genomic Data Commons (GDC), the NIH Gabriella-Miller Kids First Data Resource, and the NIH INCLUDE Data Resource Center on Down Syndrome.  We will provide a comprehensive overview of the CQDG project, describe in detail its open source software infrastructure, and introduce the main functionalities of the CQDG web data portal."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Fei Wang","Title":"FraGNNet: A Deep Probabilistic Model for Mass Spectrum Prediction","Abstract":"Tandem mass spectrometry (MS\/MS) plays an important role in metabolomics analysis. MS\/MS workflows attempt molecular structure inference from mass spectral data: this is a challenging problem, and reliable automated solutions remain elusive. Existing identification strategies often rely on retrieval from spectral libraries; these approaches are limited by poor library compound coverage._x000D_\n_x000D_\nWe propose FraGNNet,a novel computational method for high resolution and interpretable MS\/MS prediction. It combines both combinatorial fragmentation and deep learning. First, a bond-breaking algorithm generates a large set of plausible fragment structures from the input molecule. Then, a graph neural network predicts a probability distribution over the fragments, determining the ones most likely to occur in the spectrum. This distribution is subsequently mapped to a distribution over chemical formulae, whose masses are used to determine peak m\/z values in the spectrum._x000D_\n_x000D_\nFraGNNet achieves over 0.70 cosine similarity (0.01 Da binning resolution) when evaluated on held-out data, outperforming other spectrum predictors. In terms of retrieval-based spectrum identification, FraGNNet performs well as an spectra library generation tool. FraGNNet is highly interpretable, providing fragment annotations for each predicted peak and an estimate of the total peak intensity that the model cannot explain. Through ensembling, unreliable peak intensities and annotations can be identified to increase user confidence in the modelâ€™s predictions._x000D_\n_x000D_\nIn summary, FraGNNet is a state-of-the-art spectrum prediction model with a number of unique features that make it a compelling choice for MS\/MS-based structure identification."},{"Track":"Bioinformatics in Canada","Room":"520a","Weekday":"Sunday","Date":"14 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Jennifer Geddes-McAlister","Title":"Formation of the Canadian Artificial Intelligence and Mass Spectrometry Consortium (CAN-AIMS)","Abstract":"Disease spans diverse demographics and negatively impacts human health, affecting each individual in a specific manner. The ability to diagnose, monitor, and treat individuals in a strategic and personalized manner is limited due to lack of disease knowledge and discrepancies in accessibility of healthcare. To overcome these limitations, an increased understanding of the causes, regulatory mechanisms, and treatment options for diseases are needed. Importantly, the identification of proteins, metabolites, and pathways responsible for disease presents a critical starting point. However, the integration of datasets across biological networks and platforms is challenging to ensure comprehensive and robust analyses. Herein, we introduce the Canadian Artificial Intelligence and Mass Spectrometry Consortium (CAN-AIMS), which brings together researchers from across Canada with diverse expertise in human disease, proteomics, computation, and bioethics. The goals of CAN-AIMS are three-fold, to: i) explore innovative research strategies from discovery to translation, ii) develop a hands-on training platform for the next generation of scientists, and iii) build capacity in leading-edge instrumentation and computational recourses for Canada. Together, CAN-AIMS provides the first cohesive group of researchers working collectively to define and mitigate disease within Canada using a combination of mass spectrometry-driven technologies and computational platforms for integration of disease knowledge to improve diagnostics and treatments."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-10:50","Format":"In Person","Speaker":null,"Title":"COSI Opening Remarks","Abstract":null},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"10:50-11:55","Format":"In Person","Speaker":"Mayla Boguslav","Title":"Learning from our collective scientific ignorance: How can ontologies help us determine what isn't yet?","Abstract":"Ontologies beg the question what is or exists (known knowns). I seek to determine what isn't or doesn't exist yet (known unknowns or questions). Ontologies aim to make knowledge accessible, transparent, and searchable. Biological ontologies define the entities and relations in biological domains. The community seeks to organize, present, and disseminate knowledge in biomedicine and the life sciences more generally. This can also be done for our collective scientific ignorance - our missing or incomplete knowledge. Let's make our collective scientific ignorance accessible, transparent, and searchable. In fact, research begins with a question and progresses by exploring new and uncharted territory. Enumerating what we don't know yet can help students, researchers, funders, and publishers generate novel research questions, prioritize resources, and rebuild trust in science. Further, ideally, we combine both knowledge and ignorance to determine solved and unsolved questions. I will present my ignorance taxonomy and ignorance-base (comparable to a knowledge-base) that used ontologies. More generally, I will present a new scientific method framework that shifts the focus to ignorance and questions, not just knowledge. Join me to talk about what we donâ€™t know yet."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"11:55-12:20","Format":"In Person","Speaker":null,"Title":"Poster Madness","Abstract":"Opportunity for poster presenters to give a brief overview of their work and advertise their upcoming poster session"},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-15:05","Format":"In Person","Speaker":"Jiyu Chen","Title":"Integration of Background Knowledge for Automatic Detection of Inconsistencies in Gene Ontology Annotation","Abstract":"Biological background knowledge plays an important role in the manual quality assurance (QA) of biological database records. One such QA task is the detection of inconsistencies in literature-based Gene Ontology Annotation (GOA). This manual verification ensures the accuracy of the GOA based on a comprehensive review of the literature used as evidence, Gene Ontology (GO) terms, and annotated genes in GOA records. While automatic approaches for the detection of semantic inconsistencies in GOA have been developed, they operate within predetermined contexts, lacking the ability to leverage broader evidence, especially relevant domain-specific background knowledge. This paper investigates various types of background knowledge that could improve the detection of prevalent inconsistencies in GOA. Additionally, the paper proposes several approaches to integrate background knowledge into the automatic GOA inconsistency detection process._x000D_\nWe extended a previously developed GOA inconsistency dataset with several kinds of GOA-related background knowledge, including GeneRIF statements, biological concepts mentioned within evidence texts, GO hierarchy and existing GO annotations of the specific gene. We proposed several effective approaches to integrate background knowledge as part of the automatic GOA inconsistency detection process. The proposed approaches can improve automatic detection of self-consistency and several of the most prevalent types of inconsistencies._x000D_\nThis is the first study to explore the advantages of utilizing background knowledge and to propose a practical approach to incorporate knowledge in automatic GOA inconsistency detection. We established a new benchmark for performance on this task. Our methods may be applicable to various tasks that involve incorporating biological background knowledge."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"15:05-15:30","Format":"In Person","Speaker":"Michael Bradshaw","Title":"The cyclic nature of biases against understudied genes and diseases in knowledge graph embedding link prediction models","Abstract":"Knowledge graph embedding (KGE) models have been successfully used for a variety of biomedical applications, but have yet to be effectively applied to rare disease variant prioritization; certain limitations need to be addressed to facilitate application of these models, namely node degree bias. We found there is a cyclical form of bias against under-studied genes and diseases when using KGE models. We found that commonly studied genesâ€“like those related to heretable forms of cancer â€“ perform very well in KGE link prediction tasks (median normalized rank (MNR)=0.91); while less studied genes â€“ like those differentially expressed in females and males, or diseases caused by ancestry specific variations â€“ are deprioritized by the same systems (MNR=0.63-0.71). Our results revealed that not all information contained within large biomedical knowledge graphs is useful for training KGE models. There was a 7-10% improvement in gene-gene edge prediction when the KG was filtered to include only nodes and edges describing genes and diseases. This filtration step also drastically sped up hyperparameter optimization and training times reducing them to 1 - 2.5% that of using the full KG. Several alternative methods for exploring the KG filtration space are explored in this project. Our results show that KGE link prediction performance for gene and disease association is a very nuanced space where careful consideration of the learning model and underlying KG are required. Performance can vary by 5-11% for gene-gene edges and 11-34% for gene-disease predictions depending on the combination of KG and KGE model."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"15:30-15:55","Format":"In Person","Speaker":"Azza Althagafi","Title":"Prioritizing Causative Genomic Variants by Integrating   Molecular and Functional Annotations from Multiple Biomedical Ontologies","Abstract":"Whole-exome and genome sequencing are widely used for diagnosing_x000D_\npatients with rare diseases, but many remain undiagnosed due to_x000D_\nundiscovered disease genes\/variants or novel phenotypes arising from_x000D_\ncombinations of variants in multiple genes. Interpreting phenotypic_x000D_\nconsequences of variants relies on information about gene functions,_x000D_\nexpression, and other genomic features. Existing phenotype-based_x000D_\nprioritization methods link molecular features to phenotypic effects_x000D_\nof altering gene functions but are limited by incomplete_x000D_\ngene--phenotype associations and applicability only to genes with_x000D_\nknown phenotypes. We developed several computational methods to_x000D_\nprioritize genes based on phenotypes. Our methods incorporate genomic_x000D_\ninformation, gene functions from the Gene Ontology, anatomical site of_x000D_\nexpression from Uberon, celltype of expression using the Cell_x000D_\nOntology, and clinical phenotypes. We integrate this information and_x000D_\napply knowledge-enhanced machine learning to prioritize candidate_x000D_\ngenes. We apply this work to the prioritization of different types of_x000D_\ngenomic variants, including single nucleotide exonic variants,_x000D_\nnon-coding variants, and structural variants._x000D_\nThe methods we develop leverage large amounts of background knowledge,_x000D_\nfrom databases with ontology annotations as well as from ontology_x000D_\naxioms. We evaluated these methods using synthetic and patient-derived_x000D_\nclinical genomes."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:05","Format":"In Person","Speaker":"Emily Bordeleau","Title":"Taking AIIM at antibiotic resistance: harmonizing the nomenclature for aminoglycoside inactivating enzymes","Abstract":"Multidrug-resistant pathogens continue to challenge aminoglycoside antibiotics with the spread of genetic elements encoding aminoglycoside modifying enzymes (AMEs). Unfortunately, these enzymes have a discordant naming history that further complicates stewardship and surveillance programs. We are undertaking the management and adoption of a single AME nomenclature. We will abide by the guidelines first proposed in 1975 while incorporating additional rules accounting for the scale at which sequencing technology permits AME discovery. Cell-based and biochemical data has been curated from the literature that supports the AMEs characterized to date. CARD will utilize this data to develop software that will guide researchers in the classification of new and existing AME variants. Additionally, CARD will provide tools to evaluate AME benchmarks and recommend available namespace, resolve conflicts, or suggest additional analyses if applicable. After conducting a full review of the AME terminology in the Antibiotic Resistance Ontology (ARO), CARD has updated the ARO to categorize AMEs with the nomenclature guidelines. The revised ontology is reflective of AME biochemistry and phenotype, with strict definitions for each allele family based on antibiotic susceptibility testing. Planning of an interactive web interface to assist authors in naming and analyzing proposed novel AMEs is underway. Going forward, novel published AMEs will only be included in CARD if they have a unique proper name. There remains an ongoing process to review existing AMEs and resolve naming conflicts, for which CARD will engage authors and the research community for feedback."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"17:05-17:30","Format":"In Person","Speaker":"Hande McGinty","Title":"Investigating Food Composition Components in Cancer Prevention and Therapy using Knowledge Graphs","Abstract":"Flavonoids are polyphenolic compounds found in plants and naturally occur in fruits, vegetables, teas, wines, and chocolate. Flavonoids also have known health benefits due to their anti-oxidative, anti-inflammatory, anti-mutagenic, and anti-carcinogenic properties and their ability to inhibit\/modulate enzymatic systems. During this research we explored the relationships among different flavanoids, different foods, and different cancers using knowledge graphs and statistical methods. Our preliminary results show that the relationships among these concepts are more complex than the insights simple statistical methods can provide. In this presentation, we present our approach using KNARM methodology to data collection, data cleaning, and representation using graph databases and knowledge graphs in addition to the preliminary results of our statistical approaches. As we continue our research, we're enriching our knowledge graph by incorporating data on known cancer drugs and drug targets to the knowledge graph and adopting more complex analysis approaches to understand the dynamic interplay of flavanoid-food-cancer interactions as well as using Large Language Models (LLMs) for enhancing our knowledge graph."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Saturday","Date":"13 July","Timespan":"17:30-18:00","Format":"In Person","Speaker":null,"Title":"COSI Day 1 Wrap-up","Abstract":"Wrap-up and open time for questions"},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-10:50","Format":"In Person","Speaker":null,"Title":"COSI Announcements","Abstract":null},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"10:50-11:55","Format":"In Person","Speaker":"Karin Slater","Title":"Exploring Multiple Perspectives for Associative Knowledgebases","Abstract":"Databases encoding associative relationships between biomedical entities function as background knowledge which are leveraged for a range of purposes. For example, disease-phenotype associations are used for differential diagnosis and variant prediction, while gene-function associations are used in gene set enrichment analyses. _x000D_\n_x000D_\nIn the ontology world, these associative knowledgebases lie somewhere between the conceptualisation and instance spaces, defining foundational knowledge that is often probabilistic, associative, or uncertain, rather than axiomatic. They are formed through some combination of manual curation from expert knowledge, experimental data, and analysis of co-occurrence in literature text. Due to this aetiology of associations, existing databases represent a particular perspective on biomedical knowledge, and it is one that differs from those that might be cultivated from analysis of other sources, such as clinical data, public discussion, or alternative modularisations of literature text._x000D_\n_x000D_\nWe will explore the similarities and differences between associative knowledgebases derived from these contexts, including methodological concerns, hypothesis generation, characterisation, and implications for downstream applications."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"11:55-12:20","Format":"In Person","Speaker":"Matthew Cannon","Title":"Extracting Clinical Significance for Drug-Gene Interactions using FDA Label Packages","Abstract":"The drug-gene interaction database (DGIdb) is a resource that aggregates interaction data from over 40 different resources into one platform with the primary goal of making the druggable genome accessible to clinicians and researchers. By providing a public, computationally accessible database, DGIdb enables therapeutic insights through broad aggregation of drug-gene interaction data. _x000D_\n_x000D_\nAs part of our aggregation process, DGIdb preserves data regarding interaction types, directionality, and other attributes that enable filtering or biochemical insight. However, source data are often incomplete and may not contain the therapeutic relevance of the interaction. In this report, we address these missing data and demonstrate a pipeline for extracting physiological context from free-text sources. We apply existing large language models (LLMs) to tag and extract indications, cancer types, and relevant pharmacogenomics from free-text, FDA approved labels. We are then able to utilize the Variant Interpretation for Cancer Consortium (VICC) normalization services to ground extracted data back to formally grouped concepts._x000D_\n_x000D_\nIn a preliminary test set of 355 FDA labels, we were able to normalize 59.4% of extracted chemical entities back to ontologically-grounded therapeutic concepts. We can link this therapeutic context data back to interaction records already searchable within DGIdb. By using LLMs to extract this data set, we can supplement our existing interaction data with relevant indications, pharmacogenomic data and mutational statuses that may inform the therapeutic relevance of a particular interaction. Inclusion of these data will be invaluable for variant interpretation pipelines where mutational status can lead to the identification of a lifesaving therapeutic."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-15:05","Format":"Live Stream","Speaker":"Fernando Zhapa-Camacho","Title":"Predicting protein functions using positive-unlabeled ranking with ontology-based priors","Abstract":"Automated protein function prediction is a crucial and widely studied problem in bioinformatics. Computationally, protein function is a multilabel classification problem where only positive samples are defined and there is a large number of unlabeled annotations. Most existing methods rely on the assumption that the unlabeled set of protein function annotations are negatives, inducing the false negative issue, where potential positive samples are trained as negatives. We introduce a novel approach named PU-GO, wherein we address function prediction as a positive-unlabeled ranking problem. We apply empirical risk minimization, i.e., we minimize the classification risk of a classifier where class priors are obtained from the Gene Ontology hierarchical structure. We show that our approach is more robust than other state-of-the-art methods on similarity-based and time-based benchmark datasets. Data and code are available at https:\/\/github.com\/bio-ontology-research-group\/PU-GO."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"15:05-15:30","Format":"In Person","Speaker":"An Phan","Title":"Protein Function: how much do we know and how much do we care?","Abstract":"The resources required to study gene function are limited, especially when considering the number of genes in the human genome and the complexity of their function. Genes are prioritized for experimental studies based on many different considerations, including, but not limited to, perceived biomedical importance and the understanding of biomedical processes. At the same time, the lion's share of genes are not studied or are under-characterized, with detrimental results to our understanding of the functions inherent to them, and their effects on human health and wellness. However, the size of this disparity in knowledge has not yet been quantified. Understanding function annotation disparity is a necessary first step toward understanding how much functional knowledge is gained of the human genome, and guidelines for the future studies of its component genes effectively._x000D_\nHere, we present a comprehensive longitudinal analysis of our understanding of the human proteome utilizing tools from economics and information theory. Specifically, we view the human proteome as a population of proteins with a knowledge economy: we treat quantified knowledge of the function of each protein as the equivalent of its wealth, and examine the distribution of knowledge of proteins within a proteome in the same manner distribution of wealth is studied in societies. Our results show a broad distribution of functional knowledge about human proteins over the last decade, in which the inequality in annotations of these proteins remains high."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"15:30-15:55","Format":"In Person","Speaker":"Brook Santangelo","Title":"Harmonizing human and microbial datasets to explore mechanisms of the gut microbiome in disease","Abstract":"The integration of disparate forms of biological data is essential for understanding human health and disease. Doing so is particularly challenging in the context of microbe-host interactions that contribute to both positive and negative health outcomes. There are thousands of relevant microbial species, and many interactions among those microbes and with the host. To facilitate understanding of these complex interactions, information about host and microbial physiology, genetics, and metabolism, including interactions must be assembled. We address this technical challenge by harmonizing data in the form of a knowledge graph (KG) of the gut microbiome in disease. We present a KG that integrates enzymatic data of human and over 1,500 microbial proteomes, drawn from UniProt and 8 other reaction, enzymatic, genomic, chemical, pathway and disease oriented resources. We also provide a framework that supports customizable subsets which represent a microbial community of interest. We use a version of the graph constrained by gut microbes known to be correlated with disease that contains over 8 million nodes and 30 million edges. We apply a novel semantic search to identify meaningful mechanistic hypotheses for these microbe-disease relationships. Finally, we demonstrate the predictive capabilities of the KG by using graph embeddings to identify similarities among individual microbial taxa and human disease. This KG is an important enabling technology for automated methods to uncover mechanistic explanations for microbe-disease associations."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:05","Format":"In Person","Speaker":"Alex Clark","Title":"Using ontologies to make bioassay protocols machine readable","Abstract":"Bioassay protocols have lagged other areas of drug discovery in terms of digitization. While molecules and proteins have spawned entire disciplines (cheminformatics and bioinformatics), most archives of assays carried out by companies are siloed away as a combination of bespoke pick lists, plain text, and sporadic links to globally meaningful dictionaries. Published experiments often err on the side of terseness and obfuscation by referring to similar work. This leads to serious challenges to anyone who wants to federate data, or effectively search it, or use it as the basis of any kind of machine learning inference. Reproducibility issues are further confounded by the difficulty of ascertaining whether any two experiments are comparable. Public ontologies can greatly improve the machine readability of assay protocols by virtue of having universal meaning. We will describe an open source project - BioAssay Express - that uses templates to gather and organize ontologies into a coherent user interface for curating data content. We have marked up 4000 assays from PubChem using our templates, plus another 2600 from the DataFAIRy project, using a hybrid automated model\/expert curation workflow. This freely available data can be precisely and rapidly searched as well as used for sophisticated analysis techniques and model building. We have integrated these curation tools into a commercial product in order to make the process of creating marked-up data less work than traditional writeups, with the ultimate goal of making machine readable data the standard practice rather than a post-publication cleanup chore."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"17:05-17:30","Format":"In Person","Speaker":"Ulrike Steindl","Title":"Knowledge graphs in Cancer Genomics: The Case of Mutational Signatures","Abstract":"Mutational Signatures are generated from somatic genomic mutation data based on their sequence context and have been shown to be indicative of various functional changes in cancer patients._x000D_\nStudying cancer biology using mutational signatures is an emerging field of research. The analyses are continuously being refined. _x000D_\nThe findings generated using this approach are manifold, making it challenging to make draw conclusions. Due to its distributed nature, integrating knowledge allows for new discoveries._x000D_\nIn this work, we will introduce the Mutational Signature Ontology, an ontology describing the space of mutational signature._x000D_\n_x000D_\nThe Mutational Signature Ontology represents the numeric data of the signatures in COSMIC database version 3.4 (Sondka et al. 2023) and selected metadata. It is implemented as an owl\/rdf knowledge graph, encoding necessary other information regarding the sample used, and other features encoded in the COSMIC dataset._x000D_\n_x000D_\n_x000D_\nWe also integrated are the discoveries based on Alexandrov et al. (2020), which provide a quantificational link between cancer types and mutational signatures._x000D_\nThe tumor, etiology, and treatment classes of the Mutational Signature Ontology have been designed to be interoperable with the National Cancer Institute Thesaurus (NCIT)._x000D_\n_x000D_\nThe Mutational Signature Ontology models relations between mutational signatures, mutations, and localities in the genomic location, which uses concepts from the Gene Ontology (Ashburner et al. 2000) and Sequence Ontology (Eilbeck et al. 2005). _x000D_\n_x000D_\nThe Mutational Signature Ontology and knowledge graph provides missing links in the existing ontology space in oncology. It enables interaction between previously unrelated knowledge spaces and will allow for new predictions."},{"Track":"Bio-Ontologies","Room":"522","Weekday":"Sunday","Date":"14 July","Timespan":"17:30-18:00","Format":"In Person","Speaker":null,"Title":"COSI Closing Remarks","Abstract":"Speaker Questions and COSI Closing \/ Community Discussion"},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:15","Format":"In Person","Speaker":"Swapnil Sawant","Title":"Building a Future-Proof Resource: The Comprehensive Modernization of TAIR","Abstract":"Since its inception in 1999, The Arabidopsis Information Resource (TAIR) has been an essential global resource for plant biologists, offering curated gene function data for Arabidopsis thaliana from peer-reviewed literature into a \"\"gold standard\"\" annotation set. Our recent modernizationâ€”the first major infrastructure enhancement since 2011â€”aims to improve TAIR's performance, scalability, and accessibility._x000D_\n_x000D_\nThe legacy system, built on a monolithic architecture with large servers, had become costly and inefficient, leading to technical debt and frequent feature breakdowns. To address these issues and keep pace with technological advances, we've modernized TAIR's infrastructure by developing a robust data pipeline, adopting Docker for deployment, and transitioning to a microservices architecture. This includes using lightweight servers, AWS S3 for scalable storage, and Apache Solr for fast search capabilities._x000D_\nThese changes have significantly boosted performance, reducing load times by up to 50% and search times by 70%. This shift not only enhances user experience but also cuts costs by moving away from expensive, large-scale servers, solidifying TAIR's position as a cutting-edge platform for the foreseeable future._x000D_\n_x000D_\nOur presentation will detail the technical challenges and solutions of this upgrade, the integration of modern tools, and our strategies for virtual team collaboration, which has been crucial in this transition. The modernization of TAIR serves as a model for other organizations looking to update their digital resources, demonstrating how leveraging current technology can enhance efficiency, reduce expenses, and improve service delivery."},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"11:15-11:45","Format":"In Person","Speaker":null,"Title":"TBA","Abstract":null},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"11:45-12:20","Format":"In Person","Speaker":null,"Title":"AI and LLMs in cores: How are we using them now?","Abstract":"A panel discussion around the adoption and use of LLMs and AI within bioinformatics core facilities or similar settings. What works, what doesn't? Practical advice and best practices."},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-14:50","Format":"In Person","Speaker":"Francesco Lescai","Title":"Streamlining Bioinformatics Pipelines with Nextflow: A Scalable, Portable, Reproducible, and Collaborative Solution.","Abstract":"Bioinformatics cores face significant challenges, especially when running pipelines across multiple computing environments, including portability, scalability, and reproducibility issues. Nextflow, an open-source workflow manager, offers a comprehensive solution to these pain points. This talk will present the key features and advantages of Nextflow, showcasing its ability to ease the deployment and execution of bioinformatics pipelines across diverse environments, from local clusters to cloud and high-performance computing infrastructures. By leveraging Nextflow's containerisation, cloud-native design, and automated resource management, researchers and core facilities can ensure seamless pipeline execution, reduce costs, and increase collaboration. Nextflow's capabilities for automatic reports generation and quality control enable the tracking of pipeline performance and data quality, ensuring that results are reliable and reproducible. Additionally, Nextflow's flexibility and customization options allow it to respond to the needs of a diverse range of stakeholders. Further value is provided by the nf-core community, which adds standardisation and best-practice pipelines on top of Nextflowâ€™s capabilities, as well as key accessibility features, allowing workflows to be easily executed by both experts and beginners. _x000D_\nWe will demonstrate how Nextflow's adoption can overcome common challenges, such as environment limitations, data management, and resource allocation, ultimately accelerating scientific discovery and improving analysis services. _x000D_\nBy providing a unified and transparent way to manage complex bioinformatics workflows, Nextflow enables bioinformatics cores to focus on their mission: delivering high-quality results and advancing scientific knowledge."},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"14:50-15:10","Format":"In Person","Speaker":"Nikhil Kumar","Title":"Leveraging the NF-Core Framework for sharable institutional Nextflow modules at Memorial Sloan Kettering Cancer Center","Abstract":"Four 10 minute talks about different aspects of pipelines within the context of core facilities or related settings:_x000D_\n_x000D_\nNikhil Kumar (nikhilkumar516@gmail.com) - Leveraging the NF-Core Framework for sharable institutional Nextflow modules at Memorial Sloan Kettering Cancer Center._x000D_\n_x000D_\nDena Leshkowitz (dena.leshkowitz@weizmann.ac.il): UTAP2: User-friendly Transcriptome and Epigenome Analysis Pipeline_x000D_\n_x000D_\nGrace Pigeau (gpigeau@oicr.on.ca): Managing Big Data in a High-Throughput Genomics Pipeline_x000D_\n_x000D_\nGeorge Bell (gbell@wi.mit.edu): Novel Linux-style code helps us all down the road"},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"14:50-15:10","Format":"In Person","Speaker":"Leshkowitz Dena","Title":"UTAP2: User-friendly Transcriptome and Epigenome Analysis Pipeline","Abstract":"UTAP2 empowers researchers to unlock the mysteries of gene expression and epigenetic modifications with ease. This user-friendly, open-source pipeline, built by unit programmers and deep sequencing analysts, streamlines transcriptome and epigenome data analysis, handling everything from sequences to gene or peak counts and differentially expressed genes or genomic regions annotation. Results are delivered in organized folders and rich reports packed with plots, tables, and links for effortless interpretation. Since the debut of UTAP original version [1] in 2019, it has been embraced by thousands of runs at the Weizmann Institute and 118 citations, thus highlighting its scientific contribution. _x000D_\nUTAP2 is available to the broader biomedical research community as an open-source installation. With a single image, it can be easily installed on both local servers and cloud platforms, allowing users to leverage parallel cluster resources (detailed information and installation instructions are provided on our GitHub site: https:\/\/utap2.readthedocs.io\/en\/latest\/)._x000D_\nReference:_x000D_\n1.\tUTAP: User-friendly Transcriptome Analysis Pipelineâ€, BMC Bioinformatics 2019, 20(1):154(PMID: 30909881)"},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"15:10-15:30","Format":"In Person","Speaker":"Grace Pigeau","Title":"Managing Big Data in a High-Throughput Genomics Pipeline","Abstract":"The Genome Sequence Informatics (GSI) team at OICR handled the analysis and processing of over 1.2 petabytes of data in 2023. The resources required to store such large amounts of data are expensive and difficult to manage. Additionally, data processing demands will increase significantly in 2024, with the on-boarding of two new sequencers and migration to larger capacity flow cells. To manage this increased data output we are implementing changes to data tracking and more aggressive data removal._x000D_\nAssays available through the genomics core consist of a distinct set of samples - defined as a case - which are analyzed together, producing a variety of deliverable files and reports.  Once all work on a case is complete, the associated data can be scheduled for deletion. However, data from cases that use our clinical reporting assays, must be retained for two years. To accommodate this, data is automatically backed up over multiple stages to cloud storage before being deleted. First, the cases which are complete and ready for archiving are identified by an automated pipeline operations system. The raw sequence data and any files that are directly used by the clinical report are encrypted and automatically backed-up to a file storage web service. Archive status and metadata are tracked in a local database. If needed, the archive retrieval is straightforward to initiate and the files are recalled for reload into the production pipeline. This allows the team to meet accreditation requirements and ensure data integrity without requiring continually increasing storage capacity."},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"15:10-15:30","Format":"In Person","Speaker":"George Bell","Title":"Novel Linux-style code helps us all down the road","Abstract":"Python, Matlab, and especially R --_x000D_\n  all have code bases that can help you go far._x000D_\nBut for biologists who can't program,_x000D_\n  asking them to try can lead to, \"\"No way, ma'am!\"\"_x000D_\nIn contrast, when sending a biologist to the command line,_x000D_\n  they typically respond, \"\"Sure -- that'd be fine!\"\"_x000D_\nAs a result, getting R and python packages into scripts_x000D_\n  doesn't cause any coding conflicts._x000D_\nTyping the command provides the syntax,_x000D_\n  so then you'll know all the practical facts._x000D_\nWe can recommend libraries like edgeR and DESeq2,_x000D_\n  and give everyone great analytic methods to pursue._x000D_\nAnd specialized figures like UpSet, Sankey, and waterfall,_x000D_\n  can be easily created, even in places like Montreal._x000D_\nWe provide input, output and sample commands_x000D_\n  which are accessible by all -- no one misunderstands._x000D_\nSo try out the scripts on our web site,_x000D_\n  It can increase your efficiency to a new height._x000D_\nhttps:\/\/github.com\/whitehead\/barc\"_x000D_\n"},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"15:30-16:00","Format":"In Person","Speaker":null,"Title":"New technologies in cores: single-cell, spatial, etc. ","Abstract":"How do cores find the time and resources for development on new technologies? Lessons learned and useful tools we have found when managing these new approaches."},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:10","Format":"In Person","Speaker":null,"Title":"Short Talks, Various topics","Abstract":"Patricia Carvajal-LÃ³pez (pati@ebi.ac.uk) - Competency framework profiles to reflect career progression within bioinformatics core facility scientists_x000D_\n_x000D_\nMichael Laszloffy (MLaszloffy@oicr.on.ca) - Dimsum: a dashboard for quality control, project tracking, turnaround time reporting and more_x000D_\n_x000D_\nAliye Hashemi (ahashem@gmu.edu) - Protein Classification Using Delaunay Tessellation"},{"Track":"BioInfo-Core","Room":"525","Weekday":"Sunday","Date":"14 July","Timespan":"17:10-18:00","Format":"In Person","Speaker":null,"Title":"Breakout Groups","Abstract":"Attendees will form breakout groups to discuss topics of interest from the day and other topics suggested by attendees in order to share knowledge and ideas between cores and make connections with others in similar roles. Groups will report back their findings to the larger group."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:40","Format":"In Person","Speaker":null,"Title":"Opening","Abstract":null},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:40","Format":"In Person","Speaker":null,"Title":"When Visualization Meets AI: Exploring Opportunities","Abstract":"Visualization and AI are increasingly intertwined. AI is being employed to enhance visualization design and to simplify interaction with data. Meanwhile, visualization can help people interpret and debug AI models, whether they are AI modelers or end users. Yet opportunities at the intersection of AI and visualization are in their infancy. This talk will explore the opportunity space in two directions: AI-for-VIS and VIS-for-AI, drawing on existing examples including my own research on natural language interaction, visualization recommendations, and medical applications."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Zeynep H. Gümüş","Title":"PRIMAVO: Precision Immune Monitoring Assay Visualization Online","Abstract":"Cancer immunotherapies are revolutionizing clinical practice, yet only a fraction of patients derive clinical benefit, and some experience adverse events. To understand immune markers that impact clinical care, comprehensive data analyses across assay types and patient cohorts are being performed by NCI-supported Cancer Immune Monitoring and Analysis Centers (CIMACs) and Cancer Immunology Data Commons (CIDC). These represent 30+ cancer immunotherapy trials with longitudinal correlative data assayed using harmonized technology platforms. However, there is an unmet need for visual exploration of multi-scale multi-omic datasets. We introduce PRIMAVO, a unified platform that empowers users to visually explore, query, and subset the results of large-scale immune monitoring multi-omics datasets, spanning transcriptomics, proteomics, genomics, metagenomics, and multiplex immunohistochemistry (mIHC). Users can select specific clinical trials and patient subgroups based on key criteria including demographics, tumor, treatment, response and assay characteristics by leveraging interactive bar plots, pie charts, and scatter plots. For user-specified subgroups, PRIMAVO offers tailored visualizations of their multi-assay datasets: CyTOF, Olink, serology and RNA-seq data are represented within interactive heatmaps with on-the-fly filtering, querying, sorting and clustering. From mIHC imaging data, user-selected cell subsets and multiple markers can be explored simultaneously. Interactive oncoprints visually summarize genomic mutations. Developed in direct collaboration with CIMAC-CIDC scientists and utilizing React, TypeScript, Python, and Django frameworks, PRIMAVO provides domain-specific, downloadable, resizable, zoomable, and scrollable charts and figures. Overall, PRIMAVO lowers barriers between complex immune monitoring data in immunotherapy trials and researchers by providing intuitive, fast, and high-quality access to longitudinal multi-scale multi-omics datasets, facilitating research capabilities."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Eric Mörth","Title":"The Best of Both Worlds: Blending Mixed Reality and 2D displays in an Hybrid Approach for Visual analysis of 3D Tissue Maps","Abstract":"We introduce a novel hybrid system that combines a Mixed Reality (MR) stereoscopic view of 3D spatial data with linked views on a conventional 2D display. Our system addresses visualization challenges in spatial biology, focused on examining molecular components within a tissueâ€™s native spatial context, resulting in complex and high-resolution 3D tissue maps. Given the diversity of techniques generating these maps, creating practical visualization tools is crucial. To tackle this, we collaborated closely with experts in spatial biology throughout the design process, conducting iterative development and testing through three case studies. The case studies include 1) single-cell cyclic immunofluorescence (CyCIF) imaging to investigate early melanoma development; 2) lightsheet microscopy of kidney tissue to understand the function of glomeruli, showcasing the insights gained into spatial relationships within kidney structures; and 3) multiplexed immunofluorescence imaging (MxIF) was used to study various structures in kidney tissue, highlighting the benefits of the hybrid approach in controlling channel information and conducting distance measurements. In extending the web-based Vitessce (http:\/\/vitessce.io) framework for single-cell analysis, a tool already familiar to domain experts, with a WebXR spatial view, we ensured usability and integration with existing workflows. A qualitative evaluation of our prototype demonstrated widespread recognition of the hybrid system's value, even among those initially skeptical of MR technology. Insights gathered from user feedback sessions strongly advocate for combining direct hand interaction in MR with traditional mouse input on a 2D display, highlighting the effectiveness of this approach in enhancing user experience and interaction with complex spatial data."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-15:20","Format":"In Person","Speaker":"Sehi L'Yi","Title":"Understanding Visualization Authoring for Genomics Data through User Interviews","Abstract":"Genomics experts use data visualization to extract and share insights from complex and large-scale data sets. The complexity of genomics data analysis, the diversity of users, and the scope of questions necessitate tools that support creating customized visualizations beyond off-the-shelf visualizations for exploratory analysis. Despite a good understanding of visualization use for genomics data exploration, research into these experts' specific authoring needs is limited. Previous work in visualization research has explored various interactive authoring techniquesâ€”such as template editing, shelf configuration, natural language input, and code editorsâ€”assessing their usefulness and the trade-offs between expressiveness and learnability, primarily for statistical visualizations. However, how genomics experts currently author visualizations and which techniques best meet their needs remains unclear. To bridge this gap in understanding, we conducted two connected user studies involving 33 genomics experts. The first consisted of semi-structured interviews (n=20) to understand current visualization authoring practices, followed by exploratory sessions with visual design probes (n=13) to gain insights about user intents and desired techniques beyond the currently supported. We identified key tasks, user characteristics, and current techniques used in tools, and discovered five personas to represent and discuss author diversity. By integrating insights from interviews and exploratory sessions, we further pinpointed task- and persona-specific patterns and used these to discuss design implications for genomics visualization tools. Our findings highlight the need for visualization tools supporting multiple authoring techniques to enhance both learnability and expressiveness, catering to the varied needs of users aiming to create highly customized genomics data visualizations."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-15:20","Format":"In Person","Speaker":"Suzanne Paley","Title":"New BioCyc Visualization Tools for Genome Exploration and Comparison","Abstract":"We introduce two new BioCyc visualization tools for exploring genomes, with the goal of accelerating science by helping users to rapidly navigate and understand complex biological systems. _x000D_\n_x000D_\nThe Genome Explorer is an information-dense genome browser with high-speed semantic zooming. As magnification level increases, features such as promoters, transcription-factor binding sites, and nucleotide and amino-acid sequences become visible. By displaying such features inline, positional relationships are easy to discern visually, and the display can wrap across multiple lines to present a longer genome region in a single screen, providing greater genomic context. A comparative mode displays chromosomes from multiple organisms aligned at orthologous genes. The Genome Explorer includes the capability to select arbitrary sequence regions for export, and provides a tracks mechanism for depicting large-scale datasets against the genome._x000D_\n_x000D_\nThe Comparative Genome Dashboard summarizes and compares predicted functional capabilities across a selected set of organisms, utilizing a hierarchical framework of cellular function based on selected concepts from MetaCyc and GO. Users begin with a one-screen overview consisting of a set of panels for high-level functions such as Biosynthesis, Transport or Central Dogma, with individual plots for subsystems such as Carbohydrate Biosynthesis or DNA Metabolism. Each plot graphs either the numbers of relevant compounds predicted to be synthesized\/degraded\/transported based on pathway prediction algorithms or the numbers of genes annotated to that subsystem. Users can click on any plot for more detailed comparisons of specific synthesized\/degraded\/transported compounds or GO annotations, and thereby rapidly explore specific subsystems of interest."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-15:20","Format":"In Person","Speaker":"Yannis Nevers","Title":"Matreex: compact and interactive visualisation of large gene families","Abstract":"Studying gene family evolution strongly benefits from insightful visualisations. However, the ever-growing number of sequenced genomes is leading to increasingly larger gene families, which challenges existing gene tree visualisations. Indeed, most of them present users with a dilemma: to display complete but intractable gene trees, or collapse subtrees, thereby hiding information. _x000D_\n_x000D_\nWe developed Matreex, a new dynamic tool to scale-up the visualisation of gene families. Matreexâ€™s key novelty is the use of â€œphylogeneticâ€ profiles, i.e. condensed representation of gene repertoires, to minimize information losses when collapsing branches of the gene tree. Moreover, the gene tree is paired with a complementary interactive species tree which facilitates manipulation and exploration of large, heavily duplicated gene families._x000D_\n_x000D_\nMatreex is user-friendly and provides an appealing and intuitive display which makes it ideal for creating easily interpretable figures for teaching and outreach. It can also be used to automate large-scale analyses of presence-absence of multiple gene families. For example, we used it to simultaneously display 22 gene families involved in intraflagellar transport across 622 species, cumulating 5,500 genes in a compact way. Doing this, we were able to report for the first time complete loss of the intraflagellar transport machinery in the myxozoan Thelohanellus kitaue._x000D_\n_x000D_\nMatreex is available on the Python Package Index (pip install matreex), and source code and documentation are available at https:\/\/github.com\/DessimozLab\/matreex."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-15:20","Format":"In Person","Speaker":"Hiruna Samarakoon","Title":"Interactive visualisation of raw nanopore signal data with Squigualiser","Abstract":"Nanopore sequencing measures ionic current during the translocation of DNA, RNA or protein molecules through a nanoscale protein pore. This raw current signal data can be â€˜basecalledâ€™ into sequence information and has the potential to identify other diverse molecular features, such as base modifications, secondary structures, etc. Despite the unique properties and potential utility of nanopore signal data, there are currently limited options available for signal data visualisation. To address this, we have developed Squigualiser, a toolkit for intuitive, interactive visualisation of sequence-aligned signal data, which currently supports both DNA and RNA sequencing data from Oxford Nanopore Technologies (ONT) instruments. A series of methodological innovations enable efficient alignment of raw signal data to a reference genome\/transcriptome with single-base resolution. Squigualiser generates an interactive signal browser view (HTML file), in which the user can navigate across a genome\/transcriptome region and customise the display. Multiple independent reads are integrated into a signal â€˜pileupâ€™ format and different datasets can be displayed as parallel tracks to facilitate their comparison. Squigualiser provides the most sophisticated framework for nanopore signal data visualisation to date and will catalyse new advances in signal analysis. We provide Squigualiser as an open-source tool for the nanopore community: https:\/\/github.com\/hiruna72\/squigualiser"},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-15:20","Format":"In Person","Speaker":"Devin Lange","Title":"Aggregate Annotated Single-Cell Heatmap Visualizations","Abstract":"Heatmaps are commonly used in the biomedical field to visualize tabular data. In the single-cell domain, heatmaps are frequently used to visualize cell counts of phenotypes (rows) across samples (columns). Clinical applications may involve thousands or even tens of thousands of samples. With modern clustering techniques, several hundred to thousands of phenotypes can be detected. Additionally, each sample and phenotype can be associated with various metadata properties that are necessary for sense-making. As datasets grow, increasing the number of rows and columns in the heatmap can become overwhelming due to limitations in screen size and human perception. Fortunately, there is often redundancy within phenotypes and samples. For instance, two phenotypes that only differ by a single marker (e.g., a surface protein or gene) could be considered the same if that marker is immaterial. Similarly, two samples may be so alike that they could be combined without a noticeable difference in the emerging pattern. In other words, the data can often be aggregated. We introduce a visualization technique that supports displaying annotated heatmaps at different levels of detail. This framework provides support for constructing complex interactive collapsible heatmaps while allowing enough customization to support different use cases."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Devin Lange","Title":"Aardvark: Composite Visualizations of Trees, Time-Series, and Images","Abstract":"How do cancer cells grow, divide, proliferate and die? How do drugs influence these processes? These are difficult questions that we can attempt to answer with a combination of time-series microscopy experiments, classification algorithms, and data visualization. _x000D_\nHowever, collecting this type of data and applying algorithms to segment and track cells and construct lineages of proliferation is error-prone; and identifying the errors can be challenging since it often requires cross-checking multiple data types. Similarly, analyzing and communicating the results necessitates synthesizing different data types into a single narrative._x000D_\nState-of-the-art visualization methods for such data use independent line charts, tree diagrams, and images in separate views. However, this spatial separation requires the viewer of these charts to combine the relevant pieces of data in memory._x000D_\nTo simplify this challenging task, we describe design principles for weaving cell images, time-series data, and tree data into a cohesive visualization. Our design principles are based on choosing a primary data type that drives the layout and integrates the other data types into that layout. We then introduce Aardvark, a system that uses these principles to implement novel visualization techniques. Based on Aardvark, we demonstrate the utility of each of these approaches for discovery, communication, and data debugging in a series of case studies."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Komlan Atitey","Title":"Boosting Data Interpretation with GIBOOST to Enhance Visualization of High-Dimensional Data","Abstract":"Effective visualization of current biomedical data from high-throughput technologies is essential for proper interpretability of complex biological processes but challenging due to its high-dimensionality, increasing volume and underlying biological complexity. This requires advanced dimensionality reduction methods (DRM) like t-SNE, UMAP, PHATE for optimal data reduction. However, different DRM may produce varied outputs, hindering consistent interpretation as well as making benchmarking challenging. Recently, we published MIBCOVIS, a Bayesian framework integrating five robust metrics with different features to enhance visualization and interpretability of high-dimensional data without relying on ground truth. We demonstrated that each visualization tool uniquely optimizes specific features and no tool was able to optimize all features jointly. Leveraging on this observation, we propose GIBOOST, a new visualization tool aimed at synergizing information from disparate sources for optimal data reduction and visualization. Given high-dimensional data, GIBOOST uses an optimized integrative autoencoder to integrate and select the two best data reduction methods from a pool of methods with the maximum additive clustering sensitivity effect condition on other visualization features. We apply GIBOOST to enhance the clustering separability of four distinct dynamic biological processes; EMT, Spermatogenesis, induced stem-cell pluripotency and placenta development. From all datasets, on average GIBOOST improved clustering sensitivity by 76% compared to the individual methods."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Xiaocheng Zeng","Title":"Unveil Cis-acting Combinatorial mRNA Motifs by Interpreting Deep Neural Network","Abstract":"Cis-acting mRNA elements play a key role in the regulation of mRNA stability and translation efficiency. Revealing the interactions of these elements and their impact plays a crucial role in understanding the regulation of the mRNA translation process, which supports the development of mRNA-based medicine or vaccines. Deep neural networks (DNN) can learn complex cis-regulatory codes from RNA sequences. However, extracting these cis-regulatory codes efficiently from DNN remains a significant challenge. Here we propose a method based on our toolkit NeuronMotif and motif mutagenesis, which not only enables the discovery of diverse and high-quality motifs but also efficiently reveals motif interactions. By interpreting deep-learning models, we have discovered several crucial motifs that impact mRNA translation efficiency and stability, as well as some unknown motifs or motif syntax, offering novel insights for biologists. Furthermore, we note that it is challenging to enrich motif syntax in datasets composed of randomly generated sequences, and they may not contain sufficient biological signals."},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"17:00-18:00","Format":"In Person","Speaker":"Fritz Lekschas","Title":"The Insightâ€™s in the Details: Challenges and Opportunities in Visually Exploring High-Dimensional BioMedical Data","Abstract":null},{"Track":"BioVis","Room":"521","Weekday":"Sunday","Date":"14 July","Timespan":"17:00-18:00","Format":"In Person","Speaker":null,"Title":"Award Ceremony and Closing","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":null,"Title":"Opening Remarks","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":null,"Title":"Open Bioinformatics Foundation Update","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":null,"Title":"CoFest Intro","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":null,"Title":"Platinum and Gold Sponsor Videos","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"Paul Pavlidis","Title":"Gemma: Curation, re-analysis and dissemination of 18,000 gene expression studies","Abstract":"Gemma (https:\/\/gemma.msl.ubc.ca) is an open source and open data project focused on increasing the utility of existing gene expression studies. While data repositories like the Gene Expression Omnibus (GEO) and other secondary sources derived from it have important use cases, Gemma goes further than most in curating and re-analyzing data sets. Here we highlight recently software features, annotations and data with the hope of engaging the genomics and bioinformatics community in using and improving Gemma as a resource for computational analyses and biological discovery. Gemma currently contains over 18,000 data sets (>600,000 samples), manually curated using formal ontologies, re-processed and batch-corrected, quality-controlled and subjected to differential expression analysis using multivariate linear models, covering approximately 40% of all human, mouse and rat transcriptome samples from GEO. Our holdings are especially strong in studies of the nervous system (~4,900 studies), perturbations of transcription regulators (~2,200), and small molecule (drug-like) exposures (>1,000). All data and analysis results are available through a web API, or via supporting R\/Bioconductor and Python packages. We also introduce a new data browser that advances usersâ€™ ability to quickly identify and access data sets of interest. We will discuss examples of insights and applications from Gemma such as â€œgenericâ€ patterns of differential expression and observations on data quality, and ongoing work to incorporate single-cell data. In summary, Gemma adds extensive value to existing gene expression studies for the benefit of the research community."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Ida Shinder","Title":"EASTR: Identifying and eliminating systematic alignment errors in multi-exon genes","Abstract":"Accurate RNA-seq alignment to reference genomes is fundamental to transcript assembly, annotation, and gene expression studies, integral to advancements in both biomedical research and basic sciences. Splice-aware aligners, tools pivotal for interpreting RNA-seq data, are now recognized through our research to introduce systemic alignment errors. Specifically, widely-used tools like STAR and HISAT2 often misalign sequences from repeat regions, resulting in spurious spliced alignments. These misalignments have propagated into reference annotations, leading to the inclusion of numerous 'phantom' intronsâ€”artifacts erroneously identified as genuine intronic sequencesâ€”in gene databases relied upon by the scientific community._x000D_\n_x000D_\nEASTR (Emending Alignments of Spliced Transcript Reads), our newly developed tool, significantly improves RNA-seq data analysis reliability. EASTR effectively detects and removes erroneous spliced alignments by examining the sequence similarity between the_x000D_\nflanking upstream and downstream regions of an intron and the frequency of their sequence occurrence in the reference genome. Our analysis of various RNA-seq datasets aligned with HISAT\/STAR, reveals that, in some datasets, up to 20% of splice-aligned reads are spurious. By removing these spurious spliced alignments, EASTR enhances the reliability of downstream analyses that depend on accurate spliced alignment, such as transcript assembly. Applying EASTR to preprocess alignments before transcript assembly with StringTie2 significantly improves transcript assembly precision and sensitivity across various species and experimental setups. Moreover, applying EASTR to reexamine gene catalogs has revealed a multitude of intronic sequences that were previously incorrectly annotated, improving the precision of annotations and equipping the research community with more accurate gene catalogs."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Jeffrey Roskes","Title":"ROC Picker: propagating statistical and systematic uncertainties in biological analyses","Abstract":"We present the first version of ROC Picker, a software package for propagating statistical and systematic uncertainties in a biomedical analysis.  The confidence bands are displayed on the ROC curve and can be used to assess the confidence of the discriminantâ€™s power to separate between two types of samples.  In contrast to previous approaches, which only include the statistical error on the number of samples in constructing the ROC curve, ROC Picker uses a likelihood-based approach that can handle sample-wise uncertainties, including systematic effects (for example, batch-based errors) that are correlated between a subset of the samples.  ROC Picker is modeled on the Combine tool by the CMS Collaboration in experimental particle physics, and is developed as part of the AstroPath project, which applies data processing and analysis methodologies from astronomy into pathology.  We plan to further develop ROC Picker to handle various other metrics (such as Kaplan-Meier curves) and other types of uncertainties applicable to biomedical analyses."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"Live Stream","Speaker":"Iain Bancarz","Title":"Djerba: Sharing and Updating a Modular System for Clinical Report Generation","Abstract":"Djerba is an open-source software package designed to streamline the translation of bioinformatic pipeline output from individual tumor samples into clinical reports. Operating within a CAP\/CLIA\/ACD accredited laboratory, use cases include clinical genome and transcriptome sequencing for therapeutic assignment or trial eligibility, cell-free DNA sequencing using targeted panels for early cancer detection and plasma whole genome sequencing to detect minimal residual disease._x000D_\n_x000D_\nDjerba integrates output from multiple bioinformatics workflows, external resources queries, and customized user inputs; generates plots, tables, hyperlinks and summary statistics; collates its results into a machine-readable JSON document; and finally renders its output as a PDF report for use by clinicians. It includes mini-Djerba, a pre-built, self-contained tool to simplify manual updates to reports. Djerba has a flexible schema for report syntax with automated validation, and supports uploading JSON report documents to a CouchDB database. This enables reports to be automatically searched, compared, and summarized, and provides a valuable resource for better understanding of clinical datasets._x000D_\n_x000D_\nThe modular structure of Djerba enables it to be easily shared, updated and customized while retaining interoperability between sites. Reproducible research is enabled by versioning components and automatically recording input parameters. Partner sites can run a set of analysis and reporting functions appropriate to their needs; while improvements are efficiently tested and integrated into the reporting framework. Examples include deployment to multiple institutions across Canada as part of the MOHCCN-O initiative; and rapidly updating an accredited clinical report with improved copy number variation (CNV) analysis."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":null,"Title":"Q&A For Flash Talks","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Idowu Olawoye","Title":"Antimicrobial resistance prediction of nontuberculous mycobacteria from whole genome sequence data","Abstract":"Nontuberculous mycobacteria (NTM) are opportunistic pathogens, predominantly causing pulmonary infections. NTMs are naturally resistant to many antibiotics, with treatment failure common due to acquired antimicrobial resistance (AMR). Whole-genome sequencing (WGS) offers an avenue for highthroughput detection of mutations and genes associated with drug resistance. Furthermore, there is the need develop bioinformatics tools and curated databases to enable detection of NTM resistance._x000D_\n\tIn this research, we have expanded the databases of ResFinder to accommodate positional mutations and resistance genes of clinically relevant NTMs (M. abscessus and M. avium) from peer-reviewed literature, that were confirmed with phenotypic susceptibility tests. The pipeline uses sequence data (FASTQ) or genome assemblies (FASTA) to predict AMR results. Accuracy was examined by the detection of known mutations using both FASTQ and FASTA inputs. _x000D_\n\tThe ResFinder database additions were successfully validated with raw data and genome assemblies of M. abscessus (n=57) and M. avium (n=58). Comparing the detection of AMR genes and known mutations in  sequence files versus genome assemblies, we observed a specificity of 100%  for both data formats, while we saw a sensitivity of 98.2% (56\/57) for FASTQ versus 100% (57\/57) for FASTA files in M. abscessus and sensitivity of 100% (58\/58) for FASTQ versus 87.9% (51\/58) for FASTA files in M. avium. _x000D_\n\tOur expansion of the ResFinder database comes with much promise in the advancement of in silico prediction to detect AMR particularly in pathogens such as NTMs. It also creates the opportunity for continous development by including AMR markers for other clinically relevant NTMs."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Vedat Yilmaz","Title":"Open2C: Advancing 3D and functional genomics research","Abstract":"Open2C (https:\/\/open2c.github.io) is a collaborative, open-source software community focused on advancing research in genome architecture and chromosome biology through computational 3D and functional genomics. Our libraries primarily leverage the scientific Python software stack and are designed to be user-friendly, flexible, scalable, and interoperable with the wider ecosystem in order to meet the demands of contemporary genomic research. Our resources aim to promote a deeper understanding of genome structure and function, accommodating large next-generation sequencing datasets from Chromosome Conformation Capture (3C\/Hi-C) technologies. Among the packages we maintain are four notable NumFOCUS-affiliated projects: Cooler (Abdennur et al, Bioinformatics, 2020), a Python library and sparse, HDF5-based storage format for genomic contact maps; Pairtools, a command-line suite for interpreting and extracting contact information from 3D genomic sequencing data; Cooltools (Open2C et al, PLOS Comp Bio, in press), a suite for analyzing genomic contact maps; and Bioframe (Open2C et al, Bioinformatics, 2024), a library for genomic interval operations on Pandas dataframes. In its commitment to open science, Open2C has been selected as a host organization for Google Summer of Code 2024, to help mentor the next generation of open-source developers and contributors. All of our packages are open-source and permissively licensed and the source code can be found at https:\/\/github.com\/open2c."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Stephanie Hao","Title":"A Framework for DNA Binding Motifs Prediction for Nontraditional Model Organism Transcription Factors","Abstract":"To improve our predictions of phenotypic outcomes, it is essential to unravel the complex networks of genes and regulatory elements across different species. Consequently, research focusing on the genomics of nontraditional model organisms is advancing rapidly. A bottleneck in non-model organism research is the lack of well-mapped transcription factors to DNA binding motifs. To this end, we implemented a modular prediction algorithm that deciphers DNA binding motifs from the DNA sequences of expressed transcription factors (TFs). We successfully applied this algorithm to 518 sequenced transcription factors of L. variegatus, a long-standing model organism for embryonic development. The modular nature of the algorithm affords species-agnostic applications across all organisms of interest, thereby unlocking new frontiers in genomic prediction, model and non-model organisms alike, and biological understanding."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Xi Zhang","Title":"Bioinformatics tools for comparative genomics analysis of highly similar duplicate genes in eukaryotic genomes","Abstract":"Gene duplication plays an important role in evolutionary mechanism, which can act as a new source of genetic material in genome evolution. However, detecting duplicate genes from genomic data can be challenging. Various bioinformatics resources have been developed to identify duplicate genes from single and\/or multiple species. Here, we developed a Basic Local Alignment Search Tool (BLAST)-based web tool (HSDFinder) and database (HSDatabase), allowing future researchers to easily identify intra-species gene duplications with their own interest genomes by following a pipeline (HSDecipher). Besides, we reviewed recent advancements of gene duplication detection tools and summarized the metrics used to measure sequence identity among gene duplicates within species, which is a quick reference guide for research tools used for detecting gene duplicates."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Q&A For Flash Talks","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"14:20-15:20","Format":"In Person","Speaker":"Mélanie Courtot","Title":"The Data Shows We Need Better Data","Abstract":"Big data, AI, LLMsâ€¦ do they live up to the hype? In a bright and hopeful future, AI accelerates progress, revolutionizes healthcare, alerts us to health risks, and creates fresh career paths. Yet, in a bleaker outlook, it obliterates jobs, fosters rampant misinformation and increases inequity._x000D_\n_x000D_\nAt the root of AI is the data it relies on. In this talk we will discuss how to steer the course by improving the data AI leverages. We will explore the vast ecosystem formed by data, projects and infrastructure. We will travel along different axes to think about the data we are generating and using every day. We will consider data governance â€“ where does it come from, who owns it, and how can we access it? We will investigate open data â€“ how can we leverage health care knowledge for research? Finally, we will share a few thoughts about data quality and data sharing to increase reproducibility and reuse._x000D_\n<p>_x000D_\nDr MÃ©lanie Courtot is the Director of Genome Informatics at the Ontario Institute for Cancer Research in Toronto, and an Assistant Professor in the Department of Medical Biophysics at University of Toronto. Dr Courtot is passionate about translational informatics â€“ building intelligent systems to gain new insights and impact human health. Her lab aims to build a globally shared knowledge ecosystem to advance science and improve health for all. Her team develops the Overture open source software suite, which supports many active large-scale cancer genomics projects including ICGC and ICGC-ARGO, VirusSeq, and the upcoming Pan-Canadian Genome Library. It also drives the African Pathogen Data Sharing and Archive Platform._x000D_\n<p>_x000D_\nDr Courtot obtained her PhD in Bioinformatics from the University of British Columbia in 2014, followed by a postdoctoral fellowship in Public Health. Dr Courtot co-leads the Clinical and Phenotypic workstream and Data Use and Cohort representation groups for the Global Alliance for Genomics and Health (GA4GH) as well as cohort harmonization efforts for the International HundredK+ Cohorts Consortium. She is an advisory board member for the Public Health Alliance for Genomic Epidemiology coalition, European Open Science Cloud for Cancer project and the eLwazi open data science platform."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Mitchell Shiell","Title":"Creating an open-source data platform.","Abstract":"At BOSC 2023, we were excited to introduce Overture, a collection of open-source software used to overcome significant obstacles in storing, managing, and sharing genome-scale datasets. We spoke about our modular and scalable microservice architecture comprising Ego, Song, Score, Maestro, and Arranger and how these services help power VirusSeq, ICGC-ARGO, and the original ICGC portal. We discussed Overtures Core capabilities and how they are split between data consumers, who retrieve data from our platforms; providers, who submit data to our platforms; and administrators, who manage users, configure data models and customize the portal interface.Â _x000D_\n_x000D_\nIn this talk, weâ€™d like to touch on our experience extracting our existing infrastructure and making it work in an open-source environment. Weâ€™d then like to show you what it takes to create a data portal, guiding you through a high-level overview of how our platforms get deployed. This will be followed by a more detailed discussion of using the platform, specifically supplying your data model to it, validating and submitting data to it, and configuring its front-end interface. To conclude, we will discuss future aims, highlighting our feature roadmap and opening up to further discussion on the value of our aims with the BOSC community. We invite you to check out our website and demo portal (https:\/\/demo.overture.bio\/) and chat with us at BOSC 2024. If you wish to contact us remotely our Slack channel, like our software, will always be open and available."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Justin Richardsson","Title":"Going Viral: The Development of the VirusSeq Data Portal","Abstract":"Tracking the evolution and spread of the COVID-19 virus prompted a rapid global initiative to sequence SARS-CoV-2 genomes. However, challenges arose concerning the organization and dissemination of this data, particularly in meeting FAIR standards (Findable, Accessible, Interoperable, and Reusable). In response, we were tasked with spearheading efforts to build and deploy an open-access data platform that would harmonise, validate, and automate submissions of all Canadian SARS-CoV-2 sequences to international databases. Having created data infrastructure for many major cancer genomics projects, this shift represented an opportunity for our group to extend our tools and expertise toward building a pathogen-based data platform. _x000D_\n_x000D_\nWe built the VirusSeq data portal using five open-source Overture software microservices (Ego, Song, Score, Maestro, and Arranger), along with five additional custom-made services, and the third-party OAuth service Keycloak. By using existing software designed for reuse, we were able to successfully build and deploy the platform in a record four weeks. Furthermore, the portal's modular architecture enabled the platform's servers to scale dramatically. From the platform's inception in 2020, it was expected to harmonise and validate the submission of 150,000 viral sequences; by 2024, it has reached over 500,000 sequences._x000D_\n_x000D_\nThe VirusSeq project stands as visible evidence of the power of collaborative effort, modular design, and the reusability of software tools. The portal implementation has already been utilised in other projects to track pathogen genomics, and serves as a testament of our ability to combat outbreaks and pandemics through mass collaboration and rapid data dissemination._x000D_\n_x000D_\nProject Website: https:\/\/virusseq-dataportal.ca\/"},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"François Belleau","Title":"intermine.bio2rdf.org : A QLever SPARQL endpoint for InterMine databases","Abstract":"This project explores converting biological data from InterMine, an open-source data warehouse, into a knowledge graph accessible via Bio2RDF. This exposes global InterMine data as RDF, enabling SPARQL queries or exploration through the OpenSearch interactive dashboard._x000D_\n_x000D_\nThe approach leverages novel semantic web technologies for Bio2RDF: JSON-LD (lightweight Linked Data format), LinkML (Linked Data modeling language), OpenSearch (search engine and dashboard software), and the scalable QLever SPARQL endpoint._x000D_\n_x000D_\nUsing our pipeline, available at Github (https:\/\/github.com\/intermineorg\/intermine2sparql), we retrieve the data model from the InterMine API and transform it into JSON-LD documents stored in an OpenSearch index. Class definitions in LinkML are used to generate JSON-LD context for RDF conversion. Finally, JSON-LD documents are converted to N-Triples format and loaded into the QLever endpoint._x000D_\n_x000D_\nThe complete data model from nine InterMine model organism instances is available through our services:_x000D_\nOpenSearch REST API: http:\/\/kibio.science _x000D_\nQLever SPARQL endpoint: http:\/\/intermine.bio2rdf.org_x000D_\nLinkML model definitions_x000D_\n_x000D_\nThis project demonstrates the feasibility of converting InterMine data into a JSON-LD. This approach facilitates querying biological data with SPARQL across InterMine instances and integrates it with other LinkML-compatible data sources. OpenSearch APIs empower data scientists with programming tools like Pandas and R, and  with BI software (Superset, Power BI, Tableau) to create insightful visualizations._x000D_\n_x000D_\nIn conclusion, the project will unlock biological data from InterMine for AI. By converting the data, it becomes accessible to Large Language Models, empowering them to analyze vast datasets, identify relationships, and potentially lead to discoveries in biological research."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"Live Stream","Speaker":"Robert T. Giessmann","Title":"Organizing community curation to create an Open database on Thermodynamics of Enzyme-Catalyzed Reactions (openTECR)","Abstract":"openTECR (\"Open database on Thermodynamics of Enzyme-Catalyzed Reactions\") is a database and a community. _x000D_\n_x000D_\nWe create a data collection of apparent equilibrium constants of enzyme-catalyzed reactions, being reliable, open and machine-actionable, with a clear change process to integrate new data and correct errors. We believe that Open Science principles, and specifically Open Data and Open Source are key to achieving our vision. _x000D_\n_x000D_\nThe openTECR database serves computational and experimental scientists in the fields of metabolic engineering, genome-scale metabolic modelling, biocatalysis and related fields by providing curated information. It is used by eQuilibrator as the data basis for making predictions about any possible reaction. _x000D_\n_x000D_\nRecently, we organized an open community curation effort (https:\/\/opentecr.github.io\/invitation-to-curate). We prepared a curation workflow to analyze 278 pages of pages densely packed with tables and textual information. We invited volunteer contributions, and are immensely grateful about 17 volunteers investing almost 100 working hours. _x000D_\n_x000D_\nAt BOSC 2024, I would like to present our initiative and share our lessons learned about organizing successful community curation. I believe that our example can serve as a blueprint for other databases \/ project ideas which require a large amount of working hours. _x000D_\n_x000D_\nWe discovered that key to receiving contributions is to offer very small packages of work and a detailed curation manual. Our smallest task was 3 minutes long and well received._x000D_\n_x000D_\nOur small community (~40 members) shares a mailing list (https:\/\/w3id.org\/opentecr) and a GitHub organization where we store our data and code under open licenses (https:\/\/github.com\/opentecr\/)."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":null,"Title":"Q&A For Flash Talks","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Ann Loraine","Title":"Connecting Integrated Genome Browser to a huge genome database using its open API solves one problem and creates another","Abstract":"Integrated Genome Browser (IGB, pronounced â€œig-beeâ€) is a fast, feature-rich, open-source desktop genome browser thousands of researchers have used to explore and analyze genomic data. To support this user audience, we maintain data delivery Web sites called â€œIGB Quickloadsâ€ that supply IGB with around 60 different reference genome assemblies. IGB can open user-provided genome assembly files, but if their desired assembly already exists in an IGB Quickload, they can avoid this inconvenient work. However, we are finding it increasingly difficult to update these Quickload sites as new assemblies are published. Fortunately, many genome database systems now offer robust computational access to their data. By accessing these computational resources, IGB could show new assemblies without our copying them to a Quickload. To test this idea, we devel-oped a new IGB version that consumes and displays data from one such resource, a JSON-emitting API (application programming interface) from the UCSC Genome Browser system. Now available as an â€œearly accessâ€ version at the BioViz.org Web site, this new IGB version can display more than 200 assemblies visible in the UCSC Browser, along with dozens of annotation and data tracks. This wealth of data has now given us a new problem to solve. The API provides information like track names and data formats, but little about what the data represent. Thus, we face a new form of an old problem in bioinformatics: how do we categorize and label data so that computer programs and people can understand and use it?"},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Carolyn T. Caron","Title":"Collaborating our way to optimal integration between Tripal 4 and JBrowse 2","Abstract":"To meet the diverse needs of their research communities, biological web portals not only strive to make data and their associated metadata accessible and reusable, but also provide tools to help analyze and make connections between the data. Tripal JBrowse extends the functionality of Tripal, an open-source toolkit for building biological web portals, to embed a very popular tool within the community, JBrowse 2. Previous versions of the Tripal JBrowse module were limited to embedding JBrowse into a Tripal site using an iFrame, due to theming collisions between JBrowse and the content management system that Tripal extends, Drupal. This prevented any exchange of data between a Tripal site and a JBrowse instance. In large part due to the JBrowse teamâ€™s reception to our feedback regarding these pain points, JBrowse 2 has been developed in such a way as to make embedding the application into a Drupal site a reality. Furthermore, advancements made in Drupal 9 and 10 have removed barriers to communication with other web frameworks allowing direct interaction with the React backend of JBrowse 2. Amidst upgrading our module to Tripal 4, we now have a working prototype of native embedding for JBrowse 2 within a Tripal site. We are very excited for the possibilities this will now bring for the Tripal 4 community. For example, one feature we aim to implement is to embed a miniature JBrowse in gene content pages provided by Tripal that show the structure and context around that gene."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Colin Diesh","Title":"An integrated environment for browsing 3-D protein structures and multiple sequence alignments in JBrowse 2","Abstract":"Recent advances in protein structure prediction have invigorated research in protein structural biology. To enable the visualization of genomic datasets and protein structures in a unified environment, we created a suite of JBrowse 2 plugins to show 3-D protein structures and multiple sequence alignments (MSAs) alongside the genome browser._x000D_\n_x000D_\nTo display 3-D protein structures, we incorporated Mol* into a JBrowse 2 plugin. Users can right-click a gene of interest in the genome browser, and the app can either (a) automatically locate AlphaFoldDB structures associated with the gene of interest or (b) allow the user to import their own PDB\/MMCIF structure files produced by tools such as ColabFold. The protein sequence encoded by a user selected transcript isoform of the gene of interest is aligned with the sequence from the protein structure file, which allows mouse clicks and mouseovers to show matching positions on the 3-D structure and the genome. _x000D_\n_x000D_\nTo display MSAs, we incorporated react-msaview, our MSA visualization tool, into a JBrowse 2 plugin. Users can select a gene of interest in the genome browser and launch an in-app NCBI BLAST workflow to recruit sequences for a MSA, or open their own MSA data files. The MSA viewer can display a dendrogram alongside the MSA to show hierarchical or phylogenetic grouping of the sequences.  Protein domains and features can be highlighted on the MSA by loading InterProScan results. Like the 3-D protein viewer, mouse clicks and mouseovers show matching positions in the genome browser."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Jiyao Wang","Title":"iCn3D, a Platform to Integrate Structures with Functions and Genomics","Abstract":"With the improvement of structural prediction such as AlphaFold, a challenge is to integrate structures with functions and genomics. We started iCn3D as a 3D structure viewer with annotations such as domains and SNPs {Wang, 2020}, then expanded iCn3D to interaction analysis and mutational analysis both interactively and in command line {Wang, 2022}. Recently we added several new features in iCn3D to integrate structures with functions and genomics. First, iCn3D shows the isoforms and their corresponding exons for the gene related to the protein sequence. Second, iCn3D shows a few new annotations: Post Translational Modification (PTM), transmembrane domain detection, and Immunoglobulin (Ig) domain detection. Third, iCn3D allows users to align proteins based on structure, sequence, or residue mapping. Fourth, iCn3D is not only used in Jupyter Notebook and 3D printing, but also expanded to be used in Virtual Reality (VR) and Augumented Reality (AR). Furthermore, all iCn3D views can be reproduced in a sharable URL or iCn3D PNG image. The source code of iCn3D is at https:\/\/github.com\/ncbi\/icn3d."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":null,"Title":"Q&A For Flash Talks","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Bhavesh Patel","Title":"Codefair: Make Biomedical Research Software FAIR Without Breaking a Sweat","Abstract":"We present codefair, an innovative solution that helps researchers make their biomedical research software Findable, Accessible, Interoperable, and Reusable (FAIR), i.e. optimally reusable by humans and machines. The FAIR Biomedical Research Software (FAIR-BioRS) guidelines provide actionable instructions for making biomedical research software FAIR. While designed to be convenient to follow, we learned that their implementation can still be time consuming for researchers. To address this challenge, we are developing codefair, a free and open source GitHub app that acts as a personal assistant for making research software FAIR. Researchers simply need to install codefair from the GitHub marketplace and proceed with their software development as usual. By leveraging GitHubâ€™s tools such as Probot, codefair monitors activities on the software repository, communicates via GitHub issues, and submits pull requests to help researchers make their software FAIR. Currently, codefair helps with including essential metadata elements such as license file, CITATION.cff metadata file, and codemeta.json metadata file. Additional features are being added to provide support for complying with best coding practices, archiving on Zenodo, registering on bio.tools, and much more to cover all the steps for making software FAIR. By alleviating their burden in the process, we believe codefair will empower and encourage biomedical researchers into adopting FAIR and open practices for their research software. We present here our approach to developing codefair, highlight the current and planned features, and explain how the community can benefit from and contribute to codefair."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Hasindu Gamaarachchi","Title":"An Open-source Ecosystem For Scalable And Computationally Efficient Nanopore Data Processing","Abstract":"Emerging long-read sequencing - recently dubbed â€œNature Method of the Yearâ€ - has now become an important tool in understanding genomics. Nanopore is a major commercially available long-read technologies that offer ultra-long reads with limited capital cost. However, computational aspects of nanopore sequence analysis (e.g., data access, storage, basecalling, methylation calling) are still a burden, impeding the scalability of population-scale experiments. In this talk, I will present a complete ecosystem that enables scale nanopore data analysis in a computationally efficient way, built on top of our file format called S\/BLOW5 (Nature Biotechnology, 2022). S\/BLOW5 reduces computational time by an order of magnitude and additionally reduces storage footprint by ~20-80% compared to existing the FAST5 format. S\/BLOW5 ecosystem which is fully open-source now includes: (i) S\/BLOW5 file format and accompanying specifications (ii) the slow5lib (C\/C++) and pyslow5 (python) software libraries for reading and writing S\/BLOW5 files; (iii) the slow5tools toolkit for creating, converting, handling and interacting with SLOW5\/BLOW5 files (Genome Biology 2023); and (iv) a suite of open source bioinformatics software packages (including basecalling and methylation calling tools) with which SLOW5 is now integrated (Bioinformatics 2023, GigaScience 2024). The research community has already started building on top of S\/BLOW5 and slow5-rs which allows S\/BLOW5 access using the Rust programming language is an example. S\/BLOW5 will continue to prioritise performance, compatibility, usability and transparency. S\/BLOW5 for nanopore signal space is analogous to the seminal SAM\/BAM  formats in the base-space that bioinformaticians are familiar with, thus making the adoption of S\/BLOW5 seamless."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Avishai Weissberg","Title":"GenomeKit, a Python library for fast and easy access to genomic resources","Abstract":"GenomeKit is Deep Genomicsâ€™ high performance Python library for fast and easy access to genomic resources such as sequence, data tracks, annotations, and variants._x000D_\nGenomeKit has been in use internally by ML & data scientists and bioinformaticians at Deep Genomics for several years, and we have decided to make it available to the rest of the community. GenomeKit serves as the computational foundation for the data generation and evaluation of  the recently published BigRNA foundation model, and most other workflows at Deep Genomics._x000D_\nAt its core, GenomeKit allows users to perform computational operations on the genome, like searching, applying variants, and comparing, extracting and expanding intervals. Classes like Genome, Interval, and Variant form the base for most of its APIs._x000D_\n_x000D_\nFor example, GenomeKit allows users to easily get the principal transcript for a particular gene on a specific annotation and patch version of an assembly, accessing interval objects for each of its coding regions, UTRs, exons, introns, etc. These interval objects can further be expanded, intersected, have variants applied to them, etc._x000D_\n_x000D_\nIn addition, GenomeKit includes a variety of APIs to open and process the contents of standard data file types (gff3, fasta, etc). GenomeKit's data formats that are highly optimized and compressed for reduced I\/O and efficient memory utilization._x000D_\n_x000D_\nThis talk aims to cover_x000D_\nthe use cases for GenomeKit,_x000D_\nan overview of the API and main capabilities_x000D_\ntechniques used to achieve GenomeKit's level of performance_x000D_\nbenchmarks comparing GenomeKit with similar libraries"},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":null,"Title":"Q&A For Flash Talks","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Masaki Fukui","Title":"Tataki: Enhancing the robustness of bioinformatics workflows with simple, tolerant file format detection","Abstract":"The increase in data volume in bioinformatics has heightened the demand for robust and reliable workflow analysis. Workflows enable the integration of various analytical tools to perform sequences of analyses in a portable and reproducible manner. However, inconsistencies in file inputs and outputs of workflow components can cause the tools to misidentify file formats of input files and terminate unexpectedly, which decreases the robustness of workflows. _x000D_\n_x000D_\nOne way to resolve this is to introduce a file format detection tool in between tools within workflows. However, current file format identification tools often fail to adequately handle the issues, as they might misidentify files with abnormalities. Additionally, because they are not always standalone components, integrating them seamlessly into workflows can be difficult._x000D_\n_x000D_\nTo enhance workflow robustness, we developed Tataki, a simple command-line file format detection tool, targeting major bioinformatics file formats. It is designed for ease of use within workflows, and also allows users to extend identification criteria using the Common Workflow Language to tolerate file format irregularities such as variance in file formats or those not predefined. We believe Tataki is a practical solution to the file format issues and boosts the productivity of bioinformatics researchers and developers."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"Live Stream","Speaker":"Jayaram Kancherla","Title":"BiocPy: Facilitate Bioconductor Workflows in Python","Abstract":"Bioconductor is an open-source software community that provides a rich repository of tools for the analysis and comprehension of genomic data. One of the main advantages of Bioconductor is the development of standardized data representations and a large number of statistical analysis methods tailored for genomic experiments. These tools allow researchers to seamlessly store, manipulate, and analyze data across various genomic experimental modalities in R. Analysts today use a variety of languages in their workflows, including R\/Bioconductor for statistical analysis and Python for imaging or machine learning tasks. _x000D_\n_x000D_\nInspired by Bioconductor, BiocPy aims to enable and facilitate these bioinformatics workflows in Python. To achieve this goal, BiocPy provides data structures that are closely aligned with Bioconductor's implementations.  These structures include BiocFrame, providing a Bioconductor-like data frame class, and GenomicRanges which aids in representing genomic regions and facilitating analysis. Together they serve as essential and foundational data structures, acting as the building blocks for extensive and complex representations. Notably, container classes such as SummarizedExperiment, SingleCellExperiment, and MultiAssayExperiment cater to the diverse needs of handling single or multi-omic experimental data and metadata._x000D_\n_x000D_\nBy adapting mature Bioconductor data structures to Python, BiocPy offers a seamless transition and ease of use across programming languages, fostering reproducible and efficient genomic data analyses. To our knowledge, BiocPy is the first Python framework to provide well-integrated Bioconductor data structures and representations specifically designed for genomic data analysis, paving the way for enhanced cross-language interoperability in bioinformatics workflows. The BiocPy ecosystem is open-source and available at https:\/\/github.com\/BiocPy."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Peter Amstutz","Title":"Arvados Project Update","Abstract":"Arvados is a comprehensive, mature, open source platform for managing and processing large scale biomedical data on HPC and cloud. By combining robust data and workflow management capabilities in a single platform, Arvados helps researchers organize and analyze petabytes of data and run reproducible and versioned computational bioinformatics workflows.  Since the last time Arvados was presented at BOSC (2022), Arvados has had 3 major releases and 8 minor releases.  This short talk will highlight major improvements including  a more modern, performant interface, expanded workflow capabilities,  and improved data storage and management. Arvados is used in production by some of the largest life sciences companies in the world as well as in academia, and welcomes community participation (https:\/\/arvados.org\/community\/)."},{"Track":"BOSC","Room":"524ab","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":null,"Title":"Q&A For Flash Talks","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"08:40-09:00","Format":"Live Stream","Speaker":"Rayo Suseno","Title":"Enhancing Reproducibility in Immunogenetics: Leveraging Containerization Technology for Bioinformatics Workflows","Abstract":"Bioinformatics is experiencing a crisis of reproducibility, which inhibits research progress and undermines scientific findings. This is driven by a variety of factors, including incomplete documentation, poor version control, lack of accessible code, and incompatible software dependencies. Leveraging containerization technology is a promising solution to address these issues and streamline the deployment of specialized bioinformatics workflows. The field of immunogenetics is especially in need of such workflows, as high levels of genomic complexity characteristic of immune loci require the development of unique tools. For example, in 2021, we published a pipeline, Pushing Immunogenetics to the Next Generation (PING), designed to genotype the killer immunoglobulin-like receptor (KIR) genes from short read data. Due to its various dependencies, however, some investigators found PING challenging to run and install. This prompted us to containerize both PING and a recently developed software from our lab, MHConstructor, a de novo short read sequence assembler for the human major histocompatibility complex (MHC) region. A particular challenge faced by MHConstructor is reliance on multiple Python versions, due to dependencies on different bioinformatics tools. This requires usage of multiple conda environments within one container, which we successfully implemented while ensuring seamless switching between environments. Singularity was chosen due to its user-friendly nature, encapsulating the entire workflow in a single file that can be effortlessly executed regardless of the operating system. The containerization of PING and MHConstructor ensures the reproducibility of these two immunogenetic pipelines, providing reliable high throughput analysis of large datasets not previously accessible with extant tools."},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"09:00-09:20","Format":"In Person","Speaker":"Nezar Abdennur","Title":"Breaking the silo: composable bioinformatics through cross-disciplinary open standards","Abstract":"The practice of data science in genomics and computational biology is fraught with friction. This is largely due to a tight coupling of bioinformatic tools to file input\/output. While omic data is specialized and the storage formats for high-throughput sequencing and related data are often standardized, the adoption of emerging open standards not tied to bioinformatics can help better integrate bioinformatic workflows into the wider data science, visualization, and AI\/ML ecosystems. Here, we present three libraries as short vignettes for composable bioinformatics. First, we present Oxbow, a Rust-based adapter library that unifies access to common genomic data formats by efficiently transforming queries into Apache Arrow, a standard in-memory columnar representation for tabular data analytics. Second, we present Bioframe, a Python library that performs genomic range operations using standard Pandas dataframes. Last, we present Anywidget, an architecture based on modern web standards for sharing interactive visualizations across all Jupyter-compatible runtimes, including JupyterLab, Google Colab, and VSCode. Together, we demonstrate the composition of these libraries to build a custom connected genomic analysis and visualization environment. We propose that components such as these, which leverage scientific domain-agnostic standards to unbundle specialized file manipulation, analytics, and web interactivity, can serve as reusable building blocks for composing flexible genomic data analysis and machine learning workflows as well as systems for exploratory data analysis and visualization."},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Luis Pedro Coelho","Title":"For long-term sustainable software in bioinformatics: a manifesto","Abstract":"I will discuss the challenges of maintaining research software in bioinformatics, especially considering the transient nature of funding and the turnover of researchers involved in coding projects. I will also discuss the approaches that my group takes to ensure that we maintain our software over the long-term, even as trainees leave the group. Maintenance involves ensuring the software performs as described. This includes updating it to handle new dependencies and fix bugs as well as providing a modicum of support to users._x000D_\n_x000D_\nI categorize research software into three levels: Level 0 (one-off scripts for specific analyses, often containing minor errors), Level 1 (Extended Methods Code, supporting specific results in publications), and Level 2 (Tools intended for broad use, requiring robustness and extensive documentation). Both Level 1 and Level 2 are made public, but they serve different purposes and upgrading from 1 to 2 involves significant effort. In the case of Tools, we aim for ease of use, reproducibility, and good error reporting._x000D_\n_x000D_\nWe follow several practices that facilitate maintenance and support: reproducible research techniques, \"dogfooding\" (using one's own tools), clear and public support channels, providing error messages that guide users to solutions, and distributing software via Bioconda to minimize installation issues. Additionally, we attempt to gather beta users before publication and improve software based on feedback._x000D_\n_x000D_\nFurthermore, I propose that journals should require a Maintenance and Support statement from authors, similar to the Data Availability statement, to ensure transparency and accountability regarding the long-term support of research software._x000D_\n"},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Jonathon Keeney","Title":"BioCompute: A Descriptive Standard for Computable Metadata","Abstract":"Scientific review of work in the life sciences has been hindered by a lack of standards relating to the communication of computational pipelines. Often, little or no information related to the computational component is described in detail, rendering the work unreviewable, unfindable, and not reproducible. This challenge has been felt particularly concretely in reviews for academic publishing and in regulatory reviews at regulatory agencies such as the US FDA. BioCompute is a descriptive standard (officially \"IEEE 2791-2020\") that is flexible enough to accommodate any pipeline but robust enough to provide a computable structure for metadata and annotation of the pipeline. The standard is supported by several major bioinformatics platforms, and two workflow languages, as is the only framework standard of its kind accepted by the FDA for regulatory reviews. This presentation will describe the need and architecture of the standard, the community, and the tools that have been developed to work with the standard. URL: https:\/\/biocomputeobject.org\/"},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Alberto Pepe","Title":"Breaking Down Research Silos and Fostering Radical Collaboration through Collective Intelligence","Abstract":"The data landscape is rapidly expanding. Scientists are required to navigate increasing amounts of multimodal data in an attempt to create high-quality, impactful research. As knowledge expands, specialization increases. This comes at a cost: the emergence of deep data silos within separate disciplines. We believe that progress lies in the open exchange of ideas between all stakeholders, harnessing our collective human and Artificial Intelligence (AI). By bringing these communities together and fostering unanticipated connections, we can drive a new age of biomedical innovation. In this talk, we present two ongoing projects at Sage Bionetworks that underscore the need for open approaches to AI\/ML to pave the next generation of biological and medical innovations._x000D_\n_x000D_\n(Extended abstract attached)"},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":null,"Title":"Q&A For Flash Talks","Abstract":null},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"In Person","Speaker":"Lacey-Anne Sanderson","Title":"Tripal: a community-driven framework supporting open science, sustainable data web portals","Abstract":"As the open science movement gains momentum, UNESCO is highlighting the need for infrastructure to (1) support building of global, inclusive research communities and (2) provide open access to research-associated data and tools. Open source bioinformatic software, like Tripal, is uniquely poised to fill such a need as both open-source and open-science embody the same core principles. Tripal (https:\/\/tripal.info) extends several open-source packages into a cohesive platform meant to make development of open science data web portals accessible. Specifically, Drupal provides user management, page templating, content curation, and site administration, while GMOD Chado provides community-developed standards for storage of biological datasets. Tripal 4 currently offers ontology-driven data pages, extensive administrative and curation interfaces, and standards-focused data importers. Sites are fully customizable through various web forms and extensive developer APIs provided by Drupal and Tripal. While there are still some key integrations outstanding, we are now at the point of expanding the default configuration to guide community builders in creating inclusive and open data portals. Our first step on this path is to provide a set of well documented default fields designed to promote high standards of data attribution and completeness of metadata. These fields are based on input and experience from our existing international community of Tripal data portals. Additionally, we are hoping to engage those in the wider open-source and open-science communities in collaboration. Please reach out to us in person at the BOSC cofest, on Github, on Slack or in our weekly cofests on GatherTown (see https:\/\/tripal.info\/community)."},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:40","Format":"In Person","Speaker":"Andrew Su","Title":"Open Data, Knowledge Graphs, and Large Language Models","Abstract":"Bioinformatics is the science of collecting, storing, analyzing, and disseminating biological data and information. As in most domains of data science, bioinformaticians have long focused on structured data â€“ information that is represented using ontologies and controlled vocabularies in well-defined data formats and often stored in databases with predefined schemas. This focus on structured data over the last 30 years has been the most efficient way to convert information into testable hypotheses and new scientific insights._x000D_\n_x000D_\nRecent developments in artificial intelligence, particularly the advent of large language models (LLMs), have started to challenge this traditional focus on structured data. By utilizing massive training sets of unstructured text, LLMs have shown exceptional capabilities not only in tasks like question answering and text generation but also in summarization, translation, and code generation. In this presentation, we will examine how LLMs are changing and will continue to change the practice of bioinformatics, particularly at the interface between structured and unstructured data._x000D_\n_x000D_\nAndrew Su, Ph.D., is the Elden and Verna Strahm Professor at the Scripps Research Institute in the Department of Integrative Structural and Computational Biology (ISCB). Dr. Su earned his PhD in chemistry at Scripps Research in 2002, and was the Associate Director of Bioinformatics at The Genomics Institute of the Novartis Research Foundation (GNF) before returning to Scripps Research as a faculty member in 2011._x000D_\n_x000D_\nThe Su lab focuses on building and applying bioinformatics infrastructure for biomedical discovery. Dr. Su has had a long-standing interest in leveraging crowdsourcing to organize and integrate knowledge though projects like the Gene Wiki and Wikidata. In partnership with Chunlei Wuâ€™s lab, he has also worked extensively on creating biomedical APIs and enabling API interoperability through the BioThings project. Most recently, his lab has a particular emphasis on constructing and mining knowledge graphs for drug repurposing. In all this work, the Su lab has embraced the principles of open science, open data, and open source software."},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Marcin Joachimiak","Title":"Gene Set Summarization Using Large Language Models","Abstract":"Molecular biologists often use statistical enrichment analysis to interpret gene lists derived from high-throughput experiments and computational analyses. This traditional method assesses the over- or under-representation of biological function terms associated with genes, based on curated assertions from databases such as Gene Ontology (GO). Alternatively, interpreting gene lists can be conceptualized as a textual summarization task, where Large Language Models (LLMs) utilize scientific texts to reduce reliance on traditional knowledge bases. This approach offers advantages because traditional knowledge bases struggle to scale their curation and integration efforts, while being unable to encompass all available knowledge._x000D_\nOur tool, TALISMAN (Terminological ArtificiaL Intelligence SuMmarization of Annotation and Narratives), employs generative AI to perform gene set summarization, complementing standard enrichment analysis. This innovative approach leverages various sources of gene functional information including: 1) structured text from curated ontological databases, 2) narrative summaries without ontology, and 3) direct model retrieval._x000D_\nLLMs can generate biologically plausible GO term summaries for gene sets but struggle to provide reliable significance scores or rankings and do not match the precision of standard methods. Notably, newer LLM models significantly outperform older versions._x000D_\nWhile these methods are not yet suitable replacements for standard term enrichment analysis, they do offer advantages for summarizing implicit knowledge across large and unstandardized datasets, particularly where the volume of information exceeds human processing capabilities. This, together with the generative capacities of LLMs, such as to suggest novel summarization terms, makes them a valuable tool for improved understanding in complex biological data analyses."},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Hilmar Lapp","Title":"FAIR, modular and reproducible image-based ML workflows for biologists: a template and case study from imageomics","Abstract":"Machine Learning (ML) has become a critical tool in the life sciences, and is being applied to diverse biological data types, including the rapidly growing vast trove of biological image data. Using image-based ML for biological research questions frequently requires combining different ML models and algorithms into complex computational workflows. We present a template for creating FAIR and reproducible workflows for imageomics, an emerging field that uses AI and ML to extract knowledge from biological images. Recognizing the inherently interdisciplinary nature of imageomics, the template distinguishes between a conceptual workflow for interdisciplinary research convergence and using the conceptual workflow to implement an executable application-specific workflow. We present how implementation technology choices can promote research software engineering best practices and enable end-to-end automation, while also accommodating ongoing ML and computer science research, and empowering biologists to make modifications. The results include a conceptual workflow for detecting and quantifying traits from biological specimen images, and a concrete workflow for a dataset of fish museum specimen images. We extended core FAIR data and software practices to ML models, such as persistent identifiers, version control, semantic versioning, and rich metadata. We find that the objective of a FAIR ML workflow promotes all workflow components to be FAIR. Ensuring full reproducibility is a separate step, and achieving workflow reproducibility requires end-to-end automation interoperable between high-performance computing environments, necessitating a formal workflow definition language with an associated workflow manager and execution engine."},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Chris Armit","Title":"Trust and Transparency in Reporting Machine Learning: The DOME-GigaScience Press Trial","Abstract":"Machine learning is increasingly applied to biological and biomedical data, and there is a need for sufficient detail to enable a researcher to understand the machine learning approach used in a research study. This is even more challenging due to Machine Learning studies being inherently difficult to interpret (the so-called â€œblack boxâ€ effect).Â  To throw light on these methods, GigaScience Press (https:\/\/www.gigasciencepress.org\/) has partnered with the DOME Consortium with the goal of encouraging authors to follow the DOME (Data, Optimisation, Model, Evaluation) recommendations._x000D_\n_x000D_\nThe role of the GigaScience DataBase (GigaDB) Data Curation team is to ensure the Data Submission process runs as smoothly as possible. The DOME Consortium has generated the DOME Wizard (https:\/\/dome.ds-wizard.org\/) which enables researchers to submit their DOME annotations to a central repository (https:\/\/registry.dome-ml.org\/) and share them with reviewers. The GigaDB team scans submitted manuscripts for Machine Learning content, and performs checks to ensure that DOME annotations in support ofÂ GigaScienceÂ andÂ GigaByteÂ manuscripts are sufficiently complete._x000D_\n_x000D_\nTo increase the visibility of the supporting DOME annotation, a link to DOME annotation is included in the GigaDB dataset that accompanies aÂ GigaScienceÂ orÂ GigaByteÂ manuscript. The DOME annotations are a great asset to peer review, providing the necessary high-level overview to properly understand a machine learning study. We recommend that other journals follow our example in encouraging DOME annotations to be submitted early in the publication process and prior to peer-review."},{"Track":"BOSC","Room":"524ab","Weekday":"Tuesday","Date":"16 July","Timespan":"14:40-15:40","Format":"In Person","Speaker":null,"Title":"Open Approaches to AI\/ML in Bioinformatics","Abstract":null},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":null,"Title":"Welcome & Overview","Abstract":null},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"11:00-12:20","Format":"In Person","Speaker":"Catherine Lozupone","Title":"CAMDA Keynote: Exploring drivers of gut microbiome compositional differences in disease and mechanistic pathways to recovery using big data","Abstract":"The commensal gut microbiome plays an essential role in protecting against opportunistic pathogens and maintaining immune homeostasis. Dysbiosis, an imbalance in microbial communities, is linked with disease when this imbalance disturbs microbiota functions essential for maintaining health or introduces processes that promote disease. By performing meta-analyses of many studies that have sequenced the 16S ribosomal RNA  gene to characterize gut microbial communities in different disease and health contexts, we have defined very young age and Western versus Developing world\/Agrarian cultures to be two major axes of gut microbiome compositional variation that are important for explaining variability across healthy humans. Interestingly, among Western adults, individuals with different diseases or microbiome disturbances have migration along both of these major axes of health-associated gut microbiome variation. For instance, obese Western individuals sometimes have microbiomes that cluster closer to Prevotella-rich\/Bacteroides-poor microbiome types in the developing world and this is more common in African versus European Americans. Related to age, gut microbiomes of adults with recurrent Clostridioides difficile infection, Inflammatory Bowel Disease, cancer, and intake of broad-spectrum antibiotics all tend to cluster closer to healthy infant gut microbiomes, characterized by low diversity with increased representation of facultative versus strict anaerobes. The relationship between highly disturbed and infant gut microbiome compositions is likely related to parallel processes that occur in primary versus secondary ecological succession, where absence of a complex community of healthy gut commensals allows for the colonization of opportunistic, early succession adapted organism that undergo an ordered turnover of membership. By coupling co-occurrence patterns and longitudinal analyses of dense time-series data with genomic and metabolic network interrogations to explore underlying drivers of microbial cooperation and competition, we have been generating hypotheses regarding important interactions that occur during succession and testing them in humanized mice."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"14:20-14:50","Format":"In Person","Speaker":"Kinga Zielińska","Title":"The Gut Microbiome based Health Index Challenge - Introduction","Abstract":"Microbiome-based disease prediction has significant potential as an early, non-invasive marker of multiple health conditions attributable to dysbiosis of the human gut microbiota, thanks in part to decreasing sequencing and analysis costs. Existing tools, or microbiome health indexes, are often based solely on microbiome richness and are heavily dependent on taxonomic classification. More recently, an ecological approach has led to increased understanding of microbiome, which reveals substantial restrictions of such approaches. In this study, we introduce a new health index created as an answer to updated microbiome definitions. The novelty of our approach is a shift from a traditional approach of phylogenetic classification, towards a more holistic consideration of metabolic function including ecological interactions between species in the effort to distinguish between healthy and diseased states. We compare this to not only the taxonomy-based Gut Microbiome Health Index (GMHI) and the high dimensional principal component analysis (hiPCA)method, the most comprehensive indices to date, but also to taxon- and function-based Shannon entropy and demonstrate a significant improvement to these approaches. We validate our indexâ€™s performance using a variety of complementary benchmarking approaches on datasets representing a range of gut health conditions and showcase the robustness of its superiority over the GMHI and the hiPCA. Overall, we emphasize the potential of this approach and advocate a shift towards functional approaches in order to better understand and assess microbiome health as well as to provide directions for future index enhancements. Our method, q2-predict-dysbiosis, is freely available as a QIIME 2 plugin (https:\/\/github.com\/bioinf-mcb\/q2-predict-dysbiosis)."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"14:50-15:20","Format":"In Person","Speaker":"Rafael Perez Estrada","Title":"Integrating Taxonomic and Functional Features for Gut Microbiome Health Indexing","Abstract":"This study aimed to enhance our understanding of metagenomic datasets by applying and innovating bioinformatics tools for the identification and functional characterization of microbial genes and pathways. We utilized tools such as Prokka, Prodigal, EggNog, mi-faser, Metacyc, and DiTing to annotate gene functions and metabolic pathways, generating a detailed functional landscape of the microbial communities. We identified key functional roles in various health conditions through Pearson-Spearman correlation networks but found a notable absence of keystone functions in several categories. On the other hand, we explored microbial health indicators by replicating indices like GMHI and hiPCA and attempting novel integrations with metabolic pathway data. The adapted GMHI and hiPCA indices could distinguish between health states in microbial communities. Moving forward, we aim to refine these indices using expanded datasets, focusing on both taxonomic and functional data. In conclusion, our study enhances the predictive capabilities of metagenomic analyses for assessing microbial community health, paving the way for future developments in microbial ecology and biomedicine."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Patrick Smyth","Title":"Using Gradient Boosting to Predict Health States from Composition and Function of the Gut Microbiome","Abstract":"This study utilizes stool samples from the Human Microbiome Project 2 and American Gut Project cohorts, along with COVID-19 patient data, to develop a superior health index using machine learning techniques._x000D_\n_x000D_\nWe employed LightGBM's Dropouts meet Multiple Additive Regression Trees (DART) algorithm, which excels in handling high-dimensional data, for predicting health states based on combined taxonomic and functional profiles. Data preprocessing involved filtering features with a minimum prevalence threshold, as well as aggregating taxonomic pathways._x000D_\n_x000D_\nTwo cross-validation strategies, nested stratified 5-fold and Leave-One-Project-Out (LOPO), were implemented to ensure robust model evaluation. Performance metrics such as AUC, F1 Score, and Balanced Accuracy were used to assess model effectiveness. Feature importance analysis identified key taxa and pathways relevant to gut health._x000D_\n_x000D_\nThe Gradient Boost Health Index from gut Microbiome data (GBHIM) was introduced, showing improved performance over existing indices like the Gut Microbiome Health Index (GMHI). The inclusion of GMHI as a feature occasionally enhanced model performance. The model demonstrated strong performance across various validation folds and projects, highlighting its potential for accurate health state predictions._x000D_\n_x000D_\nFor COVID-19 samples, the model effectively distinguished between healthy and non-healthy states, clustering more closely with non-healthy samples in Principal Coordinates Analysis. This study underscores the importance of leveraging comprehensive microbial data and advanced machine learning techniques for improved health state predictions in microbiome research"},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Karwowska Zuzanna","Title":"Microbiome time series data reveal predictable patterns of change","Abstract":"Despite the majority of microbiome studies being cross-sectional, it is widely acknowledged that the microbiome is a dynamic ecosystem. _x000D_\nHere, we analyse how the gut microbiome changes over time as a community, how different bacterial species behave over time, and whether there are clusters of bacteria that exhibit similar fluctuations? _x000D_\nWe show that a healthy human gut microbiome is stationary, seasonal, and non-random. Moreover, we demonstrate that it is self-explanatory to some extent, and its behavior can be predicted._x000D_\nThe analysis of individual bacterial species uncovered the existence of three distinct longitudinal regimes in the healthy human gut microbiome. These regimes consist of 1) stationary and highly prevalent bacteria that exhibit resistance to environmental changes; 2) volatile bacteria that exhibit dynamic reactions to external stimuli, causing their presence to fluctuate over time; and 3) white noise. Clustering analysis revealed the presence of taxonomically diverse bacterial groups that exhibit similar fluctuations over time._x000D_\nIn conclusion, our study highlights the importance of longitudinal data and provides new insights into the dynamics of the healthy human gut microbiome. We offer clear guidelines for clinicians and statisticians who conduct longitudinal studies and develop models to predict the behavior of the gut microbiome over time."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:10","Format":"In Person","Speaker":"Jesse Shapiro","Title":"Prediction in microbiome science","Abstract":"As variation in microbial community structure is implicated in an increasing number of human diseases and environmental changes, there is strong potential for microbiome-based diagnostics and therapeutics. I will discuss three brief case studies, highlighting how (1) diagnosis is easier than forecasting of future disease or environmental perturbations, (2) predicting simpler disease outcomes (e.g. infection or not) is easier than more complex outcomes (e.g. disease severity) that depend on a larger number of host- and microbe-determined factors, and (3) certain disease outcomes are more predictable based on genetic diversity within a key pathogen species than based on microbiome community composition. If these principles prove to be general, we can move toward more realistic expectations for microbiome-driven predictions and use the best combination of data and methods for the task at hand."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"17:10-17:30","Format":"In Person","Speaker":null,"Title":"The Elephant in the Room: Software and Hardware Security Vulnerabilities of Portable Sequencing Devices","Abstract":"Portable genome sequencing technology is revolutionizing genomic research by providing a faster, flexible method of sequencing DNA and RNA. The unprecedented shift from bulky stand-alone benchtop equipment confined in a laboratory setting to small portable devices which can be carried anywhere outside the laboratory network and connected to untrusted computers to perform sequencing raises new security and privacy threats not considered before. Current research primarily addresses the privacy of DNA\/RNA data in online databases and the security of stand-alone sequencing devices such as Illumina. However, it overlooks the security risks arising from compromises of computers directly connected to sequencers. While sensitive data, such as the human genome, has become easier to sequence, the networks connecting to these smaller devices and the hardware running basecalling can no longer implicitly be trusted._x000D_\nHere, we present new security and privacy threats of portable sequencing technology and recommendations to aid in ensuring sequencing data is kept private and secure. First, to prevent unauthorized access to sequencing devices, IP addresses should not be considered a sufficient authentication mechanism. Second, integrity checks are necessary for all data passed from the sequencer to external computers to avoid data manipulation. Finally, encryption should be considered as data is passed from the sequencer to such external computers to prevent eavesdropping on data as it is sent and stored. As devices and technology rapidly change, it becomes paramount to reevaluate security requirements alongside them or risk leaving some of our most sensitive data exposed."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"17:10-17:30","Format":"In Person","Speaker":null,"Title":"Improving genomic epidemiology of Giardia intestinalis with a core genome gene-by-gene subtyping schema","Abstract":"Giardia intestinalis parasites are common causes of sporadic gastroenteritis outbreaks in high-income countries. In contrast, giardiasis is endemic in low-income settings with poor sanitation, where it may cause failure to thrive and chronic malnutrition. Whole genome analyses of this microbe are rare because culturing this parasite is a laborious process with a limited success rate. Hence, subtyping for epidemiological tracking and surveillance relies on a three-loci marker scheme based on PCR amplification of stool samples. Here, we developed a nextflow pipeline that creates a core genome multilocus sequence typing (cgMLST) scheme for Giardia intestinalis using chewBBACA. This workflow takes as input all available whole genome sequencing samples of Giardia intestinalis assemblages A and B (the most commonly associated with human infections) in public biorepositories (n=128, after excluding samples producing poor-quality assemblies). The accuracy and reproducibility of the schema calling process were verified using k-fold cross-validation (70\/30 splits) with a 95% prevalence cut-off for every locus in the training samples. Finally, the selected sequences with inaccurate alignments against reference genomes by BLAST were removed from the final schema definition. Our gene-by-gene schema is scalable to specific epidemiological settings (adding locally circulating Giardia spp. genomes), and the pipeline is extendable to other pathogens of public health interest. This schema, applied to culture-free stool metagenomics, can be used by interested public health agencies to investigate outbreaks and conduct genomic surveillance of human giardiasis."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"17:30-17:50","Format":"In Person","Speaker":null,"Title":"Analysis of Inverted Repeats in Viral Genomes at a Large Scale","Abstract":"An inverted repeat (IR) in DNA is a sequence of nucleotides that is followed by its complementary bases but in reverse order (e.g., CACGGAttgTCCGTG). IRs cause fragile sites endangering genetic stability. In viruses, IRs enable host cell entry, genomic evolution in zoonotic viruses, and more. Despite their importance, IRs have not been studied comprehensively viral genomes at a large scale. We developed a tool into the Biological Language Modeling Toolkit which computes augmented suffix-arrays to efficiently identify IRs, and studied 13,023 viral genomes and catalogued their IRs. We found over 19 million IRs longer than 20 bases (1,300 IRs per virus), including 134 that are longer than 2 kilobases. Among the viruses with large IRs, we identified over 50 large IRs in herpes viruses, and over 10 IRs in pox viruses. There is a prevalence of large â€˜terminalâ€™ inverted repeats in bacteriophages. We discovered large IRs in common disease-causing viruses, such as the African swine fever virus (lethal to domestic pigs), paramecium bursaria chlorella virus (important for termination of algae blooms, found to be able to infect humans and decrease the motor skills and reaction speed), Yaba-like disease virus (important in the cancer gene therapy), and human herpes virus. We found 54 viruses with high IR density, including disease-causing viruses like pox and herpes, and lymphocystis disease virus. These results in investigating the prevalence and distribution of inverted repeats in viral genomes suggests potential for discovery of mechanism of action of some of the understudied viruses."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"17:30-17:50","Format":"In Person","Speaker":null,"Title":"Intgration of Spatial Transcriptomics into Multimodal Imaging of Skin Aging","Abstract":"Advancements in spatial transcriptomicshave advanced our understanding of cellular organization and function within skin and other tissues. However, existing techniques often encounter limitations in resolution and coverage, hindering comprehensive analysis. To address this gap, we propose a novel approach to enhance the resolution of spatial transcriptomic data and integrate it into multimodal imaging workflows._x000D_\nOur project aims to leverage advanced image processing software to generate an approximated cell-level transcriptome from spatial transcriptomic data from juvenile and aged skin. By correlating gene expression profiles with immunofluorescence staining and age-related metabolic activity assays, we seek to gain novel insights into the intricate interplay between gene expression and cellular phenotypes. This would facilitate a more nuanced and analysis and allow to locally correlate complex phenotypes of cellular aging._x000D_\nFurthermore, we establish a robust analysis pipeline tailored for evaluating skin, streamlining future workloads for similar studies. This pipeline aims to address the complexity associated with spatial transcriptomic  data analysis, ensuring accessibility to individuals within the lab, including those without programming expertise._x000D_\nThrough the integration of spatial transcriptomics data into existing analytic imaging workflows, our project seeks to overcome existing limitations and pave the way for comprehensive analyses of cellular dynamics within tissue microenvironments. Our evaluation workflow includes initial assessment and comparative data analysis, utilizing quantitative metrics and established benchmarks  to objectively evaluate the performance and accuracy of our approach._x000D_\nOverall, our project holds significant promise in advancing our understanding of skin aging and offers valuable insights into tissue organization and cellular interactions."},{"Track":"CAMDA","Room":"520b","Weekday":"Monday","Date":"15 July","Timespan":"17:50-18:00","Format":"In Person","Speaker":null,"Title":"CAMDA 1st day summary","Abstract":null},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"08:40-10:00","Format":"In Person","Speaker":"Andrey Rzhetsky","Title":"Computational dissection of complex human disease","Abstract":"I will cover a collection of interrelated topics in dissection of etiology of complex human diseases, as seen through lens of large-scale medical data analysis.  Individual studies that I will cover focus on mosaic of genetic, environmental, and geneticâ€”environmental interaction factors.  The studies relied on massive medical records from US, Sweden, Denmark, and Japan, and a battery of modeling approaches."},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:10","Format":"In Person","Speaker":"Joaquin Dopazo","Title":"The Synthetic Clinical Health Records Challenge - Introduction","Abstract":"Although data protection is necessary to preserve patientsâ€™ intimacy, privacy regulations are also an obstacle to biomedical research. An interesting alternative is the use of synthetic patients. However, conventional synthetic patients are useless for discovery given that they are built out of known data distributions. Interestingly, Generative Adversarial Networks (GANs) and related developments have emerged as powerful tools to generate synthetic data in a way that captures relationships between the variables produced even if such relationships were previously unknown. GANs became popular in the generation of highly realistic synthetic pictures but have been applied in many fields, including in the generation of synthetic patients with applications such as medGAN and others._x000D_\nTwo datasets of synthetic patients have been subsequently created for this challenge since CAMDA 2023. Both datasets were generated from a real cohort retrieved from the Health Population Database (Base Poblacional de Salud, BPS) at the Andalusian Health System (Spain), by performing a Dual Adversarial AutoEncoder (DAAE) approach and contain data on about 1 million patients._x000D_\nTwo challenges are suggested on both datasets, although any other original analysis you may think will also be welcomed:_x000D_\n1) Finding some strong relationships in diabetes-associated pathologies that allows to predict any pathology before this is diagnosed. Some well-known pathological diabetes consequences, which can be considered relevant endpoints to predict, can be: a) Retinopathy, b) Chronic kidney disease, c) Ischemic heart disease, d) Amputations._x000D_\n2) Another proposed challenge is the prediction of disease trajectories in diabetes patients"},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"11:10-11:30","Format":"In Person","Speaker":"Daniel Voskergian","Title":"Predicting Diabetes Complications from Electronic Health Record Visits Using Machine Learning Algorithms","Abstract":"This study employed a novel approach to feature engineering, utilizing XGB feature selection combined with various supervised machine learning algorithms, including Random Forest, XGBoost, LogitBoost, AdaBoost, and Decision Tree, to develop predictive models for four complications of diabetes mellitus: retinopathy, chronic kidney disease, ischemic heart disease, and amputations. These models were built on synthetic electronic health records generated by dual-adversarial autoencoders, representing nearly 1 million synthetic patients for each of the two datasets used. These synthetic patients were derived from an authentic cohort of 979,308 and 984,414 individuals with diabetes, extracted from the Health Population Database (Base Poblacional de Salud, BPS) within the Andalusian Health System in Spain. The models considered variables such as age range and chronic diseases occurring during patient visits from the onset of diabetes. The final models, tailored to each complication, achieved an accuracy between 69% and 77% and an AUC between 77% and 84%. Notably, XGBoost and Random Forest demonstrated the best overall prediction performance, highlighting the effectiveness of our feature engineering and selection approach in enhancing model accuracy and robustness."},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"11:30-11:50","Format":"In Person","Speaker":"Daniel Santana-Quinteros","Title":"Cluster-based machine learning prediction of diabetes complications","Abstract":"Background: Type 2 Diabetes Mellitus (T2D) is a prevalent metabolic disorder characterized by hyperglycemia due to defects in insulin secretion or action. T2D often leads to severe complications, including cardiovascular diseases, nephropathy, retinopathy, and neuropathy. This study explores the feasibility of employing cluster-based machine learning techniques to predict diabetes complications._x000D_\nMethods: We utilized synthetic patient data generated by CAMDA 2023 (Spain) and real-world data from the DiabetIA database (Mexico), analyzing records of 997,657 patients in total. Data transformation involved converting JSON format to tabular form, cleaning sex information, and propagating chronic conditions across subsequent visits. Machine learning models, including Support Vector Machines (SVM) and Neural Networks (NN), were trained on stratified datasets to predict the onset of diabetic complications. Clustering techniques such as UMAP and BIRCH were employed to group patients by comorbidities._x000D_\nResults: The cluster-based machine learning models demonstrated potential in classifying diabetic complications. By analyzing patient data, the models identified distinct clusters of patients with similar comorbidities and disease trajectories. The classification processes gave us an area under the curve of 0.59 for NN and 0.56 for SVM at next year prediction._x000D_\nDiscussion: Cluster analysis can effectively enhance the understanding of T2D by revealing the interplay between various comorbidities and their impact on disease progression. The integration of advanced predictive models within a precision medicine framework promises more personalized and proactive healthcare interventions, ultimately improving patient outcomes and optimizing healthcare resources."},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"11:50-12:20","Format":"In Person","Speaker":"Owen Visser","Title":"Statistical Measures for the Evaluation of Clustering Methods on Single Cell Data","Abstract":"The growing efficiency of single-cell sequencing technology has provided biologists with ample cells to identify and differentiate, often through clustering. Heuristic approaches for clustering method choice have become more prevalent and could lead to inaccurate reports if statistical evaluation of the resulting clusters is omitted. During the advent of microarray data, a similar dilemma was addressed in literature through the provision of supervised and unsupervised measures, which were evaluated through Rank Aggregation. In this paper, these measures are adapted into the single-cell framework through a leave-one-out approach. Additionally, a scheme was created to utilize the information of cluster sizes by using their ranking to assign importance to the aggregation of methods, resulting in one table of methods ranked by cluster sizes. To demonstrate the ensemble of measures and scheme, five benchmark single-cell datasets were clustered with various methods at appropriate cluster sizes. We show that through rank aggregation and our importance scheme, our adapted measures select clustering methods that perform better at cluster sizes associated with true biological groups compared to those selected through traditional measures. For four of the five datasets and with internal measures alone, the rank aggregation scheme could correctly identify methods that performed the best at cluster sizes that match the original biological groups. We plan to package this ensemble of measures in the hopes to provide others with a tool to identify the best performing clustering methods and associated sizes for a variety of datasets. "},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:25","Format":"In Person","Speaker":null,"Title":"The Anti-Microbial Resistance Prediction Challenge - Introduction","Abstract":null},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"14:25-14:55","Format":"In Person","Speaker":"Alper Yurtseven","Title":"The Antimicrobial Resistance Prediction Challenge","Abstract":"Antimicrobial Resistance (AMR) is an urgent threat to human health worldwide as microbes have developed resistance to even the most advanced drugs. In this yearâ€™s CAMDA challenge, we focused on predicting AMR status of 1820 bacterial strains that belong to 7 different species (Campylobacter jejuni, Campylobacter coli, Escherichia coli, Klebsiella pneumoniae, Neisseria gonorrhoeae, Pseudomonas aeruginosa, Salmonella enterica) with machine learning."},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"14:55-15:15","Format":"In Person","Speaker":"Anton Pashkov","Title":"Machine learning models for AMR prediction","Abstract":"Each year, the Community of Interest Critical Assessment of Massive Data Analysis (CAMDA) presents various challenges related to massive data analysis and life sciences data. One of this year's challenges addresses the problem of predicting antimicrobial resistance in isolated samples. We conducted different analyses of the data using methods such as pangenomes and RGI to obtain data frames with counts of similar genes in gene families and counts of AMR gene families.  We then applied various machine learning (ML) models: some to predict resistance-susceptibility and others to predict the amount of antibiotic needed to classify the sample. A wide variety of preprocess and dimensionality reduction methods, together with supervised and unsupervised ML models were used, yielding the best F1 scores ranging from 0.76 to 0.96, with the best result obtained with logistic regression with L1 regularization."},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"15:15-15:35","Format":"In Person","Speaker":"Dexiong Chen","Title":"Biomarker identification by interpretable Maximum Mean Discrepancy","Abstract":"Motivation:In many biomedical applications, we are confronted with paired groups of samples, such as treated vs. control.  The aim is to detect discriminating features, i.e. biomarkers, based on high-dimensional (omics-) data. This problem can be phrased more generally as a two-sample problem requiring statistical significance testing to establish differences, and interpretations to identify distinguishing features. The multivariate maximum mean discrepancy (MMD) test quantifies group-level differences, whereas statistically significantly associated features are usually found by univariate feature selection. Currently, there are few general-purpose methods that simultaneously perform multivariate feature selection and two-sample testing.\\newline_x000D_\nResults: We introduce a sparse, interpretable, and optimised MMD test (SpInOpt-MMD) that enables two-sample testing and feature selection in the same experiment. SpInOpt-MMD is a versatile method and we demonstrate its application to a variety of synthetic and real-world data types including images, gene expression, and text data. SpInOpt-MMD is effective in identifying relevant features in small sample sizes and outperforms other feature selection methods such as SHapley Additive exPlanations (SHAP) and univariate association analysis in several experiments."},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"15:35-15:45","Format":"In Person","Speaker":null,"Title":"CAMDA Trophy ceremony","Abstract":null},{"Track":"CAMDA","Room":"520b","Weekday":"Tuesday","Date":"16 July","Timespan":"15:45-15:50","Format":"In Person","Speaker":null,"Title":"CAMDA summary and closing remarks","Abstract":null},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Jennifer Geddes-McAlister","Title":"Disruption of ClpX reverses antifungal resistance","Abstract":"Fungal disease impacts the lives of almost a billion people across the globe. The opportunistic human fungal pathogen, Cryptococcus neoformans, causes cryptococcal meningitis in immunocompromised individuals with high fatality rates in response to limited treatment options. Moreover, the emergence of azole-resistant isolates in the clinic following prolonged treatment regimes, environmental fungicide exposure, and fungal evolution, threatens the outcome of current therapeutic options, endangering the survival of infected individuals. By quantitatively characterizing the proteomes of fluconazole-susceptible and -resistant C. neoformans strains using state-of-the-art tandem mass spectrometry, we defined ClpX, an ATP-dependent unfoldase, as a target to overcome resistance. We discovered that disruption of ClpX through deletion or inhibition re-introduces fluconazole susceptibility into the resistant strains, rendering treatment effective once again. We further explored the mechanism of resistance and determined interruption to heme biosynthesis and ergosterol production associated with ClpX. Our results contribute to the understanding of novel mechanisms driving fluconazole resistance and provide support for targeting proteins as a therapeutic strategy to combat resistance."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Martin Frejno","Title":"Perception and reality of FDR control, data completeness and quantitative precision in (single-cell proteomics) DIA data","Abstract":"Introduction_x000D_\n_x000D_\nRecently, single cell proteomics (SCP) moved away from TMT labelling and DDA to label-free experiments and DIA due to its reportedly higher sensitivity and data completeness. Here, we developed a tool that enables routine quality control of peptide-centric DIA results. Application to published single cell DIA data showed that data completeness was only 45% instead of 98%._x000D_\n_x000D_\nMethods_x000D_\n_x000D_\nPublicly available single-cell and bulk DIA data were downloaded from PRIDE and searched library-free with DIA NN 1.8.1, Spectronaut 18 and Chimerys 2.0 using default settings against normal or entrapment databases without imputation. Data analysis was performed in R. The most useful plots and filtering options were incorporated into a Shiny application, the source code of which is available on GitHub._x000D_\n_x000D_\nPreliminary data_x000D_\n_x000D_\nOn bulk data, Spectronaut and Chimerys detected the same number of precursors at 1% precursor false discovery rate (FDR), while DIA-NN identified 17% more. In entrapment experiments, Chimerys showed well-controlled run-specific precursor FDR, while DIA-NN and Spectronaut lost a substantial amount of their identifications at 1% empirical FDR. When accurately controlling FDR, the results from all tools are comparable._x000D_\n_x000D_\nOn SCP data, Spectronaut outperformed both DIA-NN and Chimerys when multiple raw files were analyzed together. However, fragment-level XIC peak areas showed a tri-modal distribution. For low-intensity fragments they were between 0 and 1, although inspection of the corresponding raw data showed no signal at all. Excluding these fragments reduced data completeness from 98% to 45%._x000D_\n_x000D_\nOur results highlight the importance of closely inspecting search engine results instead of solely relying on FDR control."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Mathieu Lavallée-Adam","Title":"A novel supervised learning algorithm for real-time collision energy selection to optimize peptide fragmentation in mass spectrometry","Abstract":"Mass spectrometry is the most popular technique to characterize proteins in complex biological samples. The ability to identify peptides, proteins and their post-translational modifications using mass spectrometry is directly linked to the fragmentation level of peptide ions. Typically, a well-fragmented peptide ion generates data that facilitates sequence identification. Peptide ion properties, such as mass-to-charge ratio (m\/z), charge state, and ion mobility coefficient are related to the level of collision energy required for optimal fragmentation. Nonetheless, most mass spectrometers do not make use of all these pieces of information when attempting to determine the optimal collision energy for a given peptide fragmentation, leaving many peptides suboptimally fragmented and unidentifiable. _x000D_\n_x000D_\nHerein, we designed an artificial neural network that predicts the relative fragmentation of a given peptide ion using its properties when a certain level of collision energy is applied. This network is then used to determine in real-time, during mass spectrometry analysis, the optimal collision energy for a given peptide ion. Our novel algorithm accurately predicts relative fragmentation (r2=0.72) in the proteomics analysis of commercial human cell lysates with a Bruker timsTOF Pro mass spectrometer. Furthermore, using our software to determine the optimal collision energy for peptide ions increased the number of peptide identifications by 15%. It also improved post-translational modification characterization by identifying 12% more modified peptides when applied to human cell lysate samples enriched for phosphorylated peptides. By optimizing fragmentation, our method improves proteomics characterization and therefore provides a better understanding of biological processes in samples analyzed by mass spectrometry."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Varun Ananth","Title":"A learned score function improves the power of mass spectrometry database search","Abstract":"One of the core problems in the analysis of protein tandem mass spectrometry data is the peptide assignment problem: determining, for each observed spectrum, the peptide sequence that was responsible for generating the spectrum. Two primary classes of methods are used to solve this problem: database search and de novo peptide sequencing. State-of-the-art methods for de novo sequencing employ machine learning methods, whereas most database search engines use hand-designed score functions to evaluate the quality of a match between an observed spectrum and a candidate peptide from the database. We hypothesize that machine learning models for de novo sequencing implicitly learn a score function that captures the relationship between peptides and spectra, and thus may be re-purposed as a score function for database search. Because this score function is trained from massive amounts of mass spectrometry data, it could potentially outperform existing, hand-designed database search tools. To test this hypothesis, we re-engineered Casanovo, which has been shown to provide state-of-the-art de novo sequencing capabilities, to assign scores to given peptide-spectrum pairs. We then evaluated the statistical power of this Casanovo score function, dubbed Casanovo-DB, to detect peptides on a benchmark of three mass spectrometry runs from three different species. Our results show that, at a 1% peptide-level false discovery rate threshold, Casanovo-DB outperforms existing hand-designed score functions by 35% to 88%. In addition, we show that re-scoring with the Percolator post-processor benefits Casanovo-DB more than other score functions, further increasing the number of detected peptides."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Matthew Glover","Title":"Multi-Omic Data Workflows for Drug Discovery and Development","Abstract":"The Centre for Genomics Research (CGR) at AstraZeneca aims to identify and validate novel targets and deliver insights into disease biology by using integrated genomics, proteomics, metabolomics, and lipidomics data. Our cross-functional team has leveraged expertise in mass spectrometry (MS), bioinformatics, and systems engineering to deliver platforms capable of analyzing MS-based -omics data at the near petabyte data scale. This presentation will provide an in-depth look into practical challenges and solutions developed to support proteomics, metabolomics, and lipidomics data analysis and management. We will describe our MS and informatics capabilities and how they are designed to handle the acquisition and analysis of large-scale data from >10K proteomes and metabolomes per year in a high-performance computing environment. Central to this is our focus on end-to-end data throughput and reusability, while handling the complexities of data harmonization, storage, and computational demand. In addition, we will highlight several key aspects of this end-to-end omics platform including stringent data quality control measures and streamlined metadata capture to enable automation, reuse, visualization, and interpretation across a diverse array of experiments. Collectively, the topics covered will provide key insight on the importance of robust experimental and computational MS workflows for advancing drug discovery and development."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-15:00","Format":"In Person","Speaker":"Jianguo Xia","Title":"A unified LC-MS metabolomics framework for multi-omics and systems biology","Abstract":"Metabolites are key mediators of host-environment interactions. Global or untargeted metabolomics based on liquid chromatography-mass spectrometry (LC-MS) can provide rich information on host genetics, metabolism, microbiome composition, and environmental exposures. However, processing, annotation, and analysis of LC-MS and MS\/MS metabolomics data within the context of other omics remain a major bottleneck. In this talk, I will share our recent progress on developing algorithms, platforms and resources to enable comprehensive metabolomics, microbiomics and multi-omics data analysis, and showcase the main features using two case studies on exposomics and diabetes."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:40","Format":"In Person","Speaker":"Ali Rahnavard","Title":"waveome: characterizing temporal dynamics of metabolites in longitudinal studies","Abstract":"Longitudinal studies, clinical trials, and omics measurements reshape drug development, providing a comprehensive view of disease progression, treatment responses, and biological markers. This integration enhances drug discovery efficiency and enables advanced technologies like AI. Challenges such as correlated subjects, data sparsity, high dimensionality, and limited samples hinder longitudinal omics datasets. To overcome this, we offer adaptable machine learning techniques with user-friendly software, including Gaussian Processes for temporal modeling to uncover omics relationships. Longitudinal metabolomics, specific yet challenging due to noise and dimensionality, is explored using Gaussian processes applied to Crohn's disease metabolomics data._x000D_\nAdditionally, longitudinal data span various studies, including time-based omics. Decision-making in longitudinal omics presents high-dimensional challenges. Practical studies on Maternal-Infant omics, Inflammatory Bowel Disease, and the swan gut microbiome illustrate modeling intricacies, highlighting potential and challenges. Access our software at https:\/\/github.com\/omicsEye\/waveome."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:40","Format":"In Person","Speaker":"Margaret Martin","Title":"AI-driven de novo structural candidate generation for mass spectra annotation","Abstract":"Despite the increase in reference library size and available annotation tools, the rate of assignment of molecular structures to mass spectra remains low. Importantly, as not all natural products are known nor cataloged in databases; generative AI models can be used to predict structural candidates for spectra annotation. REINVENT4 is an AI framework that may be customized to facilitate de novo generation of molecular structures with desired properties i.e., generate novel molecular structures that have desirable properties for applications such as drug discovery (Loeffler, 2024). Here, we use two features of this framework: the priors and the custom scoring functions for reinforcement learning. The priors are unbiased generators trained on large molecular datasets. We finetune the prior generator model using transfer learning with candidate molecules retrieved from PubChem based on the precursor mass. Using reinforcement learning, we utilize custom scoring functions that guide the generation of relevant molecular candidates for a queried spectrum. When sampled, the REINVENT4 framework produces SMILES of de novo candidate molecules. We evaluate our method by applying it to annotated spectra in the CANOPUS dataset.  For a sample of 30 spectra, our method suggests identifying the true structure for 20% of cases, an increase over the recently published work, MS2Mol (Butler, 2023). This result represents an increase in structural identification of spectra representing previously unknown molecules and suggests that this method may elucidate the identities of previously unannotated spectra."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:40","Format":"In Person","Speaker":"Yuanye Chi","Title":"A consensus serum metabolome by large-scale data mining reveals major gaps in metabolomic measurements and modeling","Abstract":"Blood analysis is the most common in biomedical applications and a reference metabolome will be critical for effective annotation and for guiding scientific investigations. However, compiling such a reference is hindered by many technical challenges, despite the availability of large amount of metabolomics data today. We have designed a series of data structures and tools, including asari (Nature Communications 14, 4113) and khipu (Analytical Chemistry 95, 6212), which enabled a first draft of assembling a consensus serum metabolome from large-scale public data. This assembly is based on about 77,000 metabolomes on human serum or plasma samples measured by Orbitrap mass spectrometers coupled by liquid chromatography. We first validated the approach on cross-laboratory comparison of the Checkmate dataset (1172 samples) vs our HZV029 dataset (1685 samples). Next, all 813 datasets from 110 studies were processed into feature tables with quality metrics from asari and 49,184 consensus mass tracks were extracted by kernel density estimation. Preannotation (neutral masses and associated isotopologues and adducts) can be aligned cross studies, therefore greatly improving feature annotation. About 25% of this consensus serum metabolome is covered by HMDB v5, 50% by PubchemLite and 5% by the current human genome scale metabolic models, in a frequency dependent manner. The results indicate significant gaps in the current databases and metabolic models.  We will report both the tool development and scientific findings, and the resource will be freely available via a web service."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:40","Format":"In Person","Speaker":"Gaetan De Waele","Title":"Transformers for MALDI-TOF MS-based antimicrobial drug recommendation","Abstract":"Timely and effective use of antimicrobial drugs can improve patient outcomes, as well as help safeguard against resistance development. Matrix-assisted laser desorption\/ionization time-of-flight mass spectrometry (MALDI-TOF MS) is currently routinely used in clinical diagnostics for rapid species identification. Mining additional data from said spectra in the form of antimicrobial resistance (AMR) profiles is, therefore, highly promising. Such AMR profiles could serve as a drop-in solution for drastically improving treatment efficiency, effectiveness, and costs. Stifled by a historical lack of open data, machine learning research towards models specifically adapted to MALDI-TOF MS remains in its infancy._x000D_\n_x000D_\nHere, we introduce Maldi Transformer, an adaptation of the state-of-the-art transformer architecture to the MALDI-TOF mass spectral domain. We propose the first self-supervised pre-training technique_x000D_\nadapted to mass spectra. The technique is based on shuffling peaks across spectra, and pre-training the transformer as a peak discriminator._x000D_\n_x000D_\nWe deploy the proposed method to predict AMR profiles for the whole repertoire of species and drugs encountered in clinical microbiology. The resulting model can be interpreted as a drug recommender system for infectious diseases. We find that our dual-branch method delivers considerably higher performance compared to previous approaches. In addition, experiments show that the models can be efficiently fine-tuned to data from other clinical laboratories. Maldi Transformer-based recommender systems can, hence, greatly extend the value of MALDI-TOF MS for clinical diagnostics."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Martin Frejno","Title":"The sky is the limit: a cloud-based proteomics platform for the masses","Abstract":"Background: Laboratories dealing with bottom-up proteomics data often encounter computational hurdles in the journey from raw data to conclusive insights. Challenges arise, from the absence of automated pipelines and disjointed local infrastructure for data storage to processing, systematic result management, and interpretation. The advent of fast-scanning instruments exacerbates these issues by overwhelming local infrastructure with a multitude of files and large raw data sizes. Here, we introduce a highly scalable, fully automatable, cloud-based proteomics platform designed to streamline the entire workflow._x000D_\n_x000D_\nMethods: Our cloud-native platform comprises microservices operated on AWS and orchestrated by Kubernetes. Users can access the platform through either a command line client or a browser-based interface, both interacting with an API governing all platform functionalities. Raw data undergoes processing using Chimerys 4 on an elastic compute cluster. Results are stored in a data lake and can be explored directly in the browser or downloaded. Metadata annotation facilitates navigation and contextualization of numerous files. Platform access is available through subscription or self-hosted deployment._x000D_\n_x000D_\nResults: We present a comprehensive, managed solution for proteomics data management, obviating the need for user-managed pipelines and infrastructure. The platform offers an intuitive web interface for collaborative data upload, management, and processing. File transfer occurs at speeds of up to 100 MB\/s into scalable object storage. Raw data can be annotated with metadata via a searchable tag system, simplifying organization and retrieval. A scalable compute cluster enables simultaneous processing of DDA, DIA, and PRM data from thousands of files. The platform is algorithm-independent, currently supporting Chimerys 4 with plans for additional search engines. We demonstrate the scalability by processing multiple files without significant increase in processing time compared to single file processing. Processed data can be organized using the same tag system employed for raw data, with the processing overview providing immediate insight into key parameters for data quality assessment. A fast post-processing workflow combines individually searched raw files, facilitating longitudinal data acquisition and processing without overheads. Results can be accessed via API- or browser-based download, direct API access to the result data lake, browser-based data exploration, or a customizable visualization dashboard featuring common data analyses and visualizations._x000D_\n_x000D_\nConclusions: This managed, automated proteomics data pipeline promises to streamline the journey from raw data to insights, particularly benefiting laboratories lacking the resources to develop and maintain in-house solutions."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Haixu Tang","Title":"SpecEncoder: Deep Metric Learning for Accurate Peptide Identification in Proteomics","Abstract":"Tandem mass spectrometry (MS\/MS) is a crucial technology for large-scale proteomic analysis. The protein database search or the spectral library search are commonly used for peptide identification from MS\/MS spectra, which, however, may face challenges due to experimental variations between replicated spectra and similar fragmentation patterns among distinct peptides. To address this challenge, we present SpecEncoder, a deep metric learning approach to address these challenges by transforming MS\/MS spectra into robust and sensitive embedding vectors in a latent space. The SpecEncoder model can also embed predicted MS\/MS spectra of peptides, enabling a hybrid search approach that combines spectral library and protein database searches for peptide identification. We evaluated SpecEncoder on three large human proteomics datasets, and the results showed a consistent improvement in peptide identification. For spectral library search, SpecEncoder identifies ~1-2% more unique peptides (and PSMs) than SpectraST. For protein database search, it identifies 6-15% more unique peptides than MSGF+ enhanced by Percolator, Furthermore, SpecEncoder identified 6-12% additional unique peptides when utilizing a combined library of experimental and predicted spectra. SpecEncoder can also identify more peptides when compared to deep-learning enhanced methods (MSFragger boosted by MSBooster). These results demonstrate SpecEncoder's potential to enhance peptide identification for proteomic data analyses."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:40","Format":"In Person","Speaker":"Kyowon Jeong","Title":"FLASHTagger: An open-source web application for ion type- and precursor mass-free protein identification in top-down mass spectrometry","Abstract":"The growing capacity to detect proteins and protein complexes in MS pose computational challenges in identifying them through Top-DownMS (TDMS). While alternative fragmentation methods such as ECD and UVPD open up multiple fragmentation pathways increasing sequence coverage, they also complicate the interpretation of fragment spectra. Recently developed protocols like complex-down MS often isolate protein complexes at once yielding multiplexed fragment spectra. Together with complex signal structure of TDMS spectra and frequent errors in deconvolution, they present challenges in correct precursor ion interpretation._x000D_\nAddressing these issues, we present FLASHTagger, a high-sensitivity protein identification tool for TDMS platforms. Unlike most conventional database searches, FLASHTagger is de novo sequence tag-based and thus runs without specifying fragment ion type or assuming monomeric proteoform precursors. The tags enable rapid protein searches, with a protein-level false discovery rate (FDR) control. Benchmark tests performed with EChcD datasets from monoclonal antibody and E.coli membrane proteins showed that FLASHTagger can reliably identify individual target proteins from MS\/MS scans of multimeric complexes. Analysis of the matched tags revealed various ion types, including internal ions, and well known protein modifications._x000D_\nCurrent implementation of FLASHTagger focuses on the low complexity datasets, but analysis of complex datasets will be made available in near future as a part of our new proteoform search engine. We anticipate that the precursor independent feature of FLASHTagger would open up the gate toward data independent acquisition in TDP. FLASHTagger is deployed as a part of OpenMS web application FLASHViewer at https:\/\/abi-services.cs.uni-tuebingen.de\/flashviewer\/."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:40","Format":"In Person","Speaker":"Lincoln Harris","Title":"Imputation of cancer proteomics data with a deep model that learns jointly from many datasets","Abstract":"TMT proteomics suffers from excessive missing values, especially in the large-scale, multi-batch experimental setting. Imputation is an analytical solution to the missingness problem. Many methods exist for TMT proteomics imputation, however, few of them take advantage of deep neural networks, and none of them can learn jointly from multiple datasets. We introduce Lupine, a deep learning-based imputation tool that learns patterns of missingness across many mass spectrometry runs and experiments. We demonstrate that Lupine outperforms the current state-of-the-art and learns meaningful representations of experimental structure and protein physicochemical properties. _x000D_\n_x000D_\nWe first constructed a joint protein quantifications matrix consisting of mass spectrometry runs from 10 cancer cohorts from the Clinical Proteomics Tumor Atlas Consortium (CPTAC). These data were generated with a common experimental workflow and processing pipeline. We then developed a deep learning model that leverages matrix factorization to learn low-dimensional representations of proteins and mass spectrometry runs. This model, called Lupine, was trained on our joint protein quantifications matrix. _x000D_\n_x000D_\nWe show that Lupine outperforms DreamAI, an ensemble imputation method that represents the current state-of-the-art for TMT proteomics. For each of the 10 CPTAC cohorts, the mean squared error of Lupineâ€™s imputed values is lower than DreamAIâ€™s. We also show that Lupine learns a latent representation of proteins that captures missingness fraction and other protein physicochemical properties. Lupine increases the number of differentially expressed proteins between CPTAC cohorts and improves clustering accuracy.  _x000D_\n_x000D_\nIn summary, Lupine is the only existing proteomics imputation method that can learn jointly from many datasets."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:40","Format":"In Person","Speaker":"Husen M. Umer","Title":"Proteogenomics analysis of human tissues using pangenomes","Abstract":"The genomics landscape is evolving with the emergence of pangenomes, challenging the conventional single-reference genome model. The new human pangenome reference provides an extra dimension by incorporating variations observed in different human populations. However, the increasing use of pangenomes in human reference databases poses challenges for proteomics, which currently relies on UniProt canonical\/isoform-based reference proteomics. Including more variant information in human proteomes, such as small and long open reading frames and pseudogenes, prompts the development of complex proteogenomics pipelines for analysis and validation. This study explores the advantages of pangenomes, particularly the human reference pangenome, on proteomics, and large-scale proteogenomics studies. We reanalyze two large human tissue datasets using the quantms workflow to identify novel peptides and variant proteins from the pangenome samples. Using three search engines SAGE, COMET, and MSGF+ followed by Percolator we analyzed 91,833,481 MS\/MS spectra from more than 30 normal human tissues. We developed a robust deep-learning framework to validate the novel peptides based on DeepLC, MS2PIP and pyspectrumAI. The results yielded 170142 novel peptide spectrum matches, 4991 novel peptide sequences, and 3921 single amino acid variants, corresponding to 2367 genes across five population groups, demonstrating the effectiveness of our proteogenomics approach using the recent pangenome references."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:40","Format":"In Person","Speaker":"Cecile Le Sueur","Title":"Optimising Thermal Proteome Profiling experimental design with GPMelt","Abstract":"Thermal proteome profiling (TPP) combines cellular thermal shift assay and quantitative mass spectrometry to explore protein interactions and states proteome-wide. Temperature range TPP (TPP-TR) datasets consist of protein melting curves, quantifying non-denatured proteins across a temperature gradient. Thermal stability changes are statistically evaluated by comparing melting curves between conditions, like drug treatment versus control. While powerful and versatile in uncovering new biology, TPP's experimental cost, including consumables and mass spectrometry measurement time, limits its accessibility to well-funded researchers. Moreover, the sample requirements hinder its application to rare samples. We recently introduced GPMelt, a statistical framework for TPP-TR datasets based on hierarchical Gaussian process models, robustly integrating replicates information and handling any melting curve shape. Here, we propose an enhanced GPMelt model together with an optimized low-cost and low-sample TPP-TR experimental design. By halving the consumables and mass spectrometry measurement time, this work broadens TPP-TR accessibility to a wider scientific community and opens its application to precious samples. Additionally, it establishes a smooth connection between TPP-TR and 2D-TPP protocols and analyses. 2D-TPP datasets compare protein thermal stability across a larger number of conditions, using a distinct sample multiplexing strategy that hinders melting curves reconstruction. Adapting this multiplexing strategy in combination with the enhanced GPMelt model strengthens 2D-TPP discoveries by retaining melting curves modeling, and hence key biological information. Collectively, extensions to the GPMelt model, combined with optimised experimental designs for both TPP-TR and 2D-TPP, can significantly increase TPPâ€™s effectiveness and dissemination among scientists, paving the way for groundbreaking biological discoveries."},{"Track":"CompMS","Room":"525","Weekday":"Saturday","Date":"13 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Shantanu Jain","Title":"An algorithm for decoy-free false discovery rate estimation in XL-MS\/MS","Abstract":"Motivation: Cross-linking tandem mass spectrometry (XL-MS\/MS) proteomics is an established technique that determines distance constraints between residues within a protein or between interacting proteins, thus improving our understanding of protein structure and function under native cellular conditions. To aid biological discovery, it is essential that pairs of chemically linked peptides be accurately identified, a process that requires: (i) database search, that creates a ranked list of candidate peptide pairs for each experimental spectrum, and (ii) false discovery rate (FDR) estimation, that determines the probability of false identification of the top-ranked peptide pairs for a given score threshold. Currently, the only available FDR estimation mechanism in XL-MS\/MS is the target-decoy approach (TDA). However, despite its simplicity, TDA has both theoretical and practical drawbacks._x000D_\n_x000D_\nResults: We introduce a novel decoy-free framework for FDR estimation in XL-MS\/MS. Our approach relies on multi-sample mixtures of skew normal distributions, where the latent components correspond to the scores of correct peptide pairs (both peptides identified correctly), partially incorrect peptide pairs (one peptide identified correctly, the other incorrectly), and incorrect peptide pairs (both peptides identified incorrectly). To learn these components, we exploit the score distributions of first- and second-ranked peptide-spectrum matches (PSMs) for each experimental spectrum and subsequently estimate FDR using a novel expectation-maximization (EM) algorithm with constraints. We evaluate the method on ten datasets and provide evidence that the proposed DFA is theoretically sound and a viable alternative to TDA owing to its good performance in terms of accuracy, variance of estimation, and run time."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"09:00-09:20","Format":"In Person","Speaker":"Gregoire Altan-Bonnet","Title":"Building models of CAR-T signal integration, _x000B_using automatized\/dynamic high-dimensional dynamic profiling","Abstract":"We present an experimental\/theoretical pipeline to build quantitative models of leukocyte activation. We introduce a robotic platform to quantify the dynamics of cell differentiation and cytokine production\/consumption by T cells ex vivo. These high-dimensional dynamics can be compressed into a 2D model using tools from machine learning. Our model highlights two modalities of T cell activation that enforce adaptive kinetic proofreading of antigen-TCR interactions, and that encode antigen discrimination. We test our model of antigen discrimination across varied immunological settings, including CAR-T and signaling-impaired T cells. To conclude, we highlight the power of lab automation, data integration, machine learning and theoretical modeling to usher new insights in systems immunology._x000D_\n"},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":null,"Title":"TBD","Abstract":null},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"In Person","Speaker":"Kenneth Hoehn","Title":"Inferring B cell phylogenies from single cell and bulk BCR sequence data with Dowser","Abstract":"Antibodies are vital to human immune responses and are composed of genetically variable heavy and light chains. These structures are initially expressed as B cell receptors (BCRs). BCR diversity is shaped through somatic hypermutation and selection during immune responses. This evolutionary process produces B cell clones, cells that descend from a common ancestor but differ by mutations. Phylogenetic trees inferred from BCR sequences can reconstruct the history of mutations within a clone. Until recently, BCR sequencing technologies separated heavy and light chains, but advancements in single cell sequencing now pair heavy and light chains from individual cells. However, it is unclear how these separate genes should be combined to infer B cell phylogenies. In this study, we investigated strategies for using paired heavy and light chain sequences to build phylogenetic trees. We found incorporating light chains significantly improved tree accuracy and reproducibility across all methods tested. This improvement was greater than the difference between tree building methods and persisted even when mixing bulk and single cell sequencing data. However, we also found that many phylogenetic methods estimated significantly biased branch lengths when some light chains were missing, such as when mixing single cell and bulk BCR data. This bias was eliminated using maximum likelihood methods with separate branch lengths for heavy and light chain gene partitions. Thus, we recommend using maximum likelihood methods with separate heavy and light chain partitions, especially when mixing data types. We implemented these methods in the R package Dowser: https:\/\/dowser.readthedocs.io."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":null,"Title":"TBD","Abstract":null},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"Alireza Karbalayghareh","Title":"Learning multiomic velocities of dynamic germinal center B cells using single-cell multiome","Abstract":"The epigenome and transcriptome influence each other in differentiation trajectories. We use single cell multiomic data, with RNA expression and chromatin accessibility (ATAC) readouts, to model the interplay of epigenome and transcriptome in dynamic germinal center (GC) B cells. We have developed a model, DynaVelo, to learn the latent time and joint dynamics of cells in both RNA expression and TF motif accessibility spaces. In addition to RNA velocities that inform us of cell trajectories in RNA space, we further define an analogous velocity in the motif accessibility space. We adopt techniques from variational autoencoders and neural ODEs and leverage RNA expression and velocity as well as motif accessibility to learn the joint dynamics of cells. Finally, we apply these models to the GC B cells to learn the dynamics of wildtype samples as well as samples with mutations in epigenetic regulators ARID1A and CTCF, modeling somatic alterations seen in B cell lymphomas. The learned latent times demonstrate plausible starting and end points that are consistent with the velocities. In silico perturbation of these models provide predictions for the impact of experimental interventions on the trajectories of B cells. We perform in silico perturbations of critical B cell TFs to dissect their roles in GC dynamics and identify potential perturbation targets for rescuing the loss of function in mutant samples."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Jane Siwek","Title":"Sliding Window INteraction Grammar (SWING): a generalized interaction language model for peptide and protein interactions","Abstract":"Language models (LMs) and protein LMs have now been employed in many frameworks. Traditionally for protein-protein or peptide-protein interactions (PPIs), corresponding sequences are concatenated and co-embedded. However, these are highly limited as no method utilizes a language representation of the interaction itself. We developed Sliding Window Interaction Grammar (SWING), a novel interaction LM (iLM) that leverages differences in amino acid properties to generate an interaction vocabulary. This is then input into an LM, and corresponding features are used for different downstream supervised prediction steps. _x000D_\nSWING was first applied to predicting peptide:MHC (pMHC) interactions. Currently, existing approaches have separate Class I and Class II pMHC prediction models as these interactions are distinct structurally and functionally. SWING however was not only successful at generating Class I and Class II models that have comparable prediction to state-of-the-art approaches, but a SWING model trained only on Class I alleles was predictive for Class II, a complex task not attempted by any existing approach. For de-novo data, using only Class I or Class II data, SWING also accurately predicted Class II pMHC interactions in murine models of SLE and T1D, that were validated experimentally._x000D_\nSWING also predicted the disruption of specific interactions by missense mutations. Modern methods like AlphaMissense and ESM1b can predict pathogenicity per mutation but are unable to predict interaction-specific disruptions. SWING accurately predicted the impact of Mendelian mutations and population variants on PPIs, outperforming AlphaMissense\/ESM1b. Overall, SWING is a first-in-class few-shot iLM that learns the language of PPIs using sequence alone."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:20","Format":"In Person","Speaker":"Michael Klausen","Title":"Improved Peptide-MHC Interaction Predictions through Deep Generative Adversarial Networks and a Unified MHC Class I and Class II Representation","Abstract":"In this study, we address the challenge of accurately predicting peptide presentation by Major Histocompatibility Complex (MHC) molecules, a key feature in developing efficacious personalized and precision vaccines. Our approach introduces a novel deep learning framework that enhances prediction accuracy by utilizing the growing wealth of available immunopeptidomic data. Our methodology introduces three new strategies: The creation of a unified representation for both MHC class I and -II molecules, the implementation of a deep transformer encoder-decoder architecture, and the adoption of a generative adversarial network (GAN) pretraining mechanism. _x000D_\n_x000D_\nThe unified representation allows our model to leverage MHC binding cores across MHC classes and the deep learning architecture facilitates direct training on peptide sequences without the need for pre-processing, such as the extraction or alignment of binding cores. The GAN pretraining stabilizes the training phase and enhances overall performance by generating synthetic peptide data for training enhancement. _x000D_\n_x000D_\nOur results demonstrate significant improvements in the prediction of peptide-MHC interactions, particularly for MHC class II molecules, which have historically been difficult to predict accurately. This advancement offers a more reliable and efficient tool for the design of personalized cancer immunotherapies and the development of vaccines against a wide range of pathogens. Through these technical achievements, our work contributes to the advancement of precision medicine, highlighted by a significant improvement when deployed in our AI-immunologyâ„¢ platform. An improvement shown in preclinical studies and now being used in clinical trials."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:20","Format":"In Person","Speaker":"Ha Young Kim","Title":"TSpred: a robust prediction framework for TCR-epitope interactions using paired chain TCR sequence data","Abstract":"Prediction of T-cell receptor (TCR)-epitope interactions is important for many applications in biomedical research, such as cancer immunotherapy and vaccine design. Although numerous computational methods have been developed, the prediction of TCR-epitope interactions remains challenging. The prediction is known to be particularly difficult for novel epitopes, due to the scarcity of available data. Here, we propose TSpred, a new deep learning approach for the pan-specific prediction of TCR binding specificity based on paired chain TCR data. We develop a robust model that generalizes well to unseen epitopes by utilizing an ensemble model of a CNN-based model and an attention-based model. In particular, we design a reciprocal attention mechanism which is specifically designed to extract the patterns underlying TCR-epitope interactions. Upon a comprehensive evaluation of our model, we find that TSpred achieves state-of-the-art performances in both seen and unseen epitope specificity prediction tasks. Also, compared to other predictors, TSpred is more robust to bias related to peptide imbalance in the dataset. Furthermore, the reciprocal attention component of our model allows for model interpretability by capturing structurally interacting residue pairs that contribute to TCR-epitope binding. Results indicate that TSpred is a robust and reliable method for the task of TCR-epitope binding prediction."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:20","Format":"In Person","Speaker":"Susanna Kiwala","Title":"Comprehensive neoantigen identification and prioritization using pVACtools and pVACview","Abstract":"Personalized neoantigen vaccines utilize immunogenomics and immuno-oncology strategies to combat cancer. Somatic variants in tumor cells generate neoantigens that may bind to MHC molecules and get presented on the tumor cellâ€™s surface. Immunotherapies, such as checkpoint blockade therapies, personalized cancer vaccines and other T cell based therapies, target these neoantigens to stimulate a tumor-specific immune response._x000D_\n_x000D_\nWe have developed a computational framework for neoantigen identification and prioritization, pVACtools (pVACtools.org), that integrates tumor variant and expression data (DNA- and RNA-Seq) into an end-to-end solution for design of neoantigen targeting therapies including personalized vaccines. pVACtools consists of multiple command line tools for neoantigen prediction from somatic alterations (pVACseq, pVACfuse, pVACsplice, and pVACbind), a tool for designing DNA vectorâ€“based constructs (pVACvector), as well as a web application (pVACview) for visualizing, reviewing, prioritizing, and selecting neoantigen candidates for peptide or nucleic acid vaccine manufacturing platforms._x000D_\n_x000D_\nThe full pVACtools suite seamlessly allows users to: 1) Identify altered peptides from different mechanisms (i.e. point mutations, indels, gene fusions, splice site alterations, or frameshift variants). 2) Predict peptide binding affinity, elution, and immunogenicity metrics via an ensemble of MHC Class I and II algorithms. 3) Filter based on variant allele expression, read-counts, variant allele fractions, transcript support level, and peptide binding affinities. 4) Visualize results to allow further interactive neoantigen prioritization. 5) Design DNA-vector vaccines._x000D_\n_x000D_\npVACtools rapidly and efficiently identifies potentially immunogenic neoepitopes, and is being used in both basic and translational research, as well as over 10 clinical trials on clinicialtrials.gov to date."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:20","Format":"In Person","Speaker":"Ayshwarya Subramanian","Title":"Immune checkpoint molecule Tim-3 regulates microglial function and the development of Alzheimerâ€™s disease pathology.","Abstract":"Microglia are the major resident innate immune cells of the central nervous system (CNS) playing essential roles in CNS homeostasis and disease. Although strongly implicated in Alzheimerâ€™s disease (AD) pathology, the regulatory mechanism of microglial activation remains to be fully elucidated. By combining population and single-cell (scRNA-seq) transcriptomics, IP-MS, neuroinflammation assays, and behavioral studies, we investigated the role of the AD risk gene (Wightman et al., Nat Genet, 2021) and immune checkpoint molecule HAVCR2\/TIM-3 in microglial homeostasis and activation. Tim-3 (Havcr2) is highly and specifically expressed in microglia compared to other cell types in both mouse and human brain. Gene expression profiles of Tim-3 deficient microglia resemble those of phagocytic microglia, and microglia in neurodegeneration (MGnD). Mechanistically, Tim-3 enhances TGF-b signaling by promoting phosphorylation of Smad2, thus contributing to the homeostasis of microglia, in a mechanism independent of its activity in T-cells. Importantly, we report that microglia-specific Tim-3 deficiency reduced Amyloid beta (Ab) plaque load and neuronal damage, and resulted in improved cognitive function in a 5XFAD mouse model of AD. Single-nucleus and single-cell RNA sequencing identified that microglia in Havcr2-deficient 5xFAD mice are characterized by increased pro-phagocytic and anti-inflammatory gene expression together with decreased proinflammatory gene expression. Thus, Tim-3 may serve to regulate phagocytotic and inflammatory functions of activated microglia in AD. Collectively, these results hold promise for a potential new therapeutic strategy targeting the checkpoint molecule Tim-3 in AD."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:20","Format":"In Person","Speaker":"Sarah Walker","Title":"Single-cell multi-omics reveals similarity between progenitor stem-cell-like CD8+ T cells and CD4+ follicular helper T cells in viral infection","Abstract":"Mechanisms of gene expression regulation in T cell activation and differentiation across lineages and functional states are incompletely understood. To address this, we generated and analyzed single-cell multi-omic scATAC+scRNA-seq data in splenic T cells (including both CD4+ and CD8+) responding to acute and chronic mouse infection with lymphocytic choriomeningitis virus (LCMV). Computational scATAC-seq data analysis is challenging due to extreme sparsity and incompletely understood statistical properties of the data, as well as high level of cell type specificity and lack of available epigenomic references. Thus, we collected a compendium of published bulk ATAC-seq datasets for mouse CD4 and CD8 T cells, and then built a comprehensive atlas of chromatin accessibility peaks and epigenomic signatures of functional T cell states. This allowed us to analyze our new scATAC-seq data at unprecedented level of detail. We were able to clearly distinguish between CD4 and CD8 T cells, as well as between naive and memory cells in both lineages. We observed that most cell states, including both effector and exhausted cells, as well as progenitor stem-cell-like Tcf1+ CD8 T cells, were found in both acute and chronic responses, though sometimes with different frequency. Interestingly, progenitor Tcf1+ CD8 T cells were most similar epigenetically and transcriptionally to CD4 follicular helper T cells. We hypothesize that similar molecular regulation, potentially driven by their direct or shared physical cell-cell interactions or shared microenvironment, may be associated with their shared regulatory program."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:20","Format":"In Person","Speaker":"Sara Capponi","Title":"Optimizing CAR T cell design using quantum convolutional neural networks","Abstract":"Chimeric antigen receptor (CAR) T-cells are promising new medicines that apply to the treatment of many cancers, and potentially represent novel approaches to treat infectious diseases and autoimmunity. A central challenge in expanding and enhancing CAR T-cell functions is identifying beneficial combinations of intracellular costimulatory motifs to elicit desired phenotypes. This is due to the large motif combinatorial space and high experimental costs, in time and resources, to generate and measure CAR performance. This results in a highly data-constrained problem where developing accurate predictive models of CAR T-cell behavior is difficult. State-of-the-art machine learning (ML) models based on convolutional neural networks (CNNs) combined with long-short term memory have been shown to reach an accuracy of 70% when predicting CAR T-cell stemness and cytotoxicity. With the clear need for more accurate predictive models, we investigated the performance of quantum convolutional neural networks (QCNNs) as a novel ML model for improved predictive performance. QCNNs carry advantages over classical CNNs, including good generalization in underdetermined problems and usage of significantly fewer training parameters as these scale logarithmically with the number of qubits. QCNNs also have advantages over certain other quantum ML methods, including the absence of barren plateaus. Our study showed that QCNN matches and occasionally exceeds the performance of CNNs in classifying CAR T cells by cytotoxicity levels. Employing larger and more expressive QCNN models may further enhance performance, potentially resulting in a superior predictive tool for CAR T-cell phenotypes."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:20","Format":"In Person","Speaker":"Seda Arat","Title":"A Computational Approach to Auto-Immunity Risk Assessment: Use Cases of Molecular Mimicry Hypothesis for Viral Infections and Vaccines-Associated Adverse Events","Abstract":"SARS-CoV-2 virus is a member of a large family of viruses called coronaviruses that causes a respiratory disease COVID-19. There are almost 775 million cases and more than 7 million deaths worldwide. While there are short-term effects of SARS-CoV-2 infection such as fever, fatigue, headache and loss of taste\/smell, the long-term effects can be very serious, including organ damage. In some people, lasting health effects may include, heart complications, chronic kidney impairment, stroke, and Guillain-Barre syndrome (GBS). Myocarditis is also a well-known adverse event associated with both SARS-CoV-2 infection and mRNA-based COVID-19 vaccines. This study focuses on (1) building a computational pipeline based on protein sequence and 3D structure similarities, and (2) determining if these adverse events are occurring due to molecular mimicry. Initially, we identified ~2500 human proteins that contains peptides (9 and 15 amino acids) with at least 50% similarity within the SARS-CoV-2 protein sequences. These proteins were triaged to more than 50 human endogenous proteins highly expressed in the heart (for myocarditis), kidney (for chronic kidney impairment) and brain (for GBS) that could potentially lead to immune-mediated adverse events based on 3D structure analysis using Alphafold. The presentation will describe our computational approach that can be instrumental for infectious disease, vaccine and drug safety scientists for auto-immunity risk assessment."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":null,"Title":"TBD","Abstract":null},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Joseph Ng","Title":"sciCSR infers B cell state transition and predicts class-switch recombination dynamics using single-cell transcriptomic data","Abstract":"Class-switch recombination (CSR) is an integral part of B cell maturation. Whilst tools such as pseudotime and RNA velocity exists to infer cellular dynamics in single-cell RNA sequencing (scRNA-seq) data, they ignore processes such as CSR which are specific to B cells. Here we present sciCSR (pronounced â€˜scissorâ€™, single-cell inference of class-switch recombination), a computational pipeline that analyzes CSR events and dynamics of B cells from scRNA-seq experiments. Validated on both simulated and real data, sciCSR re-analyzes scRNA-seq alignments to differentiate productive heavy-chain immunoglobulin transcripts from germline â€˜sterileâ€™ transcripts. From a snapshot of B cell scRNA-seq data, a Markov state model is built to infer the dynamics and direction of CSR. Applying sciCSR on SARS-CoV-2 time-course scRNA-seq data, we observe that sciCSR predicts, using data from an earlier time point in the collected time-course, the isotype distribution of B cell receptor repertoires of subsequent time points with high accuracy (cosine similarity ~0.9). Using processes specific to B cells, sciCSR identifies transitions that are often missed by conventional RNA velocity analyses and can reveal insights into the dynamics of B cell CSR during immune response. We believe sciCSR offers a starting point to model B cell maturation in scRNA-seq data; these models can be further analyzed to understand the molecular cues of CSR and different steps of the maturation process, their regulation in situ within tissues, and their dysregulation in diseases."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Lingyu Li","Title":"FineST: Super resolved ligand-receptor interaction discovery by fusing spatial RNA-seq and histology images","Abstract":"Cell-cell communication (CCC) is crucial for understanding complex tumor microenvironments, such as interactions between tumor and immune cells. Many of these intricate intercellular interactions are facilitated by ligand-receptor interactions (LRIs) within limited spatial ranges. Spatial Transcriptomics (ST) has advanced LRI identification by detecting spatially co-expressed ligand-receptor pairs (LR pairs). However, the widely used 10x Visium ST platform measures the transcriptome in low-resolution spots and low-capture rate tissues, which limits its ability to detect fine-grained detection of intricate communication patterns. Here, we propose FineST (Fine-grained Spatial Transcriptomics), a bi-modal deep fusion framework that combines spatial RNA-seq with histological images for super-resolution ST expression prediction and detailed LRI discovery at sub-spot and single-cell levels. We evaluated FineST's performance on various datasets, including Visium nasopharyngeal carcinoma (NPC), Xenium breast cancer (BRCA), and Visium HD colorectal cancer (CRC) from 10x Genomics, and compared its accuracy with state-of-the-art methods TESLA and iStar. As a computational toolbox, FineST further detects refined LRIs to provide insights into context-specific cellular interactions and signaling, including fine-grained interacting visualizations of spatially co-expressed LR pairs, distinct CCC pattern clustering, pattern consistency analysis at sub-spot and single-cell levels, and functional enrichment analysis of putative pathways. Our results reveal that FineST's prediction for tumor-restricted genes more closely resembles the ground truth than TESLA and iStar. Moreover, FineST's predicted super-resolved ST achieves superior power in identifying interacting LRIs and uniquely discerns CCC patterns across immune and tumor cells, enabling the enormous potential for high-throughput analysis of histology images in both research and clinical applications."},{"Track":"Computational and Systems Immunology","Room":"522","Weekday":"Tuesday","Date":"16 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Chang Lu","Title":"Computational integration of cellular circuits and immune repertoires based on multilayer network community association","Abstract":"Understanding the role of adaptive immune cells (i.e. T and B lymphocytes) in the immune system is key to treating a range of diseases from autoimmune disorders to cancer and improving vaccination. Recent biotechnological advancements enable spatial mapping of T and\/or B cell receptor sequences (VDJ sequences) within tissues, offering insights into spatially resolving clonal diversity with respect to tissue morphology and gene expression. However, existing computational tools for VDJ analysis primarily focus on single-cell or bulk VDJ sequencing data, necessitating novel methodologies to comprehensively understand how these antigen receptors relate to cellular patterns and signaling._x000D_\nIn this study, we propose a computational strategy to elucidate immune-related sets of intra- and intercellular signaling relationships at a spatial resolution, leveraging both spatial and sequence-based similarities inherent in the adaptive immune repertoires of T and B lymphocytes. Cell deconvolution techniques were first employed to infer the cell-subset-specific expression profiles of genes within the spatially resolved gene expression data. Next, gene co-expression networks were constructed based on the gene expression profiles of individual cell subsets, alongside T-cell and B-cell clonal networks derived from the similarities in clonal sequences. These networks were subsequently integrated into multilayer networks based on the spatial co-localization patterns of tissues. Finally, cross-network community associations were calculated by employing deep multi-graph clustering, thereby unraveling both intracellular signaling dynamics (intralayer relationships) and intercellular communication modalities (interlayer relationships)._x000D_\nThis approach promises profound insights into immune dysregulation, facilitating the exploitation of antigen-specific clones for tailored cellular and antibody-based therapies."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"08:40-08:41","Format":"In Person","Speaker":null,"Title":"Introductory remarks","Abstract":null},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"08:41-09:20","Format":"In Person","Speaker":"Etienne Lord","Title":"Current and new development in Digital Agriculture â€“ Implication of deep learning and robotics in this new data science","Abstract":"Substainable agriculture faces many challenges since it must reconcile both agricultural productivity, while maintaining social, environmental and economic outcomes. Digital agriculture uses the technique of precision agriculture, allowing precise management decisions, while adding some further constraints and algorithmic developments to identify the right crop and the right data needed to take action. Thus, technology opens some solutions to balance those imperatives, a necessary step to the durability of the agricultural sector._x000D_\n_x000D_\nSome of those innovations are the introduction of machine learning, deep learning and now quantum machine learning to extract more information from the now available multilayered datasets. In this talk, we will be revisiting some concepts of digital agriculture and problematic associated with this domain. Then, we will present some of our ongoing machine learning research and robotic platform that could be useful in linking phenology to genetic information. We will also present some imagery datasets developed to help in creating the next generation of machine learning models in agriculture."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Steven Maere","Title":"Single-plant omics : profiling individual plants in a field to identify processes affecting yield","Abstract":"Historically, processes influencing plant phenotypes have been studied intensively under controlled laboratory conditions. However, the results of such controlled lab studies often do not translate well to more complex field settings. To help close this lab-field gap, we developed a new experimental setup to study the wiring of plant traits directly in the field, based on omics profiling, micro-environmental profiling and phenotyping of individual plants of the same genetic background grown in the same field. We used this single-plant omics strategy on winter-type rapeseed (Brassica napus) and built models predicting yield phenotypes of field-grown rapeseed plants from their autumnal leaf gene expression and environmental data layers such as soil nutrient profiles and microbiomes at single-plant resolution. Many of the top yield predictors are linked to developmental processes known to occur in autumn in winter-type B. napus accessions, such as the floral transition. We applied methods from the single-cell field on our single-plant data to further unravel these developmental effects."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"In Person","Speaker":"Hayda Almeida","Title":"Biomarker-based learning for disease prediction in precision dairy farming","Abstract":"Metabolic diseases have great impact on dairy production and animal welfare [1, 2]. Metabolomic profiling has helped identify biomarkers to predict disease risk in dairy cows [3, 4]. Previous studies tend to overlook other biomarkers, like from milk production, which could help predict diseases in cows [5]. Our ensemble learner supports predicting disease risk based on heterogeneous biomarkers from metabolomic and health profiles, milk production history, and herd history._x000D_\nOur datasets contain biomarkers for over 13,700 health events of 1,200 cows from 50 dairy farms in Canada. Biomarkers are captured for a health event e at timepoint t. Given an upcoming lactation Ln, base predictions are obtained for all health events et occurring during lactation Lnâˆ’1._x000D_\nThe ensemble learner averages base predictions for an animal and outputs disease probabilities for lactation Ln. Binary classes are disease or non-disease, based on a curated set of nine most common diseases in dairy cows. Classification performance was evaluated for multiple combinations of biomarkers and classifiers._x000D_\nClassification models based on Logistic Regression and Random Forest classifiers yield best performances, with an average of 0.6 and 0.77 F-measure for disease and non-disease respectively. "},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Vahid Naghashi","Title":"Empowering Dairy Farmers: A Transformer-Based Framework for Informed Decision Making in Dairy Agriculture","Abstract":"In precision livestock, the decision of animal replacement requires an estimation of the lifetime profit of the animal based on multiple factors and operational conditions. In dairy farms, this can be associated with the milk income corresponding to milk production, health condition and herd management costs, which in turn may be a function of other factors including genetics and weather conditions. Estimating the cumulative income from a cow's milk production can be posed as a multivariate time-series prediction task where a late-milk income of a cow can be predicted based on early dairy factors recorded in a sequence of time-steps (lactation months). Furthermore, the predicted milk income would serve as an input to a decision making system for deciding whether to keep or remove an animal in the next lactation period. In this work, a Transformer based model is proposed to predict the cumulative dairy income over the incoming lactation period and further a recommendation procedure is devised for decision making in order to reduce the farmers cost and save their time. In the Transformer, both temporal and inter-variable correlations are captured thanks to the temporal and spatial multi-head attention modules. The proposed framework is assessed using 47749 dairy cows corresponding to more than 5000 herds and the results are compared with the other state-of-the-art models. Our Transformer model outperforms the previous baselines and provides a promising prediction performance with the highest accuracy of 76%, opening the way of better resource management in the dairy industry."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"Kuan Y. Chang","Title":"Anomaly Detection for Smart Aquaculture: Predicting Water Color Changes in Grouper Ponds","Abstract":"Healthy grouper farms thrive on \"green water,\" indicating a balanced ecosystem. A shift to brown water, however, signals potential problems. This study explores how to predict these color changes using water quality data, paving the way for smarter and more sustainable aquaculture._x000D_\nWe analyzed daily monitoring data from six grouper ponds in Fangliao Township, Pingtung, Taiwan, collected from March to December 2018, focusing on water temperature, salinity, and pH. A Long Short-Term Memory (LSTM) model was applied to detect anomalies within these parameters. _x000D_\nOur findings revealed a significant correlation between water quality anomalies and water color changes. Water temperature anomalies were the most effective indicator for early detection of undesired color shifts. Notably, the top 5% of water temperature anomalies successfully predicted over 40% of the water color changes. This means that by targeting the most extreme 5% of water temperature anomalies, farmers can maintain healthier ponds and adopt more sustainable aquaculture methods. Additionally, pH anomalies primarily occurred after the color changes, suggesting a potential consequence rather than a precursor. _x000D_\nThe early warning system enables proactive actions like oxygenation and biocontrol, leading to \"smart\" fish farms with automated water quality management. This will improve efficiency and minimize environmental impact. Future studies will include more factors and data, further refining anomaly detection for sustainable aquaculture._x000D_\nThis study demonstrates the effectiveness of anomaly detection using LSTM models to predict water color changes in grouper ponds. Early detection of water temperature anomalies empowers farmers to manage pond conditions proactively. The prospect of autonomous fish farming systems with real-time monitoring and interventions offers significant potential for sustainable grouper aquaculture. Leveraging innovative technologies and data-driven approaches, we can advance this vital industry sustainably."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Thomas Gisiger","Title":"Extracting meaningful video segments using a movement detection algorithm applied to dairy cow behavior study and welfare monitoring.","Abstract":"Precision dairy farming is essential to creating a food production system that is durable and respects animal welfare and the environment._x000D_\n_x000D_\nThis approach requires gathering many hours of videos with still cameras, which are then used for research, welfare monitoring and tool training. Extracting the video segments with the most meaningful information would allow us to study larger fractions of the recordings taken while gathering the maximum number of observations. This can be framed as a movement detection problem, or, alternatively, a detection problem using traditional or deep learning techniques. However, the latter process of training in a cluttered farm environment might prove challenging._x000D_\n_x000D_\nHere, we propose an algorithm that estimates cow movement in a robust manner without the need for object detection or training. The resulting movement indices, paired with an independently set movement threshold, can then be used to partition videos into episodes where the cow is either immobile or displaying relevant movements and behaviours. This approach takes advantage of typical cow behaviour features and allows for factoring out video sections with repetitive or little\/no movement._x000D_\n_x000D_\nThe experimental setting consists of five 15-minute videos and focuses on measuring the extent to which discarding episodes with little to no movements speeds up the process of labelling behaviours by animal science experts._x000D_\n_x000D_\nThis approach will allow for more complex experiments, novel angles of investigation and larger data-sets to study cow behaviour and interaction with their environment as well as monitoring for welfare status. _x000D_\n"},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Amanda Ashley Boatswain Jacques","Title":"Precision Farming for Profit: Leveraging Profitability Maps and ILPMZ to Optimize Return on Investment and Soil Conservation of Agricultural Fields","Abstract":"Implementing site-specific management practices like profitability zones aids in stabilizing long-term profits while conserving the environment. Profitability maps standardize data by converting yield maps (kg\/hectare) into $\/hectare for different cropping systems. Consequently, they help identify regions in a field which remain more profitable for production, or those that can be transformed into conservation zones. These conservation zones can mitigate economic losses while simultaneously restoring soil health._x000D_\n_x000D_\nVarious studies have explored methods for generating management zones using empirical thresholding, fuzzy clustering, algorithmic approaches, and machine learning. The Integer Linear Programming Management Zone delineation method (ILPMZ) has shown promise since it can solve the problem of generating homogenous and rectangular management zones to optimality. However, ILPMZ application in the literature remains limited to soil property maps like pH, Nitrogen, Phosphorus, and Potassium._x000D_\n_x000D_\nIn this study, we use the ILPMZ method to generate homogeneous rectangular management zones from profitability maps. Yearly profitability maps are derived from multi-year yield and economic data (2016 to 2021) from a field in Quebec, Canada. This data includes high-resolution yield maps, grain prices, and yearly costs and revenue summaries._x000D_\n_x000D_\nThe study underscores the ILPMZ method's potential in optimizing both field profitability and sustainability. It offers valuable insights for future site-specific management practices driven by both ROI and soil conservation."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Voncarlos Marcelo De Araujo","Title":"Revolutionizing Livestock Monitoring: AI-Powered Cow Detection in Farm Environments","Abstract":"In modern agricultural management, ensuring accurate estimations of livestock population density within farm environments is essential. Our focus lies in leveraging AI-powered image analysis to detect cows efficiently and accurately within farm environments. Traditional cow detection methods are labor-intensive and invasive, frequently depending on manual labor for image processing. These methods encounter notable challenges, including variations in camera perspectives, object occlusion, and environmental factors. To address these challenges and develop a generalized detection model capable of identifying cows across different camera types, we propose a novel approach. Our research utilizes a dataset of cow images collected from six distinct types of cameras, with three placed inside barns and three positioned outdoors. From this dataset, we selected 2,000 images of cows, representing diverse camera perspectives. The primary challenge lies in creating a model that can effectively detect cows across these varied camera setups. To tackle this, we devised a semi-manual strategy for annotating cow candidates in images captured by different cameras, considering both single and multiple cow individuals. We evaluated the performance of two state-of-the-art object-detection algorithms, Mask-RCNN and YOLOv5, on this diverse dataset. Our fine-tuned CNN-based detector, trained with semi-manual annotations, achieves robust performance in cow detection across diverse camera setups with an mAP@0.5 of 91.10%. It excels in detecting overlapping or occluded cow individuals, crucial for accurate livestock monitoring, boasting a precision of 93.29% and a recall of 88.14%. This approach streamlines farm management and enhances livestock monitoring and resource allocation in agriculture."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Houda Orchi","Title":"Temporal Synchronization of Multi-View Video for Cattle Movement Analysis in Dynamic Farm Settings","Abstract":"Synchronizing and aligning multi-angle video footage is an intricate task in computer vision, especially in complex environments like barn settings, where observing key behavioral events is critical for understanding dairy cow behavior. Various methods exist to address this challenge, but each has limitations. Frame-by-frame analysis, for instance, is too slow and error-prone for real-time use. Automated techniques like SIFT and SURF are ineffective in low-light or cluttered settings. Additionally, advanced SSM methods falter with dynamic textures and complex movements. To tackle these issues, we introduce a novel synchronization framework consisting of three main components.  _x000D_\n_x000D_\nFirst, it leverages YOLOv8-Oriented Bounding Boxes to extract bounding boxes that identify cow movements across videos, forming the cornerstone to construct self-similarity matrices that enable sophisticated comparisons of video sequences, by analyzing similarities through the dynamic positions and trajectories of the identified cows. Second, we enrich these matrices by incorporating advanced descriptors like the Histogram of Oriented Gradients (HOG) and optical flow. HOG aids in differentiating cow behaviors by analyzing edge directions and strengths while optical flow tracks pixel motion across frames. Third, we employ Dynamic Time Warping to identify minimal distances.  _x000D_\n_x000D_\nOur experiments involve real-time videos captured on the farm, comprising 50 recordings totaling nearly 1500 hours, meticulously curated and annotated by animal scientists. The synchronization quality is examined using assessment metrics like Earth Mover's Distance, Matched Frame Rate, and Mean Temporal Error. This evaluation confirms our proposed framework's effectiveness, demonstrating its potential to revolutionize livestock management and enhance animal welfare through synchronized multi-camera analysis."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"14:40-15:20","Format":"In Person","Speaker":null,"Title":"TBD","Abstract":"TBD"},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":null,"Title":"HaloClass: State-of-the-art salt tolerant protein classification with natural language models","Abstract":"Across the globe, increasing soil salinity poses a unique and dangerous threat to food supplies (Shrivastava et al. 2015). Past work has suggested that modifying critical enzymes is an important approach to confer salt-tolerance adaptations into vital crops (Wani et al. 2020). However, most proteins are destabilized by high salinity, and it is difficult to evaluate stability without slow and expensive experimental testing. Therefore, leveraging computational tools to engineer salt-tolerance into existing proteins could serve as a highly effective approach to create more resilient plant populations.\n\nHere, we present HaloClass, a protein classifier based on ESM-2 (Rives et al. 2021), that significantly outperforms all existing approaches. HaloClass accurately differentiates between similar backbone structures by identifying biologically relevant side-chain variations that confer salt tolerance. HaloClass is the first to achieve perfect accuracy on a benchmark dataset of homologous protein structures. Next, we create and release a new dataset, called HaloTest, to demonstrate broader generalizability than existing approaches. Finally, we simulate a guided protein engineering study on a DNA ligase and prove that HaloClass confidence values correlate with experimental salt-tolerance. Structures, generated with AlphaFold, underscore the difficulty of the task.\n\nTo our knowledge, HaloClass is the most generalizable computational approach ever created for analyzing and designing salt tolerant proteins. In fact, no past computational approach has ever demonstrated high efficacy on a protein engineering study. HaloClass could serve as a valuable tool for agricultural biologists seeking to design novel enzymes with salt tolerance adaptations to help protect the worldâ€™s food supply."},{"Track":"Digital Agriculture","Room":"520c","Weekday":"Tuesday","Date":"16 July","Timespan":"15:40-15:40","Format":"In Person","Speaker":null,"Title":"Final remarks","Abstract":null},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-10:40","Format":"In Person","Speaker":null,"Title":"Welcome & Introductions","Abstract":null},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Patricia Palagi","Title":"Publishing reproducibility, FAIR, and using best practices","Abstract":"In this talk, I will introduce the role scientific journals play in research reproducibility and the adoption of Open and FAIR (Findable, Accessible, Interoperable, and Reusable) principles. I will also discuss best practices in publishing, such as transparency, ethical practices, integrity, and author recognition._x000D_\n"},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Alex Bateman","Title":"Use of LLMs in writing, plagiarism and text recycling ","Abstract":null},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"David Landsman","Title":"Open Access Publishing: pros and cons","Abstract":"Open access for publicly funded research manuscripts has been available for close to 30 years. Following this move, there have been significant changes in both the publishing industry and the submission process of manuscripts to journals. These changes have improved access to scientific information substantially worldwide. I will discuss what have been good developments and what still needs improvement. Audience participation during the discussion session is strongly encouraged. "},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"Thomas Lengauer","Title":"Strategies for managing expectations for submission, review, and publication timeline","Abstract":null},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Scott Edmunds","Title":"Challenges with reviewers: reviewing fatigue, acknowledgement, and dealing with delays and rejections.","Abstract":"This talk will provide some experience from two decades at the coalface of editing journals on how peer review works behind the scenes. And the challenges editors face with the volumes of work, platforms and things that can go wrong. This providing insight for authors and reviewers on the other side on how to deal with these problems. And (if there is time) discussing some potential solutions to improve the situation in the future."},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Jason Papin","Title":"Emerging Trends in Publishing Computational Biology Research","Abstract":"I will discuss initiatives at PLOS Computational Biology to address challenges in reproducibility and open access. I will share the genesis of several pilot projects, lessons learned, and hopeful new directions."},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:15","Format":"In Person","Speaker":null,"Title":"Panel Discussion","Abstract":null},{"Track":"Demystifying the World of Scientific Publishing","Room":"524c","Weekday":"Tuesday","Date":"16 July","Timespan":"12:15-12:20","Format":"In Person","Speaker":null,"Title":"Closing","Abstract":null},{"Track":"Equity and Diversity in Computational Biology Research","Room":"522","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Aritra Bose","Title":"Epidemiological topology data analysis links severe COVID-19 to RAAS and hyperlipidemia associated metabolic syndrome conditions","Abstract":"The emergence of COVID-19 created incredible worldwide challenges but offers unique opportunities to understand the physiology of its risk factors and their interactions with complex disease conditions, such as metabolic syndrome. To address the challenges of discovering clinically relevant interactions, we employed a unique approach for epidemiological analysis powered by Redescription-based TDA (RTDA). Here RTDA was applied to Explorys data to discover associations among severe COVID19 and metabolic syndrome. This approach was able to further explore the probative value of drug prescriptions to capture the involvement of RAAS and hypertension with COVID-19, as well as modification of risk factor impact by hyperlipidemia on severe COVID-19. RTDA found higher-order relationships between RAAS pathway and severe COVID-19 along with demographic variables of age, gender, and comorbidities such as obesity, statin prescriptions, hyperlipidemia, chronic kidney failure and disproportionately affecting African Americans. RTDA combined with CuNA (Cumulant-based Network Analysis) yielded an higher-order interaction network derived from cumulants that furthered supported the central role that RAAS plays. TDA techniques can provide a novel outlook beyond typical logistic regressions in epidemiology. From an observational cohort of electronic medical records, it can find out how RAAS drugs interact with comorbidities, such as hypertension and hyperlipidemia, of patients with severe bouts of COVID-19. Where single variable association tests with outcome can struggle, TDA's higher-order interaction network between different variables enables the discovery of the comorbidities of a disease such as  COVID-19 work in concert."},{"Track":"Equity and Diversity in Computational Biology Research","Room":"522","Weekday":"Monday","Date":"15 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"George Acquaah-Mensah","Title":"Disparate radiomic imaging features are predictive of recurrence events and molecular subtype in Black and White breast cancer patients","Abstract":"Breast cancer is among the deadliest cancers for women in the world. Breast cancer has four distinct molecular subtypes which are determined by gene expression profiling. There are observed differences between the races at the molecular subtype level. Moreover, racial disparity exists between younger White and Black breast cancer patients as it relates to survival and recurrence. Differences have been characterized using molecular data. Imaging data are also valuable in breast cancer diagnostics and can be used to identify these differences as well. In this study, we applied ML techniques to identify molecular subtypes from MRI data deriving from a single-institutional, retrospective collection of 922 biopsy-confirmed invasive breast cancer patients that were collected over a decade. We limited our analyses to Black or White patients who were 50 years or younger at diagnosis (n=346). _x000D_\nRandomForest and AdaBoostM1 in WeKa were applied to over 500 MRI features. We found that imaging features alone or in combination with TCGA gene expression data were predictive of molecular subtype and recurrence events for both racial groups. As an example, the most predictive imaging features in the Breast Fibroglandular Tissue Volume imaging category for Black patients was BreastVol and for White patients was TissueVol_PostCon.Notably, these two imaging features were significantly associated with one race compared to the other.  These results suggest that radiomic imaging data can be used to predict breast cancer recurrence and molecular subtype and can have an impact on clinical outcomes."},{"Track":"Equity and Diversity in Computational Biology Research","Room":"522","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Mona Singh","Title":"Towards Equitable MHC Binding Predictions: Computational Strategies to Assess and Reduce Data Bias","Abstract":"Deep learning tools that predict peptide binding by major histocompatibility complex (MHC) proteins play an essential role in developing personalized cancer immunotherapies and vaccines. In order to ensure equitable health outcomes from their application, MHC binding prediction methods must work well across the vast landscape of MHC alleles. Here we show that there are alarming differences across individuals in different racial and ethnic groups in how much binding data are associated with their MHC alleles. We introduce a machine learning framework to assess the impact of this data disparity for predicting binding for any given MHC allele, and apply it to develop a state-of-the-art MHC binding prediction model that additionally provides per-allele performance estimates. We demonstrate that our MHC binding model successfully mitigates much of the data disparities observed across racial groups. To address remaining inequities, we devise an algorithmic strategy for targeted data collection. Our work lays the foundation for further development of equitable MHC binding models for use in personalized immunotherapies."},{"Track":"Equity and Diversity in Computational Biology Research","Room":"522","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Selene L Fernandez-Valverde","Title":"From CABANA to CABANAnet â€“ Building bioinformatics and knowledge exchange capacity in Latin America.","Abstract":"The CABANA project (Capacity Building for Bioinformatics in Latin America), funded by the UK's Global Challenges Research Fund from 2017 to 2022, aimed to strengthen bioinformatics capacity and broaden applications across Latin America. Focusing on three change areas: communicable diseases, sustainable food production, and protection of biodiversity, the five-year initiative executed a comprehensive array of activities spanning ten countries (9 in Latin America in addition to the UK). These included data analysis workshops, train-the-trainer programs, secondments, eLearning development, knowledge exchange forums, and collaborative research endeavours. CABANA proved highly successful, accomplishing all objectives while generating a substantial regional impact. The project pioneered a novel approach where research needs dictated the training provided. Its enduring legacy encompasses over 800 trainees and multiple scholarly publications. _x000D_\n_x000D_\nIts continuation, CABANAnet (funded by the Chan Zuckerberg Initiative), is building upon this foundation. Premised on computational biology, CABANAnet's capacity-building initiatives will feature research collaborations, workshops, a Train-the-Trainer curriculum, internships, eLearning, knowledge-sharing events, website development, and publications. Synergistic collaborations with complementary bioinformatics efforts such as SoiBio, ISCB, Global Data Alliance, and others will be encouraged. Through this multifaceted program, we aspire to foster positive change across local policies, economies, and scientific advancements throughout the region."},{"Track":"Equity and Diversity in Computational Biology Research","Room":"522","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Nelly Sélem-Mojica","Title":"Closing the computational biology â€œknowledge gapâ€: Spanish Wikipedia as a case study","Abstract":"Motivation: Wikipedia is a vital open educational resource in computational biology. The quality of computational biology coverage in English Wikipedia has improved steadily in recent years. However, there is an increasingly large â€˜knowledge gapâ€™ between computational biology resources in English Wikipedia, and Wikipedias in non-English languages. Reducing this knowledge gap by providing educational resources in non-English languages would reduce language barriers which disadvantage non-native English speaking learners across multiple dimensions in computational biology._x000D_\nResults: Here, we provide a comprehensive assessment of computational biology coverage in Spanish Wikipedia, the second most accessed Wikipedia worldwide. Using Spanish Wikipedia as a case study, we generate quantitative and qualitative data before and after a targeted educational event, specifically, a Spanish-focused student editing competition. Our data demonstrates how such events and activities can narrow the knowledge gap between English and non-English educational resources, by improving existing articles and creating new articles. Finally, based on our analysis, we suggest ways to prioritise future initiatives to improve open educational resources in other languages."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Rolanda Julius","Title":"Expanding data science training and health innovations in Africa: the DS-I Africa Consortium ","Abstract":null},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Marta Lloret-Llinares","Title":"Expanding the ISCB competency framework to describe professionals in bioinformatics core facilities","Abstract":"The International Society of Computational Biology recently revised its competency framework, which facilitates the design of curricula and professional development in the area of bioinformatics. The framework is a minimum standard defining the knowledge, skills and attitudes required in a range of bioinformatics-related professions. It includes a series of career profiles and the minimum levels of competence required for each of them, but lacks information on career progression and possible career pathways. _x000D_\n_x000D_\nDemand for bioinformatics service professionals, many of whom lead or operate  bioinformatics core facilities, continues to grow, but there are no defined roles or career pathways for them, and therefore they vary between institutions. A detailed definition of the knowledge, skills and attitudes required by bioinformatics core facility staff at different career stages would support the development of career progression and facilitate the design of professional development opportunities. A group of professionals in bioinformatics core facilities from the ISCBâ€™s Bioinfo-Core group, the Curriculum Task Force of the ISCB Education Committee, and other interested individuals are working to expand the ISCB framework in this direction. In addition, the effort will create a common language for professionals to communicate about their own competence, which can be used in recruitment and assessment processes._x000D_\n_x000D_\nThe presentation will describe the methodology used to work with the community of bioinformatics core facility scientists, the outcome of the work to expand the ISCB competency framework, and our vision for how this work might support professional development for aspiring and existing bioinformatics core facility staff."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Nilson Coimbra","Title":"Seven Domain Topics in Bioinformatics Education - Refining the ISCB Core Competencies to Access Diversity in Training","Abstract":"The ISCB Regional Student Group of Brazil (RSG-Brazil) is at the forefront of promoting bioinformatics and computational biology education in Brazil. In 2019, RSG-Brazil launched its Educational Committee (EduComm) with a dual mission: to develop educational materials in Portuguese and to assess the efficacy of bioinformatics training in the country. Leveraging the ISCB core competency framework 3.0, the EduComm devised a novel training model to evaluate confidence in various technical aspects of bioinformatics. This model encompasses seven domain topics crucial for bioinformatics education: Biology, Statistics, Computer Science, Ethics, Bioinformatics Applications, Communication, and Professional Development. To gauge the educational needs of the Brazilian bioinformatics community, a survey was conducted in November 2023, collected 375 responses from across 21 states, predominantly from academia. Notably, the majority of respondents identified themselves as Bioinformatics Users, with a significant representation from Undergraduate and Graduate students. The survey revealed regional and profile-specific interests in Bioinformatics Topics, providing valuable insights for curriculum development. Through the efforts of EduComm, RSG-Brazil aims to tailor educational courses to meet the diverse needs of Brazil's computational biology student community. This work presents the findings from the survey conducted by RSG-Brazil's EduComm and highlights the importance of localized educational initiatives in advancing bioinformatics education on a global scale."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Dusanka Nikolic","Title":"Support consistent, competent practice for data science in pathogen genomics: the development of an innovative competency framework","Abstract":"Background: A barrier to the application of pathogen genomics for service delivery is due to a lack of trained healthcare and public health professionals. We developed a pathogen genomics competency framework (CF) outlining the knowledge, skills and attitudes (KSAs) required for performing genomics pipelines, supporting analysis and interpretation._x000D_\n_x000D_\nMethods: A mixed-method approach was applied, starting with a questionnaire aimed at healthcare and research professionals (n=90) to gather data on respondentsâ€™ roles and self-assessed proficiencies using an initial set of pathogen genomics competencies based on domain topics including biology, data science, genomic analysis and interpretation. We employed a consensus-building approach among the representative professionals to establish personas and ascertain the proficiency levels for each persona and competency. A database of competency statements reflecting six proficiency levels (Blooms taxonomy) was developed._x000D_\n_x000D_\nAn iterative design approach was adopted for continuous refinement and updating of the competency framework through consensus building among relevant stakeholders, allowing adaptation to the evolving landscape of genomics across various settings._x000D_\n_x000D_\nResults: The CF covers six clinical and public health roles including Bioinformatician. The CF also encompasses a database of 60 pathogen genomics competencies, containing statements across 6 Blooms levels  for each competency, allowing for the new roles to be created. This CF can evolve, catering to the diverse needs and interests of stakeholders._x000D_\n_x000D_\nConclusions: Providing a link between practice, education and training, the CF aids: institutions determining skills gaps and designing of training programmes; self-assessment by individuals to identify areas for development; the recruitment of staff with relevant skills."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"Live Stream","Speaker":"Sara Fumagalli","Title":"Bridging Education and Research: Data Hunters Workshop Empowering Bioinformatics Education via Microbiome Studies","Abstract":"Ensuring access to the bioinformatics field shall be on the agenda of life sciences degrees. This especially applies to biology-related degrees, where future biologists often ignore its existence due to the scarcity of dedicated courses. _x000D_\n_x000D_\nWe present our contribution to bioinformatics education, using the Data Hunters Workshop as a case study. Kicked off on February 28th, Data Hunters constitutes an ongoing student-science activity for students of the University of Milano-Bicocca, particularly those in the Biotechnology and Biosciences Department. Combining educational engagement with scientific research, this initiative enables students to tackle the key issue of metadata standardizationâ€™s lack in metagenomics, while supporting their learning. We provided a 6-hour lecture and an autonomous hands-on phase, structured as a learn-and-play activity with in-house built online and command-line educational resources. Thus, 29 students gained the fundamentals of metagenomics and Python language. Thanks to these tools, they have now stepped into the role of bioinformaticians, actively curating metadata from 379 amplicon-based and shotgun sequencing projects of the human skin microbiome to collaboratively create a curated metadata collection. Upon the workshopâ€™s conclusion, we will assess the efficacy of our activity via surveys and standardized evaluation scales._x000D_\n_x000D_\nOur workshop represents the effort to bridge educational and research aims, including students as bioinformaticians of the future. As a reflection of the synergy of our objectives, the workshop's outcomes will include significant educational impacts that will lead to the development of a collaborative curated collection of human skin microbiome metadata, alongside the advancement of bioinformatics dissemination."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Nia Hughes","Title":"Increasing training access with a new Distributed workshop model","Abstract":"The Canadian Bioinformatics Workshop series (CBW), hosted by Bioinformatics.ca, has provided short-form bioinformatics and computational biology training for the past 25 years. Among the challenges of offering training across Canada is the populationâ€™s significant geographical spread: for single-location workshops, potential trainees often find that the cost of travel is often a prohibitive factor. While virtual training resolves this access limitation, virtual training lacks the relationship and community building opportunities inherent in in-person events._x000D_\n_x000D_\nTo address this issue, CBW developed a Distributed model for training delivery. This model involves multiple venues simultaneously running the same workshop, with instructors, TAs, and students live at each location while lectures are broadcast back and forth. We piloted this format with our successful Metabolomics Analysis workshop in 2023, using MontrÃ©al and Edmonton as locations.  The model allowed us to double our participant capacity and reach individuals who would not have otherwise been able to attend while also decreasing our faculty travel costs and environmental footprint. _x000D_\n_x000D_\nHere, we describe the logistics of the Distributed model and provide a detailed case study of our pilot workshop. We review the successes and challenges of our first Distributed offering and provide advice for educators interested in implementing this model themselves."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Anna Swan","Title":"Learning through play: using games in bioinformatics training","Abstract":"Games are increasingly recognized as effective tools in training to engage participants and facilitate the learning of new concepts. This approach can be applied in bioinformatics training to a range of contexts, including face-to-face courses and self-paced eLearning. _x000D_\n_x000D_\nTo illustrate our experience of using games in face-to-face bioinformatics training we describe two games focused on the learning objective â€˜recall the range of EMBL-EBI data resourcesâ€™. The first, a competitive card game, was designed for an informal environment, such as during a break in a course or as an icebreaker. During the game learners discover the range of EMBL-EBI data resources and some potential data management issues whilst interacting socially. The second, a treasure hunt used within a session as part of a face-to-face course, takes learners through a series of challenges requiring the exploration of selected EMBL-EBI data resources. Participants work together in pairs and compete with others to complete the challenges. The treasure hunt is also available as an interactive eLearning course._x000D_\n_x000D_\nFor asynchronous eLearning, small interactive elements, such as crosswords, drag-and-drop games, and interactive scenarios can also be effective in introducing new concepts. This kind of interactivity encourages users to continue engaging with the self-paced learning, despite the physical lack of peers who are focused on the same learning objectives._x000D_\n_x000D_\nFeedback has demonstrated that learners appreciate the interactivity and alternative ways of learning that games can bring to bioinformatics training. Trainers also benefit from the creative process of game development whilst maintaining focus on specific learning outcomes."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Priyanka Surana","Title":"Empowering Global Genomic Innovation: The BioDev Network's Educational Blueprint for Cutting-Edge Science and Inclusion","Abstract":"The Biodata Developers' (BioDev) Network is committed to advancing open-access genomic and life science discovery globally through strategic educational initiatives and community-driven practices. Our targeted communities of practice in bioinformaticsâ€”Machine Learning & Artificial Intelligence, Bioinformatics Workflows, and Software & Web Developmentâ€”each boast over 100 members and host monthly or more frequent meet-ups. These communities facilitate regular engagements through seminars, workshops, and symposia, promoting continuous skill enhancement and fostering a cutting-edge technological culture._x000D_\n_x000D_\nOur Future Innovators Mentorship Scheme, a cornerstone of our educational framework, is designed to equip early-career scientists from underrepresented regions with critical skills and exposure. This program integrates hands-on sessions and a structured curriculum, covering a broad range of practical and theoretical topics. With over 300 applications for its inaugural round, it showcases a robust demand for such transformative experiences._x000D_\n_x000D_\nAdditionally, our Horizon Scanning Workshops provide a forward-looking platform that explores emerging trends and challenges in genomics and bioinformatics. Each workshop engages experts to discuss future scientific opportunities and the ethical, legal, and social implications of new technologies, fostering a multidisciplinary dialogue that prepares the community for upcoming advancements._x000D_\n_x000D_\nAll our programs are free and offered in a hybrid format to reduce barriers to entry and promote inclusivity. Together, these initiatives not only strengthen the capabilities of individual scientists and developers but also enhance the collective proficiency of the global bioinformatics community. By focusing on cutting-edge practices, mentorship, and proactive exploration of future trends, the BioDev Network is shaping a more inclusive and innovative scientific landscape."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":null,"Title":"Report from New York City: Bioinformatics Education Summit 2024","Abstract":"This session will provide a summary of topics covered at the Bioinformatics Education Summit (BES), an annual international gathering of the bioinformatics education committee.  The BES has become the primary annual working meeting of the global bioinformatics education committee, where participants gather from around the world to share strategies and develop teaching materials. The session will report back on major outcomes of the 2024 Summit, which was held May 20-22 in New York, USA."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Pavlin G. Poličar","Title":"Teaching Bioinformatics through the Analysis of SARS-CoV-2: Project-Based Training for Computer Science Students","Abstract":"We learn more effectively through experience and reflection than through passive reception of information. Bioinformatics offers an excellent opportunity for project-based learning. Molecular data is abundant and accessible in open repositories, and important concepts in biology can be rediscovered by reanalyzing the data. In the manuscript, we report on five hands-on assignments we designed for masterâ€™s computer science students to train them in bioinformatics. These assignments are the cornerstones of our introductory bioinformatics course and are centered around the study of the SARS-CoV-2 virus. They assume no prior knowledge of molecular biology but do require programming skills. Through these assignments students learn about genomes and genes, discover their composition and function, relate SARS-CoV-2 to other viruses, and learn about the bodyâ€™s response to infection. Student evaluation of the assignments confirms their usefulness and value, their appropriate mastery-level difficulty, and their interesting and motivating storyline."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Nelly Sélem-Mojica","Title":"Closing the computational biology â€œknowledge gapâ€: Spanish Wikipedia as a case study","Abstract":"Motivation: Wikipedia is a vital open educational resource in computational biology. The quality of computational biology coverage in English Wikipedia has improved steadily in recent years. However, there is an increasingly large â€˜knowledge gapâ€™ between computational biology resources in English Wikipedia, and Wikipedias in non-English languages. Reducing this knowledge gap by providing educational resources in non-English languages would reduce language barriers which disadvantage non-native English speaking learners across multiple dimensions in computational biology._x000D_\nResults: Here, we provide a comprehensive assessment of computational biology coverage in Spanish Wikipedia, the second most accessed Wikipedia worldwide. Using Spanish Wikipedia as a case study, we generate quantitative and qualitative data before and after a targeted educational event, specifically, a Spanish-focused student editing competition. Our data demonstrates how such events and activities can narrow the knowledge gap between English and non-English educational resources, by improving existing articles and creating new articles. Finally, based on our analysis, we suggest ways to prioritise future initiatives to improve open educational resources in other languages."},{"Track":"Education","Room":"521","Weekday":"Saturday","Date":"13 July","Timespan":"17:20-18:00","Format":"In Person","Speaker":"Francis Ouellette","Title":"Celebrating 25 Years of Bioinformatics.ca","Abstract":"Over the past 25 years, Bioinformatics.ca has been at the forefront of bioinformatics education in Canada. Initially launched as a series of introductory workshops spanning one to two weeks spearheaded by the Canadian Genetics Diseases Network (CGDN) in British Columbia, the Canadian Bioinformatics Workshop (CBW) Series identified the need for specialized, cutting-edge course material. Responding to this demand, the CBW adapted its teaching model and expanded its scope through a new partnership with the Ontario Institute for Cancer Research (OICR)._x000D_\n_x000D_\nThis evolution has seen the CBW Series diversify its offerings and significantly enhance its impact. To date, over 170 workshops have been successfully conducted across 12 cities in Canada and even in New York, thanks to the dedication of key instructors, project managers, and numerous sponsors. These workshops have provided invaluable training to thousands of participants, helping to advance the field of bioinformatics._x000D_\n_x000D_\nIn this presentation, we will delve into the rich history of Bioinformatics.ca, highlighting the major successes, challenges, and lessons learned over the past quarter-century. We will also explore the collaborative efforts and innovative strategies that have enabled the CBW Series to maintain its relevance and effectiveness in an ever-evolving scientific landscape. "},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Bertrand Marchand","Title":"Median  and Small Parsimony Problems on RNA trees","Abstract":"Motivation:_x000D_\nNon-coding RNAs (ncRNAs) express their functions by adopting molecular structures. Specifically, RNA secondary structures serve as a relatively stable intermediate step before tertiary structures, offering a reliable signature of molecular function. Consequently, within an RNA functional family, secondary structures are generally more evolutionarily conserved than sequences. Conversely, homologous RNA families grouped within an RNA clan share ancestors but typically exhibit structural differences. Inferring the evolution of RNA structures within RNA families and clans is crucial for gaining insights into functional adaptations over time and providing clues about the Ancient RNA World Hypothesis._x000D_\nResults:_x000D_\nWe introduce the median problem and the small parsimony problem for ncRNA families, where secondary structures are represented as leaf-labelled trees. We utilize the Robinson-Foulds (RF) tree distance, which corresponds to a specific edit distance between RNA trees, and a new metric called the Internal-Leafset (IL) distance. While the RF tree distance compares sets of leaves descending from internal nodes of two RNA trees, the IL distance compares the collection of leaf-children of internal nodes. The latter is better at capturing differences in structural elements of RNAs than the RF distance, which is more focused on base pairs. We study the theoretical complexity of the median problem and the small parsimony problem under the three distance metrics and various biologically-relevant constraints, and we present polynomial-time maximum parsimony algorithms for solving some versions of the problems. Our algorithms are applied to ncRNA families from the RFAM database, illustrating their practical utility."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"11:10-11:20","Format":"In Person","Speaker":"Wend Yam Donald Davy Ouedraogo","Title":"Inferring transcript phylogenies based on precomputed groups of conserved transcripts","Abstract":"Alternative Splicing (AS) is a mechanism in eukaryotic gene expression by which different combinations of introns are spliced to produce distinct transcript isoforms from a gene. Recent studies have highlighted that the transcript isoforms of human genes are often conserved in orthologous genes from various species. The conserved transcripts are referred to as transcript orthologs, and the identification of transcript ortholog groups provides valuable insights for studying their functions. Exploring the evolutionary histories of transcripts enhances our understanding of their proteins functions and their origins. It also allows us to better understand the role of alternative splicing in transcript evolution._x000D_\nIn a previous work(DOI: 10.1007\/978-3-031-36911-7_2), we addressed the problem of inferring orthology and paralogy relations at the transcript level. In this work, we focus on the reconstruction of transcript evolutionary histories. We present a progressive supertree construction algorithm that relies on a dynamic programming approach to infer a transcript phylogeny based on precomputed clusters of orthologous transcripts._x000D_\nWe applied our algorithm to transcripts from simulated gene families, as well as to two case studies involving the transcripts of real gene familiesâ€”specifically, the TAF6 and PAX6 gene families from the Ensembl-Compara database. The results align with those of previous studies aimed at reconstructing transcript phylogenies, while improving the computing time. The results also show that accurate transcript phylogenies can be obtained by first inferring accurately the pairwise homology relationships among transcripts and then using the latter to compute a phylogeny that agrees with the homology relationships."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Louxin Zhang","Title":"A Representation for Phylogenetic Trees and Networks","Abstract":"Good representations for phylogenetic trees and networks are important for human-computer interface and implementation of scalable heuristic methods for inference of evolution for genes, genomes and species. We present a new representation for phylogenetic trees. It maps every binary tree on n taxa to a string of taxa in which each taxon appears exactly twice. The new representation is i) shorter than the Newick format, ii) bijective in the space of phylogenetic trees and iii) easy for recovering tree edges. Using this new format, we introduce a tree operation that enables to traverse tree space in at most 2n steps and a new metric for tree comparison that is computable in linear time and correlated with the Subtree Prune and Regraft distance better than the Robinson-Foulds distance. The new representation can be further generalized to the so-called tree-child networks."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Anshu Gupta","Title":"Accurate, scalable, and fully automated inference of species trees from raw genome assemblies using ROADIES","Abstract":"Species tree inference is crucial in advancing our understanding of evolutionary relationships of life on Earth and has immense significance for diverse biological and medical applications. Extensive genome sequencing efforts are currently in progress across a broad spectrum of life forms, unraveling intricate branching patterns within the tree of life. However, estimating species trees starting from raw genome sequences is quite challenging, and the current cutting-edge methodologies require a series of error-prone steps involving gene annotations, orthology inference, and accounting for gene tree discordances, which are neither entirely automated nor standardized and require substantial human intervention. Therefore, we present ROADIES, a novel pipeline for species tree inference from raw genome assemblies that is fully automated, easy to use, scalable, free from reference bias, and provides flexibility to adjust the tradeoff between accuracy and runtime. The ROADIES pipeline eliminates the need to align whole genomes, choose a single reference species, or pre-select loci such as functional genes found using cumbersome annotation steps. Moreover, it leverages recent advances in phylogenetic inference to allow multi-copy genes, eliminating the need to detect orthology. Using genomic datasets released from large-scale sequencing consortia (Birds 10K Genome Project, Zoonomia) across three diverse life forms (placental mammals, pomace flies, and birds), ROADIES infers species trees that are comparable in quality with the state-of-the-art approaches while achieving >100x speedup compared to the conventional pipelines. ROADIES supports various modes of operation and is expected to improve the accuracy, speed, scalability, and reproducibility of phylogenomic analyses."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Chun Wu","Title":"Generalized c\/Âµ Ratio Test for Detecting Molecular Adaptation: Beyond the conventional Ka\/Ks Ratio test without Assuming Synonymous Site Neutrality or Limitation to Translated Regions","Abstract":"The 60-year debate in evolutionary biology over \"neutralist-selectionist\" views demands a robust method to measure fitness changes due to mutations, yet the Ka\/Ks ratio test, despite its prominence, has significant limitations. This test, which assesses fitness changes in the genome's Translated Region (TR) based on non-synonymous (Ka) and synonymous (Ks) substitution rates, presupposes the neutrality of synonymous mutationsâ€”a notion increasingly challenged by evidence highlighting their non-neutral impacts in replication, transcription, and translation processes. Our previous work (Comp in Bio and Med 153 (2023) 106522) introduced the relative substitution rate c\/Âµ test (c: a mean value of Ka and Ks; Âµ: mutation rate) as a versatile alternative, offering a broader application without assuming synonymous site neutrality. This paper derives a general equation linking c\/Âµ with Ka\/Âµ, Ks\/Âµ, and Ka\/Ks, demonstrating c\/Âµ's superior accuracy in quantifying fitness changes across both TR and UTR. Through a comparative analysis of the c\/Âµ and Ka\/Ks tests across 10 genes, 11 UTRs, and significant SARS-CoV-2 mutations, using three independent genomic datasets from December 2019 to July 2021, we validate our molecular adaptation predictions with activity data from existing literature. Our findings advocate for the c\/Âµ test as a more effective tool than the traditional Ka\/Ks test, potentially resolving the longstanding debate in evolutionary biology by accommodating non-neutral effects at synonymous sites and extending applicability beyond the TR. This method was applied to over 2000 viruses with at least 50 genomes sequences, the preliminary results will be discussed."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Christophe Dessimoz","Title":"AlphaHOGs, a protein structure-based reference classification to improve orthology inference","Abstract":"The increasing availability of genomic sequences is driving forward our understanding of the diverse life forms on Earth. However, the ability to generalize organism-specific knowledge is limited by how well we relate genomes to each other. Genomes can be compared in terms of orthologous genes â€” genes of different species that derive from a single gene in the last common ancestor. Currently, the majority of orthology prediction software is based on the amino acid sequence, the most abundant information about proteins. However, the sequence signal of distantly related genes is weak, distributed in saturated positions, and confounded by evolutionary forces. In this work, I show how the more conserved protein 3D-structure can improve orthology prediction. I devised a method that combines sequence k-mers with k-mers generated from a local structural alphabet. The enriched sets of k-mers can be used to generate a reference classification of proteins into Hierarchical Orthologous Groups (HOGs) â€” a coarse-grained representation of protein families. The structure-informed reference HOGs, here named AlphaHOGs, can be exploited to infer orthology in thousands of proteomes, by using the recently developed software FastOMA. As a test case, we reconstruct the ancestral genome of the first multicellular animal with an unprecedented resolution, paving the way to higher-level analyses such as the ancestral gene content and protein interaction network, potentially shedding light on the current uncertainties about the origin of the animal lineage."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Viola Chen","Title":"Joint inference of cell lineage and mitochondrial evolution from single-cell sequencing data","Abstract":"Eukaryotic cells contain organelles called mitochondria that have their own genome. Most cells contain thousands of mitochondria which replicate, even in non-dividing cells, by means of a relatively error-prone process resulting in somatic mutations in their genome. Because of the higher mutation rate compared to the nuclear genome, mitochondrial mutations have been used to track cellular lineage, particularly using single-cell sequencing that measures mitochondrial mutations in individual cells. However, existing methods to infer the cell lineage tree from mitochondrial mutations do not model heteroplasmy, which is the presence of multiple mitochondrial clones with distinct sets of mutations in an individual cell. Single-cell sequencing data thus provides a mixture of the mitochondrial clones in individual cells, with the ancestral relationships between these clones described by a mitochondrial clone tree that must be concordant with the cell lineage tree. We formalize the problem of inferring a concordant pair of a mitochondrial clone tree and a cell lineage tree from single-cell sequencing data as the NESTED PERFECT PHYLOGENY MIXTURE (NPPM) problem. We derive an algorithm, MERLIN, to solve the NPPM problem. We show on simulated data that MERLIN outperforms existing methods that do not model mitochondrial heteroplasmy nor the concordance between the mitochondrial clone tree and the cell lineage tree. We use MERLIN to analyze single-cell whole genome sequencing data of 5220 cells of a gastric cancer cell line and show that MERLIN infers a more biologically plausible cell lineage tree and mitochondrial clone tree compared to existing methods."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"14:50-15:00","Format":"In Person","Speaker":"Jonghyun Lee","Title":"Tracking tumorigenesis and the transition state through copy number variation-based pseudotime","Abstract":"Driver mutations for different cancer types are extensively categorized. However, there appears to be no fixed number of driver mutations that guarantee the transition from normal to cancer cells. Therefore, we hypothesized that there are ranges of genetic alterations where both normal and cancer cells coexist, where two types of cells share similar genetic backgrounds while exhibiting vastly different phenotypes. By leveraging the technical advances of single-cell sequencing that captures the characteristics of thousands of cells, we sought to identify cells that belong to the transition state, where the tumor and normal cells share similar genetic alterations._x000D_\nOne of the well-established factors regarding the genetic changes during tumorigenesis is the accumulation of aneuploidy. Copy number variation (CNV) inference algorithms such as CopyKat and SCEVAN deduce the aneuploidy from the single-cell expression data. we utilized the accumulation of CNV to construct a pseudotime to describe the genetic changes during the tumorigenesis._x000D_\nWe found that there is indeed genetic background overlap between the tumor and the normal epithelial cells of breast cancer, from both patient data and mouse model. Cells within the transition state appear to share CNV events, which are represented by a NJ-based tree. These transition cells are also located between tumor and normal cell clusters in expression space. This result demonstrates that sufficient sampling can identify per-malignant normal cells with similar mutation profiles of tumor cells, which may aid in early detection and prevention of oncogenesis."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Abigail Bunkum","Title":"SPICE: Probabilistic reconstruction of copy-number evolution in cancer","Abstract":"Somatic copy number alterations (SCNAs) are frequent genetic alterations that accumulate in tumour cells during cancer evolution and amplify or delete large genomic regions. SCNAs are implicated to drive cancer progression, providing cancer cells with the ability to metastasise or resist treatment. Therefore, cancer sequencing studies aim to reconstruct the evolutionary history of SCNAs to investigate their role in cancer progression. Whilst several related phylogenetic methods have been introduced, these methods rely on the reconstruction of a single tumour phylogeny explaining SCNA evolution, discarding the innate uncertainty of this complex problem. In fact, modelling SCNA evolution is challenging and many different explanations for SCNA evolution are equally plausible. Therefore, reconstructing a single phylogeny might hinder the ability to accurately characterise SCNA evolution. _x000D_\n_x000D_\nIn this work, we introduce SPICE (Subclone Probability Inference of Copy-number Evolution), a novel algorithm that enumerates equally plausible explanations of SCNA evolution, enabling the estimation of the probabilities of SCNA events. We show, using a novel, realistic simulation framework, that SPICE outperforms previous methods on simulated datasets by combining multiple inferred phylogenies. To highlight the impact of our method, we applied SPICE to 49 bulk samples from metastatic prostate cancers to detect the presence of well-known driver cancer genes that appear to be recurrently affected by similar events in the same tumour, providing evidence for parallel evolution. Finally, we leverage information regarding the uncertainty of inferred phylogenetic topologies to identify novel metastatic migration patterns and characterise the probability of migrations between different tumour sites."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Meaghan Parks","Title":"Uncovering Cancer's Fitness Landscape","Abstract":"CRISPR-based genome editing technologies have enabled massively-parallel genomic screens, such as DepMap â€“ a Broad Consortium effort to catalog gene knockouts in cancer cell lines. These projects find that the growth effects of a mutation depend heavily on the background genotype of a cell. Evolutionary theory has studied the effects of background genotype on mutations for generations and has uncovered general patterns across the tree of life These patterns found in evolving populations have culminated in a â€˜Geometric Modelâ€™ of adaptation that has successfully predicted the effects of novel combinations of mutations in yeast and E. coli. This model could in principle be applied to DepMap and other massively-parallel genomic screens to learn genotype to phenotype to fitness mappings and potentially predict the evolution of a population. Fitting this model to large-scale real data, however, is challenging because the model infers a latent (hidden) space of phenotypes with mathematical symmetries which confuse regression methods. Here, we present a methodology for fitting a Geometric Model of adaptation to large-scale genomic screens that is guaranteed to converge to a single inferred background genotype for any mutant. This methodology eliminates rotational, translational, and permutation symmetries in the inferred phenotype space and successfully reconstructs genotype to phenotype to fitness mappings of simulated cancer cell line knockout data. Thus, making comprehensive quantitative models of genotype to phenotype to fitness mappings possible in a multitude of diseases, which in turn will allow us to infer phenotypic complexity and predict treatment response."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Valeriia Vasylieva","Title":"Measuring pseudogenes' kinship to unravel overlooked evolutionary patterns","Abstract":"Pseudogenes are defined as copies of protein-coding genes that have lost their ability to encode proteins and are functionless elements of our genomes. Yet, thousands are transcribed, and hundreds encode proteins. These discoveries question the definition of pseudogenes and their roles in our genome. To unravel the contribution of pseudogenes to the evolution of our genomes, we need to understand their origin. However, identification of the parental transcript and parental gene of pseudogenes is complex and currently incomplete. PsiCube database is the most up-to-date reference for pseudogene-parental gene annotation in human, yet it only references parental genes for 48% (8,225) of the pseudogenes currently annotated in Ensembl (17,349). _x000D_\nHere, we present a method based on the Mash distance, commonly used in metagenomics approaches, to measure kinships between transcripts of pseudogenes and protein-coding genes. Our strategy outperforms PsiCube, without any significant biases for sequence length, complexity, or GC content. We applied our method to unravel the evolutionary relationships between GAPDH and its pseudogenes. Mash distance was able to confidently separate unrelated sequences from the GAPDH paralog and from the parental GAPDH. Interestingly, our methodology highlighted pseudogenes with other pseudogenes as their closest related sequence. We expanded our Mash distance analysis to the whole human genome and identified pseudogenes arising from other pseudogenes amongst many gene families, including in loci associated with diseases. _x000D_\nOur work highlights an overlooked mechanism of gene evolution where pseudogenes can arise from existing pseudogenes and contribute to the diversity and evolution of our genomes."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Dustin Hanke","Title":"Pseudogenes in plasmid genomes reveal past transitions in plasmid mobility","Abstract":"Evidence for gene non-functionalization due to mutational processes is found in genomes in the form of pseudogenes. Pseudogenes are known to be rare in prokaryote chromosomes, with the exception of lineages that underwent an extreme genome reduction (e.g., obligatory symbionts). Much less is known about the frequency of pseudogenes in prokaryotic plasmids; those are genetic elements that can transfer between cells and may encode beneficial traits for their host. Non-functionalization of plasmid-encoded genes may alter the plasmid characteristics, e.g., mobility, or their effect on the host. Analyzing 10,832 prokaryotic genomes, we find that plasmid genomes are characterized by threefold-higher pseudogene density compared to chromosomes. The majority of plasmid pseudogenes correspond to deteriorated transposable elements. A detailed analysis of enterobacterial plasmids furthermore reveals frequent gene non-functionalization events associated with the loss of plasmid self-transmissibility. Reconstructing the evolution of closely related plasmids reveals that non-functionalization of the conjugation machinery led to the emergence of non-mobilizable plasmid types. Examples are virulence plasmids in Escherichia and Salmonella. Our study highlights non-functionalization of core plasmid mobility functions as one route for the evolution of domesticated plasmids. Pseudogenes in plasmids supply insights into past transitions in plasmid mobility that are akin to transitions in bacterial lifestyle."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Yuri Wolf","Title":"Long range segmentation of prokaryotic genomes by gene age and functionality","Abstract":"Bacterial and archaeal genomes encompass numerous operons that typically consist of two to five genes. On larger scales, however, gene order is poorly conserved through the evolution of prokaryotes. Nevertheless, non-random localization of different classes of genes on prokaryotic chromosomes could reflect important functional and evolutionary constraints. We explored the patterns of genomic localization of evolutionarily conserved (ancient) and variable (young) genes across the diversity of bacteria and archaea. Nearly all bacterial and archaeal chromosomes were found to encompass large segments of 100-300 kilobases that were significantly enriched in either ancient or young genes. Similar clustering of genes with lethal knockout phenotype (essential genes) was observed as well. Mathematical modeling of genome evolution suggests that this long-range gene clustering in prokaryotic chromosomes reflects perpetual genome rearrangement driven by a combination of selective and neutral processes rather than evolutionary conservation."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Yiqing Wang","Title":"The evolution of antibiotic resistance islands occurs within the framework of plasmid lineages","Abstract":"Bacterial pathogens carrying multidrug resistance (MDR) plasmids are a major threat to human health. The acquisition of antibiotic resistance genes (ARGs) in plasmids is often facilitated by mobile genetic elements that copy or translocate ARGs between DNA molecules. The agglomeration of mobile elements in plasmids generates resistance islands comprising multiple ARGs. However, whether the emergence of resistance islands is restricted to specific MDR plasmid lineages remains understudied. Here we show that the agglomeration of ARGs in resistance islands is biased towards specific large plasmid lineages. Analyzing 6,784 plasmids in 2,441 Escherichia, Salmonella, and Klebsiella isolates, we quantify that 84% of the ARGs in MDR plasmids are found in resistance islands. We furthermore observe the rapid evolution of ARG combinations in resistance islands. Most regions identified as resistance islands are shared among closely related plasmids but rarely among distantly related plasmids. Our results suggest the presence of barriers to the dissemination of ARGs between plasmid lineages, which are related to plasmid genetic properties, host range, and the plasmid evolutionary history. The agglomeration of ARGs in plasmids is attributed to the workings of mobile genetic elements that operate within the framework of existing plasmid lineages."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Adena Collens","Title":"Elucidating the Co-Evolution and Genetic Diversity of Acquired Phototrophy in Marine Worm Convolutriloba longifissura","Abstract":"While instances of acquired phototrophy can be found across the eukaryotic tree of life, much about the evolution and maintenance of these endosymbiotic interactions remains unknown. Marine acoel worms are one such group that host photosynthetic algae (Tetraselmis sp.) within their tissues. For example, past work shows that unfed Convolutriloba retrogemma with endosymbiotic Tetraselmis algae lose less biomass in the light than in the dark, suggesting a transfer of photosynthethates to the host. However, the mechanisms and likely benefits to host and alga have yet to be described. Further, genetic diversity of the alga T. convolutae (associated with acoel Symsagittifera roscoffensis) is minimal even across diverse host genotypes and geographies, suggesting an intimate, long-term coevolution between acoel worm hosts and their algae. _x000D_\nOur study centers on Convolutriloba longifissura - Tetraselmis sp. photosymbiosis, a case for which even less is known. We present the first genetic characterization of the intercellular algae using low-pass whole genome sequencing data. Using short-read Illumina data, we assembled and annotated organellar genomes from both the acoel worm and the Tetraselmis alga, as well as nuclear ribosomal repeats from the acoel worm. We conducted phylogenetic analyses using several assembled markers to elucidate the relationships of C. longifissura and Tetraselmis sp. to existing sequencing data from acoel worms and green alga, respectively. In light of these findings, we intend to expand to the comparative analysis of transcriptome data to illuminate possible indicators, mechanisms, and interactions of this likely acquired phototrophic interaction."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Yannis Nevers","Title":"Quality assessment of gene repertoires with OMArk","Abstract":"The amount and diversity of new genomes getting sequenced across the world opens the doors for large-scale comparative genomics. Thus, reliably ensuring the quality of the protein-coding gene repertoire derived from these data before including them in an analysis is becoming more critical. State-of-the-art genome annotation assessment tools measure some aspects of this quality but are blind to errors such as gene over-prediction or contamination.  _x000D_\nWe developed OMArk, a method that relies on fast alignment comparisons to precomputed gene families from the OMA orthology database. By identifying differences between a gene annotation and the typical gene repertoires of closely related species, OMArk assesses not only the completeness, but also the consistency of the gene repertoire as a whole compared to closely related species. This includes classification of genes with no relatives in close species, with dubious gene models, or those resulting from contamination. Through this global assessment, OMArk helps point out flaws in any given annotation. _x000D_\nWe validated OMArkâ€™s performances on simulated data, then performed an analysis of over 8,000 proteomes from public reference databases (UniProt, Ensembl, RefSeq..). We identified and confirmed cases of contaminations in multiple proteomes, characterized the improvements in gene repertoire quality resulting from improvement in genome assemblies, and found evidence of systematic errors induced by annotation pipelines in certain datasets. OMArk is available on GitHub (https:\/\/github.com\/DessimozLab\/OMArk), as a Python package on PyPi, as a bioconda package, and as an interactive online webserver at https:\/\/omark.omabrowser.org."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Ethan Wolfe","Title":"Leveraging machine learning to predict antimicrobial resistance in ESKAPE pathogens","Abstract":"Since the clinical introduction of antibiotics in the 1940s, antimicrobial resistance (AMR) has become an increasingly dire threat to global public health. Pathogens acquire AMR much faster than we discover new drugs, warranting new methods to better understand the molecular underpinnings of AMR. Traditional approaches for detecting AMR in novel bacterial strains require time-consuming, labor-intensive assays. Here, we introduce a machine learning approach to identify AMR-associated features. We focus on six highly drug-resistant bacterial pathogens responsible for most nosocomial infections: the â€œESKAPEâ€ pathogens. We use all NCBI-PGAP-annotated ESKAPE genomes with known AMR phenotype data from the Bacterial and Viral Bioinformatics Resource Center (BV-BRC). Then, for all complete and WGS genomes for each ESKAPE species, we cluster similar genes and construct pangenomes with Panaroo. To uncover the molecular mechanisms behind drug-\/drug family-specific resistance and cross-resistance, we train logistic regression and random forest models on our pangenomes, which include antibiotic resistance\/susceptibility labels per genome. The models are tested rigorously to yield ranked lists of AMR-associated genes and protein domains. In addition to recapitulating known AMR genes, our models have identified novel candidates for individual and cross-resistance mechanisms that await experimental validation. Our holistic approach promises thorough, reliable prediction of existing or developing resistance in newly identified pathogen genomes, along with mechanistic molecular contributors of resistance."},{"Track":"EvolCompGen","Room":"518","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Evan Brenner","Title":"Predicting pathogen preferences and host adaptation by leveraging microbial genomics and machine learning","Abstract":"Most emerging infectious diseases (EIDs) of humans originate in animals and are transmitted through zoonotic spillover events. However, the genetic determinants underlying host adaptation or host switching are often unclear. We hypothesize that genomic markers of pathogen adaptation to different hosts are detectable and can yield valuable insights into EID pathobiology. Utilizing publicly available databases, millions of bacterial and viral genomes with metadata, including their hosts of origin, are accessible for study. To leverage these, we are training machine learning models that associate pathogen genetic elements (e.g., genes, k-mers) with host labels. Our models are simple and interpretable (e.g., decision trees), run with reasonable computational requirements, and have been tested on a sampling of phylogenetically distinct bacterial and viral pathogens. _x000D_\n_x000D_\nOur preliminary results have yielded high predictive performance for bacterial and viral pathogens, and top-ranked features in these models often pinpoint genomic elements that are 1) associated with horizontal gene transfer elements, and 2) demonstrated to play biologically relevant roles to host adaptation in prior literature. We will expand these models to new species, build more complex models that incorporate additional levels of genomic information (e.g., protein domains), and begin testing performance across species or genera rather than solely within. These advances offer promise in assessing threats to different host populations posed by new EIDs."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"08:40-09:00","Format":"In Person","Speaker":"Gary Hu","Title":"Maximum Likelihood Phylogeographic Inference of Cell Motility and Cell Division from Spatial Lineage Tracing Data","Abstract":"Recently developed spatial lineage tracing technologies induce somatic mutations at specific genomic loci in growing cells and then measure these mutations in the sampled cells along with their physical locations. These technologies enable high-throughput studies of developmental processes over space and time. However, these applications rely on accurate reconstruction of a spatial cell lineage tree describing the history of bothcell divisions and locations. We demonstrate that standard phylogeographic models based on Brownian motion are inadequate to describe the symmetric spatial displacement of cells during cell division. We introduce a new model for cell motility that includes symmetric displacements of daughter cells from the parental cell followed by independent diffusion of daughter cells. We show that this model more accurately describes the locations of cells in a real spatial lineage tracing of Drosophila melanogaster embryos. Combining the spatial model with an evolutionary model of DNA mutations, we obtain a comprehensive model for spatial lineage tracing, namely spalin. Using this model, we estimate time-resolved branch lengths, spatial diffusion rate, and mutation rate. On both simulated and real data, we show that the proposed method accurately estimates all parameters while the Brownian motion model overestimates spatial diffusion rate in all test cases. In addition, the inclusion of spatial information improves accuracy of branch length estimation compared to sequence data alone, suggesting augmenting lineage tracing technologies with spatial information is useful to overcome the limitations of genome-editing in developmental systems."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"09:10-09:20","Format":"In Person","Speaker":"Hoi Man Chung","Title":"Interpretable variational encoding of genotypes identifies comprehensive clonality and lineages  in single cells geometrically","Abstract":"Despite the wide accessibility of genetic information in multiple omics assays, analyzing single-cell _x000D_\ngenomics remains a challenge due to its diverse high-dimensional macrostructures and many _x000D_\nmissing signals. For the sake of numerical convergence in diverse macrostructures, existing statistical _x000D_\nmethods often pose strong constraints on the form of predicted mutation patterns, and therefore _x000D_\neasily identify underfitted or overfitted local or global optima that are biologically incomprehensive in _x000D_\ncomplex contexts. To solve this problem, we developed SNPmanifold, a Python package that detects_x000D_\nflexible mutation patterns with a shallow binomial variational autoencoder and UMAP (schematic _x000D_\nshown in Figure 1). After reducing allele count matrix to lower-dimensional latent space, SNPmanifold _x000D_\nthen performs 3 downstream analyses on the genomic geometrical manifold: 1. Clustering of cells _x000D_\nwith similar genotypes, 2. Ranking of important SNPs, and 3. Phylogenetic tree construction. Based _x000D_\non nuclear or mitochondrial variants, we demonstrated that SNPmanifold can effectively identify a _x000D_\nlarge number of multiplexed donors of origin (k = 18) that all existing methods fail and lineages of _x000D_\nsomatic clones with promising biological interpretation (detailed results of an example dataset shown _x000D_\nin Figure 2). Compared to existing methods, SNPmanifold can better identify the optimal degree of _x000D_\nfitting with enhanced generalizability and human-interpretability. SNPmanifold therefore can reveal _x000D_\ninsights into single-cell clonality and lineages more comprehensively and straight-forwardly."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":" Priya","Title":"PHALCON: Phylogeny-aware variant calling from large-scale single-cell panel sequencing datasets","Abstract":"Single-cell sequencing (SCS) technologies bring cellular resolution in resolving intra-tumor heterogeneity, which can cause drug resistance and relapse in cancer. Nonetheless, SCS methods pose several technical challenges, such as uneven coverage, allelic dropout (ADO), or artifacts subjected to erroneous amplification. Single-cell variant callers have been developed to distinguish the true variants from technical artifacts. However, recently emerging parallel sequencing methods can now sequence up to thousands of cells by targeting only disease-specific genes. Current variant callers are not scalable for such high-throughput datasets and do not effectively address the amplification biases in panel-based sequencing protocols._x000D_\n_x000D_\nTo address these, we present a statistical variant caller, PHALCON, which enables scalable mutation detection from large-scale single-cell panel sequencing data by modeling their evolutionary history under a finite-sites model along a clonal phylogeny.  PHALCON infers the underlying cellular sub-populations based on genotype likelihoods of candidate sites and reconstructs a clonal phylogeny and the most likely mutation history (loss and recurrence included) using a probabilistic framework that maximizes the likelihood of the observed read counts given the genotypes._x000D_\n_x000D_\nUsing numerous simulated datasets across varied experimental settings, we showed that PHALCON outperforms existing state-of-the-art methods in terms of variant calling accuracy (7.29-51.67% improvement), accuracy in inferring the tumor phylogeny (410.43-32931.8% improvement) and runtime (60-70 times faster). Furthermore, we applied PHALCON on real tumor single-cell panel sequencing datasets from triple negative breast cancer patients where PHALCON detected novel somatic mutations in important oncogenes and tumor suppressor genes with high functional impact and orthogonal support in bulk datasets."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"In Person","Speaker":"Juliette Luiselli","Title":"Genome streamlining: effect of mutation rate and population size on genome size reduction","Abstract":"Genome size reduction, also known as genome streamlining, is observed in bacteria with very different life traits, including endosymbiotic bacteria and several marine bacteria, raising the question of its evolutionary origin. None of the hypotheses proposed in the literature is firmly established, mainly due to the many confounding factors related to the diverse habitats of species with streamlined genomes. Computational models may help overcome these difficulties and rigorously test hypotheses. We use Aevol, a platform designed to study the evolution of genome architecture, to test two main hypotheses: that an increase in population size (N) or mutation rate (Î¼) could cause genome reduction. Pre-evolved individuals were transferred into new conditions, characterized by an increase in population size or mutation rate. In our experiments, both conditions lead to genome reduction. However, they lead to very different genome structures. Under increased population size, genomes loose a significant fraction of non-coding sequences, but maintain their coding size, resulting in densely packed genomes (akin to streamlined marine bacteria genomes). By contrast, under increased mutation rate, genomes loose coding and non-coding sequences (akin to endosymbiotic bacteria genomes). Hence, both factors lead to an overall reduction in genome size, but the coding density of the genome appears to be determined by N Ã— Î¼. Thus, a broad range of genome size and density can be achieved by different combinations of N and Î¼. Further analyses suggest that genome size and coding density are determined by the interplay between selection for phenotypic adaptation and selection for robustness."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"In Person","Speaker":"Felix Langschied","Title":"Evolutionary dynamics of microRNAs pinpoint innovations in the gene regulatory network of vertebrates","Abstract":"The evolution of the regulatory network formed by miRNAs and their target mRNAs remains poorly understood because scalable and accurate frameworks for miRNA ortholog detection are missing. We closed this methodological gap, and our tool ncOrtho identifies miRNA orthologs in large collections of unannotated genome assemblies matching manually curated annotations in sensitivity and precision. With ncOrtho, we have investigated the plasticity of the human miRNA repertoire across 402 phylogenetically diverse vertebrates. This revealed four main bursts of miRNA acquisition of which the oldest predates the diversification of the vertebrates, and the youngest is specific to the Simiiformes. Overall, miRNA loss is rare which directs the attention to 16 miRNA families that are absent in the Eumuroidea (Rodentia). To investigate the impact of these losses on the corresponding gene regulatory networks, we overexpressed Mir-197 and Mir-769 in induced pluripotent stem cells (iPSCs) of human and mouse. Overlapping sets of silenced mRNAs in the two species reveal that miRNA-dependent regulatory networks remain partly intact despite the miRNA losses. The prevalence of target sites specific to either lineage indicates a considerable evolutionary flexibility of the target gene repertoire. Interestingly, human protein-coding genes with a similar history of gene loss as the 16 miRNAs are enriched for transcription factors. This indicates that mouse but also rat have substantially modified their regulatory network of gene expression on transcriptional and post-transcriptional level compared to other vertebrate model organisms."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Tal Pupko","Title":"A machine-learning based alternative to phylogenetic bootstrap","Abstract":"Currently used methods for estimating branch support in phylogenetic analyses often rely on the classic Felsenstein's bootstrap, parametric tests, or their approximations. As these branch support scores are widely used in phylogenetic analyses, having accurate, fast, and interpretable scores is of high importance._x000D_\nHere, we employed a data-driven approach to estimate branch support values with a probabilistic interpretation. To this end, we simulated thousands of realistic phylogenetic trees and the corre-sponding multiple sequence alignments. Each of the obtained alignments was used to infer the phylogeny using state-of-the-art phylogenetic inference software, which was then compared to the true tree. Using these extensive data, we trained machine-learning algorithms to estimate branch support values for each bipartition within the maximum-likelihood trees obtained by each software. Our results demonstrate that our model provides fast and more accurate probability-based branch support values than commonly used procedures."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"11:10-11:20","Format":"In Person","Speaker":"Soham Dibyachintan","Title":"Neutral variation in a protein interaction network limits predictability of protein evolution","Abstract":"The evolutionary fate of a mutation is dependent on its phenotypic effects. In recent years, multiple evolutionary models have been developed that use variation in natural sequences to predict the impact of a mutation in any given protein. However, many proteins display multiple phenotypes, most mediated by specific protein domains.  Furthermore, many proteins originate from gene duplication and share most of their evolutionary history. How such factors affect the predictability of evolution is unknown owing to the lack of comprehensive experimental data on such proteins. We combined genome editing and high-throughput phenotypic assays to quantify the impact of all single-amino acid substitutions on the binding of two functionally redundant paralogous Src Homology 3 domains to their cognate interaction partners in yeast. These interaction partners have peptides which satisfy a consensus polyproline motif recognized by the SH3 domains. We observed that the effect of many mutations was not conserved across phenotypes or between the paralogs. A comparison of our experiments with evolutionary models revealed that these models only capture few differences in the effect of mutations between paralogs. Ancestral sequence reconstruction revealed that for mutations whose effects differed between domains, there was no difference between ancestral substitutions and mutations sampled at random. Broadly, our results illustrate that neutral sequence variation over time in the components of a protein interaction network limits our ability to predict protein evolution accurately using existing methods. Our work underscores the importance of using experimental data to inform computational models and improve the prediction of protein evolution."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Mathieu Gascon","Title":"Simultaneously Building and Reconciling a Synteny Tree","Abstract":"Our lab recently presented Synesth (for SYNteny Evolution in SegmenTal Histories), an extended reconciliation model for synteny trees accounting for fissions, losses, gains, duplications and transfers potentially going through unsampled species. Synesth takes as input a synteny tree and a species tree, and outputs a most parsimonious evolutionary history. As reconciliation is very sensitive to the input trees (a slight modification may lead to a significant difference in the inferred evolutionary scenarios), obtaining accurate trees is essential. This is particularly challenging in the case of our model requiring a synteny tree as input, while phylogenetic methods on gene sequences rather output sets of gene trees, one for each gene family. If the individual gene trees are ''consistent'', meaning that they do not represent contradictory phylogenetic information, then a supertree (a tree displaying them all) can be obtained. Such a supertree can be used as an input for Synesth to represent the evolution of the syntenies containing the individual genes. As finding the optimal super-tree as been shown to be an NP-hard problem, the solution we proposed in a previous work was to test each possible supertree and retain the one leading to the most parsimonious reconciliation. In this presentation, we explore a new way to solve this problem by simultaneously building and reconciling the optimal supertree, leading to an algorithm that is exponential in the number of gene trees rather than in the total number of genes. We compare this new algorithm to the previous one using simulated datasets."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Inanc Birol","Title":"ntSynt: multi-genome synteny detection using minimizer graph mappings","Abstract":"In recent years, the landscape of reference-grade genome assemblies has seen substantial diversification. With such rich data, there is pressing demand for robust tools for scalable, multi-species comparative genomics analyses, including detecting genome synteny, which informs on the sequence conservation between genomes and contributes crucial insights into species evolution. Here, we introduce ntSynt, a scalable utility for computing large-scale multi-genome synteny blocks using a minimizer graph-based approach. After computing the initial multi-genome synteny blocks using this constructed minimizer graph, the synteny blocks are refined in multiple rounds through indel detection, merging collinear blocks and extending block coordinates using decreasing minimizer window sizes. Through extensive testing utilizing multiple ~3 Gbp genomes, we demonstrate how ntSynt produces synteny blocks with coverages between 79â€“100% in at most 2h using 34 GB of memory, even for genomes with appreciable (>15%) sequence divergence. In addition, we used ntSynt to compare 11 bee genomes of the genus Andrena from the Earth BioGenome Project, and achieved synteny blocks with high coverage (85% for the smallest genome) in less than 15 minutes, despite these genomes varying in both chromosome number (3â€“7) and genome size (247 Mbp â€“ 443 Mbp). Compared to existing state-of-the-art methodologies, ntSynt offers enhanced flexibility to diverse input genome sequences and synteny block granularity. We expect the macrosyntenic genome analyses facilitated by ntSynt to enable critical evolutionary insights within and between species across the tree of life. ntSynt is freely available at https:\/\/github.com\/bcgsc\/ntsynt."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Maureen Stolzer","Title":"Automated clade-level detection of Incomplete lineage sorting","Abstract":"Phylogenetic population modeling, combined with sequencing of large collections of closely related taxa, has enabled unprecedented exploration of population processes in evolutionary and ecological contexts.  Incomplete Lineage Sorting (ILS) and introgression can result in gene trees that disagree with the species tree. For example, the history of a gene sampled from three species with phylogeny A|BC may agree with the species tree or have one of two incongruent topologies, B|AC or C|AB. The resulting distribution of gene tree topologies provides a wealth of information for testing alternate hypotheses and estimating population parameters.  Despite these advances, quantification of ILS, while excluding incongruence due to introgression and paralogy, remains a challenging problem. _x000D_\nHere, we present an algorithm that extracts gene tree statistics associated with ILS from all species internodes in a single computational procedure, supporting automated, large-scale phylogenomic analyses of entire clades. Characterizing ILS can help to resolve phylogenetic uncertainty and is important for understanding the relative contributions of incomplete lineage sorting, introgression, and convergent evolution to trait evolution and present-day genetic variation.  Our method accounts for uncertainty due to gene loss and missing data and screens out incongruence due to distant introgression and paralogy. As such, it can be applied to both multigene families and single-copy orthologs. The algorithm is polynomial in tree size and is thus applicable to very large species trees. We demonstrate our approach through the reanalysis of several phylogenomic datasets discussed in the literature."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Semih Kurt","Title":"Sparse Neighbor Joining: rapid phylogenetic inference using a sparse distance matrix","Abstract":"Phylogenetic reconstruction is a fundamental problem in computational biology. The Neighbor Joining (NJ) algorithm offers an efficient distance-based solution to this problem, which often serves as the foundation for more advanced statistical methods. Despite prior efforts to enhance the speed of NJ, the computation of the n^2 entries of the distance matrix, where n is the number of phylogenetic tree leaves, continues to pose a limitation in scaling NJ to larger datasets. In this work, we propose a new algorithm which does not require computing a dense distance matrix. Instead, it dynamically determines a sparse set of at most O(n log n) distance matrix entries to be computed in its basic version, and up to O(n log^2 n) entries in an enhanced version. We show by experiments that this approach reduces the execution time of NJ for large datasets, with a trade-off in accuracy."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Lars Arvestad","Title":"Scalable distance-based phylogeny inference using divide-and-conquer","Abstract":"Distance-based methods for inferring evolutionary trees are important subroutines in computational biology, sometimes as a first step in a statistically more robust phylogenetic method. The most popular method is Neighbor-Joining, mainly due to its relatively good accuracy. Unfortunately, Neighbor-Joining has cubic time complexity, which limits its applicability on larger datasets. Similar but faster algorithms have been suggested, but the overall time complexity of a Neighbor-Joining computation remains essentially cubic as long as the input is a distance matrix that must be computed. In practice, memory usage is today a limiting factor because distance matrix sizes grow quadratically. These constraints become a bottleneck in studies that rely on distance-based phylogeny estimation. With ever increasing data sizes, a scalable distance-based phylogeny inference method would change how scientists think about evolutionary-based studies. _x000D_\n_x000D_\nWe present two randomized divide-and-conquer heuristics, dnctree and dnctree-k, that selectively estimate pairwise sequence distances and infers a tree by connecting increasingly large subtrees. The divide-and-conquer approach avoids computing all pairwise distances and thereby saves both time and memory. The time complexity is at worst quadratic, and seems to scale like O(n lg n) in practice. Both algorithms have been implemented and tested, and dnctree-k shows similar accuracy as Neighbor-Joining in terms of inference accuracy in our experiments. We show that both algorithms scale very well, which is verified in computational experiments. In fact, they are applicable to very large datasets even when implemented in Python._x000D_\n_x000D_\nA Python implementation, dnctree, is available on GitHub (https:\/\/github.com\/arvestad\/dnctree) and PyPI.org."},{"Track":"EvolCompGen","Room":"518","Weekday":"Tuesday","Date":"16 July","Timespan":"15:20-16:00","Format":"In Person","Speaker":null,"Title":"Panel session ","Abstract":null},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":null,"Title":"Roll Call and Introduction to the Function COSI","Abstract":"Free cash will be given to the 7th and 131st persons to show up. Maybe."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"11:00-11:40","Format":"In Person","Speaker":"Valerie de Crécy-Lagard","Title":"Linking Gene and function in the post-genomic era: issues and opportunities","Abstract":"Identifying the function of every gene in all sequenced organisms is the major challenge of the post-genomic era and an obligate step for any systems biology approach. This objective is far from reached. By various estimates, at least 30-70% of the genes of any given organism are of unknown function, incorrectly annotated, or have only a generic annotation. Using comparative genomic approaches, we have linked genes and functions for over 65 gene families related mainly to coenzyme metabolism, nucleic acid modification, protein modification, and metabolite repair. Building on this body of work I will discuss the lessons learned, the next steps needed to improve protein function prediction and discovery, and the role of machine learning in this process."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Alex Warwick Vesztrocy","Title":"Unveiling the Functional Fate of Duplicated Genes Through Expression Profiling","Abstract":"Gene duplication is a major evolutionary source of functional innovation â€“ if one copy maintains the ancestral function then the other is no longer under selective pressure and is free to diverge. It is presumed that the more slowly evolving copy, in terms of sequence, maintains the ancestral function â€“ â€œthe least diverged orthologue (LDO) conjectureâ€. This is a specific case of the much-debated orthologue conjecture â€“ that orthologous genes are functionally more similar than paralogous genes. However, annotation bias has led to issues when using gene ontology annotations in previous studies. In an attempt to remove this bias, this study uses gene expression profiles as a proxy for function._x000D_\n_x000D_\nThis study analysed 15,693 gene families from PANTHER, paired with an extensive and large-scale expression atlas for 16 animal and 20 plant species. Using an evolutionary model, accounting for varying evolutionary rates between gene families, branches were categorised according to whether they display symmetric or asymmetric evolution following duplication._x000D_\n_x000D_\nPairs of genes resulting from asymmetric duplications displayed a significantly lower expression profile similarity compared to those from symmetric duplications (Mann-Whitney, p<0.001). Additionally, the more diverged copy exhibited increased tissue specificity, suggesting specialisation._x000D_\n_x000D_\nThe results of this study support the hypothesis that the gene copy with the least evolutionary change following duplication tends to retain the ancestral function. Additionally, the more diverged copy may acquire a novel, potentially specialised, function. This likely has implications when predicting functional annotations â€“ particularly when only using pairwise distances between sequences."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Harsh R. Srivastava","Title":"Leveraging deep learning for characterization of malaria parasite PUFs â€” proteins of unknown function","Abstract":"Exploiting the sequence-structure-function paradigm is crucial for annotating proteins of unknown function (PUFs) in Plasmodium falciparum, a member of the diverged eukaryotic SAR (Stramenopiles, Alveolates, and Rhizarians) supergroup. P. falciparum, a malaria-causing parasite, accounted for ~250 million cases and over 600,000 deaths in 2022. Discovery of diagnostic and therapeutic targets in P. falciparum is hindered, as ~23% of proteins are classified as PUFs while ~40% of proteins are partially annotated. Predicting GO annotations for these PUFs is difficult given low sequence similarity to annotated proteins and limited generalization of deep learning models trained on well-studied SwissProt species to diverged organisms. Here, we focused on the structure-function relationship of SAR sequences and developed a new method to predict GO terms in P. falciparum. PFP (Plasmodium Function Predictor) is a collection of structural-homology based deep learning models trained using evolutionarily relevant structure-aware TM-Vec embeddings. We used a deep feedforward architecture with a dropout layer to predict GO annotations and quantify uncertainty using Monte Carlo dropout. When benchmarked against DeepGOPlus, PFP, demonstrated a significant improvement in Fmax, Smin, and AUPR-micro\/AUPR-macro for our test split as well as our Plasmodium holdout split. PFP predicted GO terms respected the hierarchical structure of GO and aligned with expected information content distributions. For poorly annotated proteins, PFP imputed GO terms which are biologically plausible given existing annotations. Additionally, predictions made by PFP were categorized into confidence levels and aligned with published data targeting specific P. falciparum PUFs. PFP is the first curated function prediction model developed specifically for a subset of eukaryotic species. We will discuss findings in model architecture and highlight specific GO predictions contributing to an increase of more than 25% in P. falciparum proteome annotation."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-15:00","Format":"In Person","Speaker":"M. Clara De Paolis Kaluza","Title":"Crowdsoursing the Fifth Critical Assessment of Protein Function Annotation Algorithms (CAFA 5)","Abstract":"The Critical Assessment of Functional Annotation (CAFA) is a long-standing, ongoing community effort to independently assess computational methods for protein function prediction, to highlight well-performing methodologies, to identify bottlenecks in the field, and to provide a forum for dissemination of results and exchange of ideas. Every three years since its inception in 2010, CAFA has solicited participation from computational groups and engaged biocurators and experimental biologists to collect high-quality data on which to evaluate algorithmic performance in a series of prospective computational challenges to predict function for a large set of target proteins. For the 5th installment (CAFA 5) launched in 2023, a partnership with Kaggle Inc. facilitated the participation of a much broader community of data scientists. Predictions were collected as entries to a competitive challenge on the crowdsourced science platform. The reach and technology of this approach resulted in a 22x increase in the number of participating teams, comprised of entrants from 77 counties and various scientific and technical backgrounds. At the conclusion of the challenge, predictions were evaluated using a summary metric on a limited set of proteins which had accumulated annotations during a four month period. In this talk, we will present the outcomes of the increased and diversified participation on the quality of predictions on Gene Ontology (GO) term annotations on an expanded set of annotations and in greater detail across ontology aspects and in relation to past CAFA evaluation."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Chengxin Zhang","Title":"StarFunc: interplaying template-based and deep learning approach for accurate protein function prediction","Abstract":"Despite significant advancements in the development of novel methods for protein function prediction via deep learning, template information often remains a critical component. Common predictions typically utilize templates identified through sequence homology or protein-protein interactions. Yet, relatively few methods leverage structure similarity for template detection, even though protein structures underpin function. In response to this, we developed StarFunc, a comprehensive approach that seamlessly marries cutting-edge deep learning models with template information. This information is garnered from a range of sources, including sequence homology, protein-protein interaction partners, similar structures, and protein domain families. StarFunc was put to the test in large-scale benchmarking and blind tests during the 5th Critical Assessment of Function Annotation (CAFA5). The results consistently highlight its advantage over not only conventional template-based predictors but also other state-of-the-art deep learning methods."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"15:20-15:40","Format":"Live Stream","Speaker":"Alexander Chervov","Title":"ProtBoost: Prediction of functional properties of the proteins by Py-Boost and protein language models (CAFA5 top2)","Abstract":"We will describe machine learning approach to predict protein functions based on their sequences - which allowed our team to win  CAFA5 top 2 position with significant gap to top 3 team. The main novelty of our approach: Py-boost - new gradient boosting algorithm designed to predict multiple targets simultaneously (designed by  one of the team members (NeurIPS 2022) ). Gradient boostings (XGBoost, LightGBM, CatBoost) are the most powerful methods update to work with tabular data, however they are too slow for the tasks with hundreds or thousands targets. Py-Boost overcome that problem by special approximation of the loss function, thus combining effectiveness of gradient boostings with ability to predict thousands targets at once.  Other key ingredients - extensive use of protein language models (T5, ESM2),  GCN (graph neural network) to aggregate predictions across the Gene Ontology, ensemble of neural networks. Language models are becoming quite quite popular tools for various tasks in bioinformatics: for proteins, DNA\/RNA, as well as small-molecule SMILES data. We will report our analysis for the different language models obtained during CAFA5 challenge as well as subsequent analysis."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"15:40-16:00","Format":"Live Stream","Speaker":"Huiying Yan","Title":"GORetriever: Reranking protein-description-based GO candidates by literature-driven deep information retrieval for precise protein function annotation","Abstract":"The vast majority of proteins still lack experimentally validated functional annotations, which highlights the importance of developing high-performance automated protein function prediction\/annotation (AFP) methods. While existing approaches focus on protein sequences, networks, and structural data, textual information related to proteins has been overlooked. However, roughly 82% of SwissProt proteins already possess literature information that experts have annotated. To efficiently and effectively use literature information, we present GORetriever, a two-stage deep information retrieval-based method for AFP. Given a target protein, in the first stage, candidate Gene Ontology (GO) terms are retrieved by using annotated proteins with similar descriptions. In the second stage, the GO terms are reranked based on semantic matching between the GO definitions and textual information (literature and protein description) of the target protein. Extensive experiments over benchmark datasets demonstrate the remarkable effectiveness of GORe- triever in enhancing the AFP performance. Note that GORetriever is the key component of GOCurator, which has achieved the first place in the latest critical assessment of protein function annotation (CAFA5: over 1,600 teams participated), held in 2023â€“24."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Quancheng Liu","Title":"InterLabelGO+: Unraveling label correlations in protein  function prediction","Abstract":"Accurate prediction of protein functions is crucial for understanding biological processes and advancing biomedical research. However, the rapid growth of known protein sequences far outpaces experimental characterization of protein function, necessitating the development of automated computational methods. We present InterLabelGO, a cutting-edge deep learning approach that leverages protein language models and addresses the challenges of label imbalance and label dependencies in protein function prediction. By incorporating a novel loss function that captures complex functional relationships and integrates alignment-based methods through dynamic weighting, InterLabelGO achieves remarkable performance in predicting Gene Ontology (GO) terms. In the recent CAFA5 challenge, a preliminary version of  InterLabelGO ranked 6th out of 1,625 teams worldwide, showcasing its effectiveness compared to state-of-the-art methods. Comprehensive evaluations on large-scale datasets demonstrate InterLabelGO's ability to accurately predict GO terms across various functional categories and evaluation metrics. With its innovative approach to harnessing deep learning and label correlations, InterLabelGO represents a significant advancement in automated protein function prediction, offering the potential to greatly accelerate and enrich the functional annotation of the ever-expanding universe of protein sequences."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Joel Roca Martinez","Title":"Discovery of PETases using a computational classification system","Abstract":"Plastic accumulation is a pressing environmental issue that has escalated in recent decades. With around 25 million tons produced yearly, polyethylene terephthalate (PET) is the most common single use plastic polymer. Current PET recycling protocols rely on mechanical processes that lead to a loss of properties and poor recycling rates. Enzymatic PET degradation emerged as an alternative solution a decade ago, with characterized natural enzymes and variants showing PET degrading activity (PETases). Considering PET has only been present in the environment for less than 50 years, PETases have not evolved naturally to degrade it, hindering the identification of very active enzymes with the right properties. In this study, starting from a dataset with over 1 billion sequences from the MGnify database, we filtered it to define a set of potential PETases that we refer to as the PETase zone, containing over 20.000 sequences. After splitting it in functional families, we focused on 2 families closely related to the family containing most of the known active PETases, while showing a high sequence diversity. To select a final set of promising candidates to text experimentally, we focused on a set of positions that we identified as differentially conserved among the functional families, thus likely important for the proteinâ€™s function. We used those positions alongside other information such as docking score, protein solubility or pocket properties to shortlist 11 potential PETases, from which 3 were experimentally active, proving the protocolâ€™s efficacy and providing new starting points in the PETase sequence space."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"David Medina-Ortiz","Title":"Plastic-Ml-Tool,  a machine learning tool for discovering and optimising plastic degrading enzymes","Abstract":"Plastic contamination is a significant environmental threat that negatively impacts habitats, species, and ecosystems. Recycling strategies can involve chemical and thermo-mechanical processes to convert waste into energy. Nevertheless, these technologies have a slow processing time and a limited degrading rate. Biodegradation provides an environmentally friendly alternative through microorganisms with enzymes capable of degrading plastics. Machine learning approaches have been successfully applied to guide enzyme engineering for improving optimal temperature and to build classification systems for detecting plastic degrading enzymes. However, challenges persist in improving the generalisation of the predicted methods and discovering new plastic-degrading enzymes. This work presents Plastic-Ml-Tool, a machine-learning library that discovers and assists in designing new plastic-degrading enzymes. The proposed tool has a classification system to detect plastic-degrading enzymes and recognise different plastic targets. Besides, predictive models for optimal catalytic temperature were built using deep learning. Plastic-Ml-Tool has different generative approaches to discover new plastic degrading enzymes and an MLDE approach to guide the design of enzymes with desirable optimal catalytic temperatures. Different explorations were made to demonstrate the usability of the proposed methods, including the mapping of KEGG databases to recognise new plastic degrading enzymes, the generation of new PET enzymes through the generative approaches implemented in Plastic-ML-Tool, and the protein engineering of target enzymes to improve PET-plastic degradation. The high usability and the ease of in-silico navigation of new enzymes demonstrate the advantages of the proposed work, which is becoming a valuable and high-impact tool to support experimental methods."},{"Track":"Function","Room":"520b","Weekday":"Saturday","Date":"13 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Yang Lu","Title":"A BLAST from the past: revisiting BLAST's E-value","Abstract":"The Basic Local Alignment Search Tool, BLAST, is an indispensable tool for genomic research._x000D_\nBLAST established itself as the canonical tool for sequence similarity search in large part thanks to its meaningful_x000D_\nstatistical analysis. Specifically, BLAST reports the E-value of each reported alignment, which is defined_x000D_\nas the expected number of optimal local alignments that will score at least as high as the observed alignment score,_x000D_\nassuming that the query and the database sequences are randomly generated._x000D_\nHere we critically reevaluate BLAST's E-values, showing that they can be at times significantly conservative_x000D_\nwhile at others too liberal. We offer an alternative approach based on generating a small sample_x000D_\nfrom the null distribution of random optimal alignments, and testing whether the observed alignment score_x000D_\nis consistent with it._x000D_\nIn contrast with BLAST, our significance analysis seems valid, in the sense that it did not deliver inflated significance estimates_x000D_\nin any of our extensive experiments. Moreover, although our method is slightly conservative, it is often significantly less so than_x000D_\nthe BLAST E-value. Indeed, in cases where BLAST's analysis is valid (i.e., not too liberal), our approach seems to deliver a_x000D_\ngreater number of correct alignments._x000D_\nOne advantage of our approach is that it works with any reasonable choice of substitution matrix and gap penalties,_x000D_\navoiding BLAST's limited options of matrices and penalties._x000D_\nIn addition, we can formulate the problem using a canonical family-wise error rate control setup, thereby dispensing_x000D_\nwith E-values, which can at times be difficult to interpret."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Rafael Najmanovich","Title":"Fast, high-performance biophysics-based computational methods in function prediction","Abstract":"In this presentation I will discuss a number of computational methods that rely o a simple approximation to molecular interactions to being proportional to the surface area in contact between the constituting atoms modulated by a pairwise atom-type pseudo-energetic term. I will discuss three methods in this presentation: Surfaces, for the quantification of molecular interactions, NRGTEN, for normal-mode analysis and NRGDock for ultra-massive virtual screening. Their simplified nature makes them fast while still being accurate, allowing their utilization in a high-throughput manner for molecular function prediction."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"R. Gonzalo Parra","Title":"Energetic Local Frustration Through Time and Species","Abstract":"According to the Principle of Minimal Frustration, folded proteins minimize the amount of strong energetic conflicts in their native states. However, not all interactions are energetically optimized for folding but some remain in energetic conflict, i.e. they are highly frustrated. This remaining local energetic frustration has been shown to be statistically correlated with distinct functional aspects such as protein-protein interaction sites, allosterism and catalysis. Fuelled by the recent breakthroughs in efficient protein structure prediction that have made available good quality models for most proteins, we have developed a strategy to calculate local energetic frustration within large protein families and quantify its conservation over evolutionary time. Based on this evolutionary information we can identify how stability and functional constraints have appeared at the common ancestor of the family and have been maintained over the course of evolution. _x000D_\n_x000D_\nI will summarize the results of two of our recent publications, plus some unpublished results, where we show how local frustration in proteins and conservation of it in extant protein families and protein ensembles can be used to shed light into the biophysical understanding of the relationships between sequences, structures, dynamics and functions. Moreover, our local frustration based strategies can be exploited to better understand the protein features that are captured and output by novel Machine Learning methods and help to guide protein design strategies."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Mahta Mehdiabadi","Title":"Function Prediction of Intrinsically Disordered Proteins and Regions: A Graph Auto-Encoder Approach","Abstract":"Intrinsically disordered proteins\/regions (IDPs\/IDRs) lack a well-defined three-dimensional structure yet carry out essential biological functions. Due to their highly dynamic nature and poor sequence conservation, conventional homology and structure-based methods cannot determine their functions. Here, we develop a Graph Auto-Encoder (GAE) model that exploits the information available to all forms of proteins to predict Gene Ontology (GO) functions for the entire protein and individual IDRs. _x000D_\nOur model is capable of encoding the proteins' structural units, such as domains and disordered regions, which are then used to assign functions not only to the entire protein but also to its specific regions. This allows us to map the proteins into a latent space, find similar protein embeddings, and transfer the functions among them. Due to training the model in an unsupervised end-to-end manner independent of GO labels, the model is not affected by the incomplete annotation of data and can be trained on large datasets, which boosts the encoding power of the GAE._x000D_\nThe model's predictive performance was assessed using an independent test set from the DisProt database, which shows significant improvements compared to the standard approaches. The model achieves high throughput, processing hundreds of protein sequences per second. Its highly scalable nature enables integration into production environments like the MobiDB database and functionally categorize and cluster more than 18 million disordered regions available in the database."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Yaron Orenstein","Title":"Mapping the affinity of protein-protein interactions with multiple amino acid mutations using deep neural networks","Abstract":"Protein-protein interactions (PPIs) play vital roles in diverse biological processes. Hence, measuring PPIs is critical for decoding the evolution of proteins, and for developing powerful interactions for drugs. To date, studies focused mainly on a narrow range of affinities and on single mutations in the amino acid sequence of a given protein to develop high-affinity PPIs due to limitations in the experimental techniques. Our study introduces a novel approach to comprehensively map PPIs and identify multiple (affinity-enhancing or affinity-reducing) mutations by applying machine-learning methods to next-generation sequencing selection data. We present a novel method to accurately predict the impact of multiple interacting mutations that were not observed in the experimental data. We applied our method to the N-TIMP2\\MMP9 protein complex as a case study due to its unique interface, which consists of seven positions in N-TIMP2 crucial for binding. We developed a neural network to accurately and quantitatively predict the impact of multiple potentially interacting mutations on binding affinity. Our neural network achieved in cross-validation (training on 90% and testing on a held-out 10%) a Pearson correlation of 0.963 between predicted and observed enrichment ratios. In addition, on an independent dataset of 26 experimentally validated variants, the Pearson correlation between their affinity constants and predicted enrichment ratios was 0.545. Currently, we are testing the affinity of five novel multiple-mutations variants that we predicted as high-affinity variants. Generally, our innovative approach can be applied to many more protein-function datasets to provide a rich characterization of a PPI affinity landscape."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Vishal Joshi","Title":"Utilising Large Language Models for GO Term Extraction in UniProt Annotation","Abstract":"Automatic Annotation(AA) objective_x000D_\nManually reviewed records (UniProtKB\/SwissProt) constitute only about 0.23% of UniProtKB; expert curation is time-intensive and most published experimental data focuses on a rather limited range of model organisms. Simultaneously, the number of unreviewed records is growing continuously, yet for a large proportion of these records there is no experimental data available. UniProtKB uses three prediction systems UniRule, Association-Rule-Based Annotator (ARBA) & Googleâ€™s ProtNLM to functionally annotate around 88% of unreviewed (UniProtKB\/TrEMBL) records automatically which we define as Automatic Annotation._x000D_\n_x000D_\nLarge Language models for GO term extraction from literature & literature summarisation_x000D_\n_x000D_\nWe have prototyped a new pipeline that employs the GPT-4 model (version gpt-4-1106-preview) to extract GO(Gene Ontology) terms from scientific literature accessible through PubMed. The extracted GO terms are validated against the GO Annotation database (GOA) at EMBL-EBI I, in order to exclude those that conflict with taxonomic constraints, are obsolete, or have been blacklisted. We have evaluated our prompts to extract GO terms against GPT-4 (version gpt-4-1106-preview) & other open-source quantized models, like Mixtral-8x7B-Instruct-v0.1 and Mistral-7B-Instruct-v0.2, results of which we will be sharing. Once evaluation is complete against a structured annotation like GO, we plan to potentially expand it to other annotation like keywords, EC numbers etc_x000D_\n_x000D_\nThese models will not only assist in manual curation but also act as collaborative tools in creating UniProt entry-style summaries for proteins\/genes from relevant literature. The performance of these models will be assessed for their potential contributions to augmenting existing manually curated entries."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Weining Lin","Title":"ProstGOPred: Advancing Protein Function Prediction through Graph Contrastive Learning and Structure-Aware Protein Language Model Embeddings","Abstract":"We introduce ProstGOPred, a state-of-the-art protein function prediction model that integrates both protein sequence and structural information with embeddings from the ProstT5 protein language model. By combining embeddings from ProstT5 and network information from the STRING database, our approach employs a graph contrastive learning strategy to optimise the model's ability to recognise functional similarities among proteins. This contrastive learning strategy optimises model performance by minimising the distance between an anchor and a positive sample while maximising the distance between the anchor and a negative sample, serving as a regularisation term in conjunction with supervised learning._x000D_\n_x000D_\nProstGOPred was benchmarked on the CAFA3 dataset and achieved state-of-art performance with f1max scores of BP: 0.534, MF: 0.561, CC: 0.64 on three sub-ontologies, surpassing the performance of traditional BLAST method (0.26, 0.42, 0.45 respectively), DomainPFP (0.38, 0.56, 0.63 respectively), and DeepGOPlus (0.469, 0.544, 0.623 respectively). By leveraging embeddings from ProstT5, a structure-aware protein language model, and protein network information, our model requires no additional structural information or multiple sequence alignment (MSA) data to efficiently predict protein functions._x000D_\n_x000D_\nOur research demonstrates the immense potential of utilising protein language models and graph neural networks for predicting protein functions. In the next stage, we will include evolutionary data based on CATH-FunFams which are being regenerated using domain assignments based on AlphaFold structures and which will exploit structure embeddings to detect functional similarity."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Mengzhou Hu","Title":"Evaluation of large language models for discovery of gene set function","Abstract":"Gene set analysis is a mainstay of functional genomics, but it relies on curated databases of gene functions that are incomplete. Here we evaluate five Large Language Models (LLMs) for their ability to discover the common biological functions represented by a gene set, substantiated by supporting rationale, citations and a confidence assessment. Benchmarking against canonical gene sets from the Gene Ontology, GPT-4 confidently recovered the curated name or a more general concept (73% of cases), while benchmarking against random gene sets correctly yielded zero confidence. Gemini-Pro and Mixtral-Instruct showed ability in naming but were falsely confident for random sets, whereas Llama2-70b had poor performance overall. In gene sets derived from â€˜omics data, GPT-4 identified novel functions not reported by classical functional enrichment (32% of cases), which independent review indicated were largely verifiable and not hallucinations. The ability to rapidly synthesize common gene functions positions LLMs as valuable â€˜omics assistants."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Dana Varghese","Title":"Transformer based data mining for predicting moonlighting in proteins and comparison with first principle annotation","Abstract":"Moonlighting proteins are a specific group of multifunctional proteins that independently carry out distinct functions at various time points, under different conditions, in conjunction with different partners, or in different locations without distinct domains associated with each function. The discovery of moonlighting behavior in proteins across various functional classes and organisms, from unicellular to multicellular eukaryotes, suggests that this evolutionary adaptation to multitask is widespread, despite not following conservation patterns within or between closely related species. Protein functional annotation has become more challenging due to this intriguing phenomenon. Several attempts have been made to predict moonlighting proteins from pre-existing annotations, sequence or structural features and computed features with varying degrees of accuracy (Khan et al. 2016; 2017). We have recently developed a method to specifically predict moonlighting in DNA-binding proteins and showed that within this class, a first principle approach worked well (Varghese et al. 2022). In this work we present a scaled up version of our prediction method to consider all human moonlighting proteins. Separately we developed independent models to identify moonlighting proteins using Natural Language Processing (NLP) methods from published literature and achieved superior results compared to the existing model by leveraging transformer-based models that were pre-trained on PubMed data.  Together, the NLP and first principle methods provide the best performance to mine existing and predicted candidate moonlighting proteins with high confidence."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Asaf Salamov","Title":"Gene families of unknown function conserved across Fungi","Abstract":"We constructed conserved gene families from over 2000 fungal genomes using MMseqs2 clustering algorithm, which included 339 gene clusters, encompassing ~162K proteins. Our criteria included: a) conservation in at least 50% of all fungal species or  over 90% of specific fungal clades, with representation  in at least 100 species;  b) lack of known function, indicated by absence of Pfam domains  or meaningful functional annotations via EggNOG mapper; c) support from transcriptomics data for at least 20% of genes within each cluster_x000D_\nAround half of these gene families (48%) are unique to the Fungal kingdom. Surprisingly, the second-largest group comprises 104 families (31% of total), shared solely between Fungi and the taxonomically distant Viridiplantae clade, underscoring the substantial number of conserved uncharacterized gene families between plants and fungi._x000D_\nTo infer potential functions, we employed Foldseek, aligning AlphaFold2 predicted structures with the PDB database, resulting in PDB hits (excluding hypothetical\/uncharacterized proteins) for 14 gene clusters with TMscore > 70. Additionally, we utilized the deep-learning algorithms DeepFRI and Proteinfer for function prediction, revealing that only around 3% of 'Molecular Function' GO terms with high information content were shared between these methods. Furthermore, over 90% of different GO terms assigned by both methods to the same proteins showed low semantic similarity (<25%) according to GOGO software._x000D_\nThis resource is intended for the fungal genomics community to characterize individual family members functionally and extend their annotations across the Fungal Tree of Life. The portal for conserved fungal families of unknown function is accessible at:_x000D_\nhttps:\/\/mycocosm.jgi.doe.gov\/conserved-clusters\/run\/run-2024;cyS6e"},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Jun-Tao Guo","Title":"Improved prediction of DNA and RNA binding proteins with deep learning models","Abstract":"Nucleic acid-binding proteins (NABPs), including DNA-binding proteins (DBPs) and RNA-binding proteins (RBPs), play important roles in essential biological processes. To facilitate functional annotation and accurate prediction of different types NABPs, many machine learning-based computational approaches have been developed. However, the datasets used for training and testing as well as the prediction scopes in these studies have limited their applications in real world problems. In this paper, we developed new strategies to overcome these limitations by generating more accurate and robust datasets and developing deep learning methods including both hierarchical and multi-class approaches to predict the types of NABPs for any given proteins. The deep learning models employ two layers of convolutional neural network (CNN) and one layer of long short-term memory (LSTM). Our approaches outperform the existing DBP and RBP predictors and have a balanced prediction between DBPs and RBPs. The multi-class approach greatly improves the prediction accuracy of DBPs and RBPs, especially for the DBPs with about 12% performance improvement. Moreover, we explored the prediction accuracy of single-stranded DNA binding proteins (SSBs) and their effect on the overall prediction accuracy of NABP predictions."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Giulia Babbi","Title":"Analysing multifunctional proteins with MultifacetedProtDB","Abstract":"We recently proposed MultifacetedProtDB (https:\/\/multifacetedprotdb.biocomp.unibo.it), a curated database providing a collection of 1103 multifunctional human proteins, of which 812 are enzymes. The characterization of multifunctional proteins is an expanding research area aiming to elucidate the complexities of biological processes. In our resource, we merge information from UniProt, Humsavar, Monarch, and ClinVar, reporting disease nomenclatures as MONDO, ICD10, OMIM and Orphanet catalogues. Some 30% of multifunctional proteins in our database (321 enzymes and 110 non-enzymes) are associated with 895 MONDO diseases classified into 213 ICD10 categories and in 17 out of the 19 ICD10 main chapters. Out of the 895 diseases, 323 are included in the Orphanet catalogue of rare diseases. Over the 431 multifaceted proteins with MONDO disease annotation, 212 are associated with multiple diseases, and 56% are associated also with multiple Reactome pathways. Performing different functions in different pathways could explicate why a protein is associated with different diseases. Thanks to the â€œADVANCED SEARCHâ€ interface in MultifacetedProtDB, it is possible to search for multifunctional proteins associated with MONDO diseases and endowed with Pfam annotations, obtaining as a result a list of 428 entries. Specifically, the Protein kinase domain (PF00069), the Cytochrome P450 domain (PF00067), the Connexin domain (PF00029) and the Î²\/Î³ crystallin domain (PF00030) are most frequently associated with diseases in multifunctional proteins. Presently, data in our DB indicate that multifunctionality is not exclusively related to multiple subcellular locations, and\/or association with diseases, and that some multifunctional proteins with specific domains seem more involved than others in diseases."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Siyu Yang","Title":"Dynamic network analysis of multi-scale -omics data for protein function prediction","Abstract":"Protein function prediction is a prominent computational task. One common approach is analyzing proteins' 3D structures by modeling them as protein structure networks (PSNs). Another common approach is analyzing a protein-protein interaction (PPI) network. To leverage both data types, our lab recently integrated PSN and PPI network data into a multi-scale \"network-of-networks\" (NoN), where each node (protein) in the PPI network at the higher scale is a network (PSN) itself at the lower scale. Protein functional prediction via NoN-based data integration often outperformed or complemented single-scale PSN-only or PPI network-only prediction. However, until recently, existing PSN approaches represented the final (native) 3D structure of a protein as a static PSN. Recognizing that protein folding is a dynamic process, our lab recently introduced dynamic PSNs, which when used in single-scale fashion, achieved a promising improvement over static PSNs in the task of protein structure classification. In our recent work to be presented at ISMB 2024, we evaluate the performance of dynamic vs. static PSNs in the protein function prediction task. First, we do so only on the PSN scale, without considering the PPI scale (and thus outside of the NoN framework). Here is where using dynamic PSNs shows the most promise: when focusing on protein functions (Gene Ontology biological process terms) where one of the two PSN approaches (dynamic vs. static PSNs) is significantly superior to the other approach, almost exclusively it is dynamic PSNs that are superior to static PSNs. Second, we compare dynamic vs. static PSNs within the NoN framework. Here, the two approaches are mostly quantitatively comparable yet qualitatively complementary, i.e., each of dynamic and static PSNs is superior to the other approach for some of the GO terms. We are currently analyzing potential biological implications of these results."},{"Track":"Function","Room":"520b","Weekday":"Sunday","Date":"14 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Jason Mcdermott","Title":"Enhanced Functional Annotation for Genome-Scale Metabolic Models Using an Omics-Informed Integrated Pipeline","Abstract":"There are inherent challenges in characterizing the metabolic potential of complex microbiomes, particularly those derived from incomplete data like metagenome-assembled genomes (MAGs). Current annotation methods often miss metabolic enzymes due to incomplete sequencing data or significant evolutionary divergence. To address these issues, the authors present a pipeline integrating three tools: MetaPathPredict, a deep learning framework for predicting metabolic modules; OMics-Enabled Global GApfilling (OMEGGA), which performs global gap-filling from experimental growth data and integrates multi-omics data; and Snekmer, a computational framework for building sequence-based models for protein families. The pipeline begins with a traditionally annotated genome, uses MetaPathPredict to predict metabolic modules, and then uses OMEGGA to perform gap filling. Snekmer is then used to identify candidates for missing enzymes. The authors demonstrate the application of this pipeline to a model soil consortium, showing how it improves annotation for genome-scale metabolic model construction and refines models for accurate prediction of growth under novel conditions. The results suggest that this integrated, data-driven approach can improve functional annotation for bacterial metabolic models and propose novel functional annotations for important metabolic enzymes in environmental bacteria."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Xinrui Lyu","Title":"An Empirical Study on KDIGO-Defined Acute Kidney Injury Prediction in the Intensive Care Unit","Abstract":"Motivation: Acute kidney injury (AKI) is a syndrome that affects up to a third of all critically ill patients, and early diagnosis to receive adequate treatment is as imperative as it is challenging to make early. Consequently, machine learning approaches have been developed to predict AKI ahead of time. However, the prevalence of AKI is often underestimated in state-of-the-art approaches, as they rely on an AKI event annotation solely based on creatinine, ignoring urine output. _x000D_\nMethods: We construct and evaluate early warning systems for AKI in a multi-disciplinary ICU setting, using the complete KDIGO definition of AKI. We propose several variants of gradient-boosted decision trees (GDBT)-based models, including a novel time-stacking based approach. A state-of-the-art LSTM-based model previously proposed for AKI prediction is used as a comparison, which was not specifically evaluated in ICU settings yet._x000D_\nResults: We find that optimal performance is achieved by using GBDT with the time-based stacking technique (AUPRC=65.7%, compared with the LSTM-based model's AUPRC=62.6%), which is motivated by the high relevance of time since ICU admission for this task. Both models show mildly reduced performance in the limited training data setting, perform fairly across different subcohorts, and exhibit no issues in gender transfer._x000D_\nConclusion: Following the official KDIGO definition substantially increases the number of annotated AKI events. In our study GBDTs  outperform LSTM models for AKI prediction. Generally, we find that both model types are robust in a variety of challenging settings arising in the ICU."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"Patrick Martin","Title":"Mapping spatial omics when tissue architecture doesnâ€™t match.","Abstract":"The reality of using spatial transcriptomics in a clinical setting is that samples taken from different patients, under different conditions, and at different times will almost always present a drastically different tissue architecture. Yet, to better understand the effects of a diseases or a treatment, comparing similar cell groups between samples is crucial. The challenge of mapping cells between spatial omics samples is to retain the spatial context. _x000D_\nTo address this challenge, we developed a context aware mapping approach that finds optimally matching cell pairs between samples. Our algorithm solves a linear assignment problem by minimizing a cost matrix constructed from a variety of cellular contexts including cell similarity, niche similarity, and tissue structure similarity. We provide a highly flexible approach that would allow the use of context specific cost matrices to fine tune cell comparisons across samples. We benchmarked the performance of our method in simulated data to unequivocally demonstrate how spatial context aids in accurately mapping cells across samples â€“ outperforming spatial alignment methods in the process.  We demonstrate its broad applicability by mapping cells across different biological samples, different technologies, and different developmental time points. Finally, we demonstrate how the use of mapping scores can be used to stratify patients by clustering patient samples with a similar spatial context."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Zoe Clarke","Title":"MALAT1 expression consistently indicates cell quality in single-cell RNA and single-nucleus RNA sequencing","Abstract":"Single-cell RNA sequencing (scRNA-seq) and single-nucleus RNA sequencing (snRNA-seq) have revolutionized our understanding of cell types, as it gives us an unbiased lens through which to view the different cell types that make up a tissue. However, empty droplets and poor quality cells are often captured in the experiment and need to be filtered out to avoid the false interpretation of certain cell types. Many automated and manual methods exist to filter out these cells or droplets, such as minimum RNA count thresholds and comparing the gene expression profile of an individual cell to the overall background expression of the experiment. Recently, a method called DropletQC was developed to calculate the overall RNA splice ratios of cells, and used this to determine if a cell is actually an empty droplet. We have found that MALAT1 expression is correlated with cell quality inferred by DropletQC, and consistently identifies low quality cells across scRNA-seq and snRNA-seq data. MALAT1 is a non-coding RNA retained in the nucleus and ubiquitously expressed across cell types, and low expression values indicate a poor quality cell. Since it is easy to visualize the expression of MALAT1 in single-cell maps, its expression can be used to determine cell quality and improve the interpretation of cells in any tissue. Further, old maps can be reviewed to retroactively determine the quality of different cell populations."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Kuan-Hao Chao","Title":"Combining DNA and protein alignments to improve genome annotation with LiftOn","Abstract":"As the number and variety of assembled genomes continues to grow, the number of annotated genomes is falling behind, particularly for eukaryotes. DNA-based mapping tools help to address this challenge, but they are only able to transfer annotation between closely-related species. Here we introduce LiftOn, a homology-based software tool that integrates DNA and protein alignments to enhance the accuracy of genome-scale annotation and to allow mapping between relatively distant species. LiftOn's protein-centric algorithm considers both types of alignments, chooses optimal open-reading frames, resolves overlapping gene loci, and finds additional gene copies where they exist. LiftOn can reliably transfer annotation between genomes representing members of the same species, as we demonstrate on human, mouse, honey bee, rice, and Arabidopsis thaliana. It can further map annotation effectively across species pairs as far apart as mouse and rat or Drosophila melanogaster and D. erecta."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Sanjana Tule","Title":"Optimal Phylogenetic Reconstruction of Insertion and Deletion Events.","Abstract":"Insertions and deletions (indels) influence the genetic code in fundamentally distinct ways from substitutions, significantly impacting gene product structure and function. Despite their influence, the evolutionary history of indels is often neglected in phylogenetic tree inference and ancestral sequence reconstruction, hindering efforts to comprehend biological diversity determinants and engineer variants for medical and industrial applications._x000D_\n_x000D_\nWe frame determining the optimal history of indel events as a single Mixed-Integer Programming (MIP) problem, across all branch points in a phylogenetic tree adhering to topological constraints, and all sites implied by a given set of aligned, extant sequences. By disentangling the impact on ancestral sequences at each branch point, this approach identifies the minimal indel events that jointly explain the diversity in sequences mapped to the tips of that tree. MIP can recover alternate optimal indel histories, if available._x000D_\n_x000D_\nWe evaluated MIP for indel inference on a dataset comprising 15 real phylogenetic trees associated with protein families ranging from 165 to 2000 extant sequences, and on 60 synthetic trees at comparable scales of data and reflecting realistic rates of mutation. Across relevant metrics, MIP outperformed alternative parsimony-based approaches and reported the fewest indel events, on par or below their occurrence in synthetic datasets. MIP offers a rational justification for indel patterns in extant sequences; importantly, it uniquely identifies global optima on complex protein data sets without making unrealistic assumptions of independence or evolutionary underpinnings, promising a deeper understanding of molecular evolution and aiding novel protein design."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Tanviben Patel","Title":"Approximating facial expression effects on diagnostic accuracy via generative AI","Abstract":"Artificial Intelligence (AI) is increasingly used in genomics research and practice, and generative AI has garnered significant recent attention. In clinical applications of generative AI, aspects of the underlying datasets can impact results, and confounders should be studied and mitigated. One example involves the facial expressions of people with genetic conditions. Stereotypically, Williams (WS) and Angelman (AS) syndromes are associated with a â€œhappyâ€ demeanor, including a smiling expression. Clinical geneticists may be more likely to identify these conditions in images of smiling individuals. To study the impact of facial expression, we analyzed publicly available facial images of approximately 3500 individuals with genetic conditions. Using a deep learning (DL) image classifier, we found that WS and AS images with non-smiling expressions had significantly lower prediction probabilities for the correct syndrome labels than those with smiling expressions. This was not seen for 22q11.2 deletion and Noonan syndromes, which are not associated with a smiling expression. To further explore the effect of facial expressions, we computationally altered the facial expressions for these images. We trained HyperStyle, a GAN-inversion technique compatible with StyleGAN2, to determine the vector representations of our images. Then, following the concept of InterfaceGAN, we edited these vectors to recreate the original images in a phenotypically accurate way but with a different facial expression. Through online surveys and an eye-tracking experiment, we examined how altered facial expressions affect the performance of human experts. We overall found that facial expression is associated with diagnostic accuracy variably in different genetic conditions."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Mauminah Raina","Title":"Using Relation Equivariant Graph Neural Networks to Explore the Mosaic-like Tissue Architecture of Kidney Diseases with Spatially Resolved Transcriptomics","Abstract":"Chronic kidney disease is one of the leading public health problems with a prevalence of around 15% adults in the world. Emerging spatially resolved transcriptomics (SRT) technologies provide unprecedented opportunities to discover the spatial patterns of gene expression in diseases. Currently most existing computational tools are designed and tested on the ribbon-like brain cortex, often making it challenging to identify highly heterogeneous mosaic-like tissue architectures, such as tissues from kidney diseases. This hurdle demands heightened precision in discerning the cellular and morphological changes within renal tubules and their interstitial niches. We present an empowered graph deep learning framework, REGNN (Relation Equivariant Graph Neural Networks), for SRT data analyses on heterogeneous tissue structures. To increase expressive power in the SRT lattice using graph modeling, REGNN integrates equivariance, handling the rotational and translational symmetries of the spatial space, and Positional Encoding to strengthen the relative spatial relations of the nodes uniformly distributed in the lattice. With limited availability of labels, both Graph Autoencoder and graph self-supervised learning strategies are built on REGNN. Our study finds that REGNN outperforms existing computational tools in identifying tissue architectures of samples in mosaic-like heterogenous samples sourced from different kidney diseases using the 10X Visium platform. In case studies, the results identified by REGNN are validated by annotations from experienced nephrology physicians. This proposed framework explores the expression patterns of highly heterogeneous tissues with an enhanced graph deep learning model and paves the way to pinpoint underlying pathological mechanisms that contribute to the progression of complex diseases."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Young Je Lee","Title":"scResolve: Recovering single cell expression profiles from multi-cellular spatial transcriptomics","Abstract":"Many popular spatial transcriptomics techniques lack single-cell resolution. Instead, these methods measure the collective gene expression for each location from a mixture of cells, potentially containing multiple cell types. Here, we developed scResolve, a method for recovering single-cell expression profiles from spatial transcriptomics measurements at multi-cellular resolution. scResolve accurately restores expression profiles of individual cells at their locations, which is unattainable from cell type deconvolution. Applications of scResolve on human breast cancer data and human lung disease data demonstrate that scResolve enables cell type-specific differential gene expression analysis between different tissue contexts and accurate identification of rare cell populations. The spatially resolved cellular-level expression profiles obtained through scResolve facilitate more flexible and precise spatial analysis that complements raw multi-cellular level analysis."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"William Bowie","Title":"Multiview factorization for joint modeling of spatial multi-omics and histology images via NMF","Abstract":"An increasing number of cancer research studies employ spatially resolved transcriptomics (SRT) to investigate the composition of tumor microenvironment (TME) in a cancer type of interest. These studies have defined TME states and spatial domains based on clustering spatial gene expression patterns in SRT in an unbiased manner, yet a more thorough delineation of TME states requires the incorporation of the tumorâ€™s histology image. Here, we implement a multiview formulation of NMF, which is a matrix factorization approach that is suitable for cancer research studies where joint profiles of spatial multi-omics and tumor histology images are available. We apply MultiNMF-XT (pronounced extra) to analyze a set of TNBC SRT primary tumor samples and reveal TME states in the stromal, epithelial, and immune enriched compartments, defined by distinct histomorphological features. MultiNMF-XT is written in C++ and is 3.5-fold faster than Python NMF implementation. _x000D_\n_x000D_\nMultiNMF-XT can factorize the SRT into components well-supported by histological evidence. It identified T-cell infiltrating regions, and EMT-enriched regions with distinct immune and stroma morphological characteristics. The T-cell infiltration neighbors a region that has an appearance of necrosis according to the pathologist evaluation. These domains further demonstrate enrichment of motifs according to SCENIC. We further illustrate approachâ€™s ability to extend to paired spatial ATAC-seq and histology of HER2 breast cancer. This analysis not only reveals ERBB2 amplicon amplification, but also derives regulatory regions for T- and B-cells that are variable between clones. MultiNMF-XT thus permits an automated, data-driven decomposition of SRT and spatial ATACseq supported by histomorphological evidence."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Erik Wright","Title":"Predicting gene functional associations from coevolutionary signals with EvoWeaver","Abstract":"The universe of uncharacterized proteins is expanding far faster than our ability to annotate their functions through laboratory study. Computational annotation approaches rely on similarity to previously studied proteins, thereby ignoring unstudied proteins. This phenomenon gives rise to a \"rich get richer\" scenario: the majority of research focuses on a small subset of proteins. Coevolutionary approaches hold promise for injecting new information into our knowledge of the protein universe by linking proteins through 'guilt-by-association'. However, existing coevolutionary algorithms have insufficient accuracy and scalability to connect the entire universe of proteins. We present EvoWeaver, an algorithm that weaves together 12 distinct signals of coevolution to quantify the degree of shared evolution between genes. EvoWeaver's signals encompass phylogenetic profiling, phylogenetic structure, gene organization, and sequence-level methods that broadly capture coevolution between sequences. EvoWeaver accurately identifies proteins involved in protein complexes or separate steps of a biochemical pathway. We demonstrate the merits of EvoWeaver by partly reconstructing known biochemical pathways without any prior knowledge other than genome sequences. Additionally, we show that EvoWeaver's predictions rival those of the widely used STRING database without reliance on prior biological knowledge. Finally, we leverage EvoWeaver's predictions to uncover experimentally validated functional associations among genes that are absent from existing databases. This work forms one of the largest scale analyses of protein functional relationships to date, encompassing 1,545 gene groups from 8,564 genomes. Given its predictive power and speed, EvoWeaver has the potential to revolutionize protein functional prediction at the scale of the protein universe."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Domagoj Doresic","Title":"Efficient parameter estimation for ODE models of cellular processes using semi-quantitative data","Abstract":"Quantitative dynamical models facilitate the understanding of biological processes and the prediction of their dynamics. The parameters of these models are commonly estimated from experimental data. Yet, experimental data generated from different techniques do not provide direct information about the state of the system but a non-linear (monotonic) transformation of it. For such semi-quantitative data, when this transformation is unknown, it is not apparent how the model simulations and the experimental data can be compared. Here, we propose a versatile spline-based approach for the integration of a broad spectrum of semi-quantitative data into parameter estimation. We derive analytical formulas for the gradients of the hierarchical objective function and show that this substantially increases the estimation efficiency. Subsequently, we demonstrate that the method allows for the reliable discovery of unknown measurement transformations. Furthermore, we show that this approach can significantly improve the parameter inference based on semi-quantitative data in comparison to available methods. Modelers can easily apply our method by using our implementation in the open-source Python Parameter EStimation TOolbox (pyPESTO)."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Emily Clough","Title":"NCBIâ€™s RNA-seq analysis pipeline produces millions of pre-computed gene expression counts to accelerate data reuse and discovery","Abstract":"The NIH Sequence Read Archive (SRA) is a diverse collection of DNA and RNA sequences that together document the genetic diversity across the tree of life. This repository provides a platform where individual contributors can add their data to a growing corpus that is available for subsequent reanalysis by the scientific community. SRA has grown exponentially over the past decade to more than 29 petabytes of data with 27 million sequence samples, of which 40% are derived from RNA. RNA-sequencing (RNA-seq) is a powerful method used to analyze transcription quantitatively and qualitatively at the subcellular, single-cell and tissue level and has become a standard molecular biology application. To propel reanalysis and use of SRAâ€™s expansive volume of RNA-seq data, SRA has built a cloud-based RNA-seq analysis pipeline using publicly available software to provide users with consistently computed gene expression counts for all human RNA-seq samples contained within SRA.  To date, counts have been produced for 1.3 million human RNA-seq runs. The pipeline runs continuously creating new count files as RNA-seq runs are released for public access. SRAâ€™s RNA-seq counts are used to create read count matrices for ~26,000 studies held in NIHâ€™s Gene Expression Omnibus (GEO) database and have been incorporated into GEOâ€™s online analysis tool, GEO2R. The run-level counts generated by SRA will be made available by Amazon Web Service (AWS). Consistently computed RNA-seq gene counts reduce cost, effort and time barriers to data reuse and enable large-scale analyses with high statistical power."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Jingtao Wang","Title":"Enabling Affordable Single-Cell Data in Large Cohort Studies via Deep Generative Neural Networks and Active Learning","Abstract":"Single-cell sequencing is a crucial tool for dissecting the cellular intricacies of complex diseases. Its prohibitive cost, however, hampers its application in expansive biomedical studies. Traditional cellular deconvolution approaches can infer cell type proportions from more affordable bulk sequencing data, yet they fall short in providing the detailed resolution required for single-cell-level analyses. To overcome this challenge, we introduce â€œscSemiProfilerâ€, an innovative computational framework that marries deep generative models with active learning strategies. This method adeptly infers single-cell profiles across large cohorts by fusing bulk sequencing data with targeted single-cell sequencing from a few carefully chosen representatives. Extensive validation across heterogeneous datasets verifies the precision of our semi-profiling approach, aligning closely with true single-cell profiling data and empowering refined cellular analyses. Originally developed for extensive disease cohorts, â€œscSemiProfilerâ€ is adaptable for broad applications. It provides a scalable, cost-effective solution for single-cell profiling, facilitating in-depth cellular investigation in various biological domains."},{"Track":"General Computational Biology","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Yashu Vashishath","Title":"HINN: A Novel Neural Network Architecture to Integrate Multi-Omics Data based on their Biological Relationships","Abstract":"Introduction: _x000D_\n_x000D_\nMethods:  _x000D_\n_x000D_\nResults:  _x000D_\n_x000D_\nConclusion:"},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-10:45","Format":"In Person","Speaker":null,"Title":"Welcome","Abstract":null},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"10:45-11:40","Format":"In Person","Speaker":"Sushmita Roy","Title":"Unsupervised learning approaches for genomics to decipher structure and dynamics of 3D genome organization and gene regulatory networks","Abstract":"Advances in genomic technologies have substantially expanded our repertoire of high-dimensional datasets that capture different modalities such as the transcriptome, epigenome and chromosome conformation across many different cellular contexts. An open challenge is to effectively analyze these datasets to extract meaningful structures such as cell types, chromosomal domains, gene modules and regulatory networks.  Unsupervised machine learning that aims to extract structure, often low-dimensional, from unlabeled data is a powerful paradigm for unbiased analysis of omic datasets. In this talk, I will present two examples of such approaches, Non-negative matrix factorization (NMF) and graph structure learning to tackle problems in regulatory genomics. We consider multi-task extensions of NMF for examining three-dimensional organization of the genome. Our results show that NMF is a powerful approach for analyzing 3D genome organization from Hi-C assays that can recover biologically meaningful topological units and their dynamics. In the second part of my talk, I will present factorization and graph learning approaches to  single cell omic datasets. Using our approaches we have identified key gene expression programs and cell type-specific gene regulatory networks that are informative of cell state and fate specification in different dynamic processes such as cellular differentiation and reprogramming."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Qi Liu","Title":"An Adaptive K-Nearest Neighbor Graph Optimized for Single-cell and Spatial Clustering","Abstract":"Unsupervised clustering is crucial for characterizing cellular heterogeneity in single-cell and spatial transcriptomics analysis. While conventional clustering methods have difficulty in identifying rare cell types, approaches specifically tailored for detecting rare cell types gain their ability at the cost of poorer performance for grouping abundant ones. We introduce aKNNO, a method to identify abundant and rare cell types simultaneously based on an adaptive k-nearest neighbor graph with optimization. Unlike traditional kNN graphs, which require a predetermined and fixed k value for all cells, aKNNO selects k for each cell adaptively based on its local distance distribution. This adaptive approach enables accurate capture of the inherent cellular structure. Through extensive evaluation across 38 simulated scenarios and 20 single-cell and spatial transcriptomics datasets spanning various species, tissues, and technologies, aKNNO consistently demonstrates its power in accurately identifying both abundant and rare cell types. Remarkably, aKNNO outperforms conventional and even specifically tailored methods by uncovering both known and novel rare cell types without compromising clustering performance for abundant ones. Most notably, when utilizing transcriptome data alone, aKNNO delineates stereotyped fine-grained anatomical structures more precisely than integrative approaches combining expression with spatial locations and\/or histology images, including GraphST, SpaGCN, BayesSpace, stLearn, and DR-SC."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Yuan Gao","Title":"Forseti: A mechanistic and predictive model of the splicing status of scRNA-seq reads","Abstract":"Motivation: Short-read single-cell RNA-sequencing (scRNA-seq) has been used to study cellular heterogeneity, cellular fate, and transcriptional dynamics. Modeling splicing dynamics in scRNA-seq data is challenging, with inherent difficulty in even the seemingly straightforward task of elucidating the splicing status of the molecules from which the underlying sequenced fragments are drawn. This difficulty arises, in part, from the limited read length and positional biases, which substantially reduce the specificity of the sequenced fragments. As a result, the splicing status of many reads in scRNA-seq is ambiguous because of a lack of definitive evidence. We are therefore in need of methods that can recover the splicing status of ambiguous reads which, in turn, can lead to more accuracy and confidence in downstream analyses._x000D_\nResults: We develop Forseti, a predictive model to probabilistically assign a splicing status to scRNA-seq reads. Our model has two key components. First, we train a binding affinity model to assign a probability that a given transcriptomic site is_x000D_\nused in fragment generation. Second, we fit a robust fragment length distribution model that generalizes well across datasets deriving from different species and tissue types. Forseti combines these two trained models to predict the splicing status of the molecule of origin of reads by scoring putative fragments that associate each alignment of sequenced reads with proximate potential priming sites. Using both simulated and experimental data, we show that our model can precisely predict the splicing status of reads and identify the true gene origin of multi-gene mapped reads."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Vikram Shivakumar","Title":"Sigmoni: classification of nanopore signal with a compressed pangenome index","Abstract":"Improvements in nanopore sequencing necessitate efficient classification methods, including pre-filtering and adaptive sampling algorithms that enrich for reads of interest. Signal-based approaches circumvent the computational bottleneck of basecalling. But past methods for signal-based classification do not scale efficiently to large, repetitive references like pangenomes, limiting their utility to partial references or individual genomes. We introduce Sigmoni: a rapid, multiclass classification method based on the r-index that scales to references of hundreds of Gbps. Sigmoni quantizes nanopore signal into a discrete alphabet of picoamp ranges. It performs rapid, approximate matching using matching statistics, classifying reads based on distributions of picoamp matching statistics and co-linearity statistics, all in linear query time without the need for seed-chain-extend. Sigmoni is 10-100Ã— faster than previous methods for adaptive sampling in host depletion experiments with improved accuracy, and can query reads against large microbial or human pangenomes. Sigmoni is the first signal-based tool to scale to a complete human genome and pangenome while remaining fast enough for adaptive sampling applications."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Harun Mustafa","Title":"Label-guided seed-chain-extend alignment on annotated De Bruijn graphs","Abstract":"Motivation: Exponential growth in sequencing databases has motivated scalable De Bruijn graph-based (DBG) indexing for searching these data, using annotations to label nodes with sample IDs. Low-depth sequencing samples correspond to fragmented subgraphs, complicating finding the long contiguous walks required for alignment queries. Aligners that target single-labelled subgraphs reduce alignment lengths due to fragmentation, leading to low recall for long reads. While some (e.g., label-free) aligners partially overcome fragmentation by combining information from multiple samples, biologically-irrelevant combinations in such approaches can inflate the search space or reduce accuracy._x000D_\n_x000D_\nResults: We introduce a new scoring model, multi-label alignment (MLA), for annotated DBGs. MLA leverages two new operations: To promote biologically-relevant sample combinations, Label Change incorporates more informative global sample similarity into local scores. To improve connectivity, Node Length Change dynamically adjusts the DBG node length during traversal. Our fast, approximate, yet accurate MLA implementation has two key steps: a single-label seed-chain-extend aligner (SCA) and a multi-label chainer (MLC). SCA uses a traditional scoring model adapting recent chaining improvements to assembly graphs and provides a curated pool of alignments._x000D_\nMLC extracts seeds from SCAâ€™s alignments, produces multi-label chains using MLA scoring, then finally forms multi-label alignments. We show via substantial improvements in taxonomic classification accuracy that MLA produces biologically-relevant alignments, decreasing average weighted UniFrac errors by 63.1â€“66.8% and covering 45.5â€“47.4% (median) more long-read query characters than state-of-the-art aligners. MLAâ€™s runtimes are competitive with label-free alignment and substantially faster than single-label alignment._x000D_\n_x000D_\nAvailability: https:\/\/github.com\/ratschlab\/mla."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Stephen Hwang","Title":"Compressed Indexing for Pangenome Substring Queries","Abstract":"Pangenomes are growing in number and size, thanks to the prevalence of high-quality long-read assemblies. However, current methods for studying sequence composition and conservation within pangenomes have limitations. Methods based on graph pangenomes require a computationally expensive multiple-alignment step, which can leave out some variation. Indexes based on k-mers and de Bruijn graphs are limited to answering questions at a specific substring length k. We present Maximal Exact Match Ordered (MEMO), a pangenome indexing method based on maximal exact matches (MEMs) between sequences. A single MEMO index can handle arbitrary-length queries over pangenomic windows. MEMO enables both queries that test k-mer presence\/absence (membership queries) and that count the number of genomes containing k-mers in a window (conservation queries). MEMO's index for a pangenome of 89 human autosomal haplotypes fits in 2.04 GB, 8.8x smaller than a comparable KMC3 index and 11.4x smaller than a PanKmer index. MEMO indexes can be made smaller by sacrificing some counting resolution, with our decile-resolution HPRC index reaching 0.67 GB. MEMO can conduct a conservation query for 31-mers over the human leukocyte antigen locus in 13.89 seconds, 2.5x faster than other approaches. MEMO's small index size, lack of k-mer length dependence, and efficient queries make it a flexible tool for studying and visualizing substring conservation in pangenomes."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Hugo Magalhães","Title":"Sequence-to-graph alignment based copy number calling using a flow network formulation","Abstract":"Variation of copy number (CN) between individuals has been associated with phenotypic differences. Consequently, CN calling is an important step for disease association and identification, as well as in genome assembly. Traditionally, sequencing reads are mapped to a linear reference genome, after which CN is estimated based on observed read depth. This approach, however, leads to inconsistent CN assignments and is hampered by sequences not represented in a linear reference. To address this issue, we propose a method for CN calling with respect to a graph genome using a flow network formulation._x000D_\nThe tool processes read alignments to any bidirected genome graph, and calculates CN probabilities for every node according to the Negative Binomial distribution and total base pair coverage across the node. Integer linear programming is then employed to find a maximum likelihood flow through the graph, resulting in CN predictions for each node. This way, the method achieves consistent CN assignments across the graph._x000D_\nThe proposed method is capable of processing a wide variety of input graphs and read mappings from different sequencing technologies. We processed reads aligned to a Verkko assembly graph for HG02492 (HGSVC) using high coverage mixed HiFi and ONT-UL reads in under 2 hours using one thread and <2Gb peak memory. For 18% nodes, the method produced different CN values than those expected from read depth alone, showcasing how the graph topology informs CN assignment. Further applications include CN assignment as part of diploid\/polyploid (pan)genome assembly workflows."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Timofey Prodanov","Title":"Targeted genotyping of complex polymorphic genes using short and long reads","Abstract":"The human genome contains numerous highly polymorphic loci, rich in tandem repeats and structural variants. There, read alignments are often ambiguous and unreliable, resulting in hundreds of disease-associated genes being inaccessible for accurate variant calling. In such regions, structural variant callers show limited sensitivity, k-mer based tools cannot exploit full linkage information of a sequencing read, and gene-specific methods cannot be easily extended to process more loci. Improved ability to genotype highly polymorphic genes can increase diagnostic power and uncover novel disease associations._x000D_\nWe present a targeted tool Locityper, capable of genotyping complex polymorphic loci using both short- and long-read whole genome sequencing, including error-prone ONT data. For each target, Locityper recruits WGS reads and aligns them to possible locus haplotypes (e.g. extracted from a pangenome). By optimizing read alignment, insert size, and read depth profiles across haplotypes, Locityper efficiently estimates the likelihood of each haplotype pair. This is achieved by solving integer linear programming problems or by employing stochastic optimization._x000D_\nAcross 256 challenging medically relevant loci and 40 HPRC Illumina datasets, 95% Locityper haplotypes were accurate (QV, Phred-scaled divergence, â‰¥33), compared to 27% accurate haplotypes, reconstructed from the phased NYGC call set. In leave-one-out (LOO) evaluation, Locityper produced 60% accurate haplotypes, a fraction that will increase with larger reference panels as >91% haplotypes were very close (Î”QVâ‰¤5) to best available haplotypes. Overall, 82% 1KGP trio haplotypes were concordant. Finally, across 36 HLA genes LOO Locityper correctly predicted protein product in 94% cases, outperforming the specialized HLA-genotyper T1K at 78%."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Varuni Sarwal","Title":"VISTA: An integrated framework for structural variant discovery","Abstract":"Structural variation (SV), refers to insertions, deletions, inversions, and duplications in human genomes. With advances in whole genome sequencing (WGS) technologies, a plethora of SV detection methods have been developed. However, dissecting SVs from WGS data remains a challenge, with the majority of SV detection methods prone to a high false-positive rate, and no existing method able to precisely detect a full range of SVs present in a sample. Here, we report an integrated structural variant calling framework, VISTA (Variant Identification and Structural Variant Analysis) that leverages the results of individual callers using a novel and robust filtering and merging algorithm. In contrast to existing consensus-based tools which ignore the length and coverage, VISTA overcomes this limitation by executing various combinations of top-performing callers based on variant length and genomic coverage to generate SV events with high accuracy. We evaluated the performance of VISTA on comprehensive gold-standard datasets across varying organisms and coverage. We benchmarked VISTA using the Genome-in-a-Bottle (GIAB) gold standard SV set, haplotype-resolved de novo assemblies from The Human Pangenome Reference Consortium (HPRC), along with an in-house PCR-validated mouse gold standard set. VISTA maintained the highest F1 score among top consensus-based tools measured using a comprehensive gold standard across both mouse and human genomes.In conclusion, VISTA represents a significant advancement in structural variant calling, offering a robust and accurate framework that outperforms existing consensus-based tools and sets a new standard for SV detection in genomic research."},{"Track":"HiTSeq","Room":"517d","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-18:00","Format":"In Person","Speaker":null,"Title":"Long-read sequencing and pangenome perspective of structural variation","Abstract":null},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:40","Format":"In Person","Speaker":null,"Title":"Why and how long reads are used to improve gene isoform quantification","Abstract":null},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Sudhanva Shyam Kamath","Title":"Telomere-to-telomere assembly by preserving contained reads","Abstract":"Automated telomere-to-telomere (T2T) de novo assembly of diploid and polyploid genomes remains a formidable task. A string graph is a commonly used assembly graph representation in the overlap-based algorithms. The string graph formulation employs graph simplification heuristics, which drastically reduce the count of vertices and edges. One of these heuristics involves removing the reads contained in longer reads. However, this procedure is not guaranteed to be safe. In practice, it occasionally introduces gaps in the assembly by removing all reads covering one or more genome intervals. The factors contributing to such gaps remain poorly understood. In this work, we mathematically derived the frequency of observing a gap near a germline and a somatic heterozygous variant locus. Our analysis shows that (i) an assembly gap due to contained read deletion is an order of magnitude more frequent in Oxford Nanopore reads than PacBio HiFi reads due to differences in their read-length distributions, and (ii) this frequency decreases with an increase in the sequencing depth. Drawing cues from these observations, we addressed the weakness of the string graph formulation by developing the RAFT assembly algorithm. RAFT fragments reads and produces a more uniform read-length distribution. The algorithm retains spanned repeats in the reads during the fragmentation. We empirically demonstrate that RAFT significantly reduces the number of gaps using simulated datasets. Using real Oxford Nanopore and PacBio HiFi datasets of the HG002 human genome, we achieved a twofold increase in the contig NG50 and the number of haplotype-resolved T2T contigs compared to Hifiasm."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Can Firtina","Title":"Rawsamble: Overlapping and Assembling Raw Nanopore Signals using a Hash-based Seeding Mechanism","Abstract":"Although raw nanopore signal mapping to a reference genome is widely studied to achieve highly accurate and fast mapping of raw signals, mapping to a reference genome is not possible when the corresponding reference genome of an organism is either unknown or does not exist. To circumvent such cases, all-vs-all overlapping is performed to construct de novo assembly from overlapping information. However, such an all-vs-all overlapping of raw nanopore signals remains unsolved due to its unique challenges such 1) generating multiple and accurate mapping pairs per read, 2) performing similarity search between a pair of noisy raw signals, and 3) performing space- and compute-efficient operations for portability and real-time analysis._x000D_\n_x000D_\nWe introduce Rawsamble, the first mechanism that can quickly and accurately find overlaps between raw nanopore signals without translating them to bases. We find that Rawsamble can 1) find overlaps while meeting the real-time requirements with throughput on average around 200,000 bp\/sec, 2) share a large portion of overlapping pairs with minimap2 (37.12% on average), and 3) lead to constructing long assemblies from these useful overlaps. Finding overlapping pairs from raw signals is critical for enabling new directions that have not been explored before for raw signal analysis, such as de novo assembly construction from overlaps that we explore in this work. We believe these overlaps can be useful for many other new directions coupled with real-time analysis._x000D_\n_x000D_\nRawsamble is integrated in RawHash and available at https:\/\/github.com\/CMU-SAFARI\/RawHash."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Igor Martayan","Title":"Conway-Bromage-Lyndon (CBL): an exact, dynamic representation of k-mer sets","Abstract":"In this paper, we introduce the Conway-Bromage-Lyndon (CBL) structure, a compressed, dynamic and exact method for_x000D_\nrepresenting k-mer sets. Originating from Conway and Bromageâ€™s concept, CBL innovatively employs the smallest cyclic_x000D_\nrotations of k-mers, akin to Lyndon words, to leverage lexicographic redundancies. In order to support dynamic operations_x000D_\nand set operations, we propose a dynamic bit vector structure that draws a parallel with Elias-Fanoâ€™s scheme. This_x000D_\nstructure is encapsulated in a Rust library, demonstrating a balanced blend of construction efficiency, cache locality, and_x000D_\ncompression. Our findings suggest that CBL outperforms existing k-mer set methods, particularly in dynamic scenarios._x000D_\nUnique to this work, CBL stands out as the only known exact k-mer structure offering in-place set operations. Its different_x000D_\ncombined abilities positions it as a flexible Swiss knife structure for k-mer set management."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Xin Yuan","Title":"Learning Locality-Sensitive Bucketing Functions","Abstract":"Many tasks in sequence analysis ask to identify biologically related sequences in a large set. Edit distance is widely used in these tasks as a measure. To avoid all-vs-all pairwise comparisons and save on expensive edit distance computations, locality-sensitive bucketing (LSB) functions have been proposed. Formally, a (d1,d2)-LSB function sends sequences into multiple buckets with the guarantee that pairs of sequences of edit distance at most d1 can be found within a same bucket while those of edit distance at least d2 do not share any. LSB functions generalize the locality-sensitive hashing (LSH) functions and admit favorable properties, making them potentially ideal solutions to the above problem. But constructing LSB functions for practical use is scarcely possible. In this work, we aim to utilize machine learning techniques to train LSB functions. With the development of a novel loss function and insights in the neural network structures that can extend beyond this specific task, we obtained LSB functions that exhibit nearly perfect accuracy for certain (d1,d2). Comparing to the state-of-the-art method OMH, the trained LSB functions achieve a 2- to 5-fold improvement on the sensitivity of recognizing similar sequences. An experiment on analyzing erroneous cell barcode data is also included to demonstrate the application of the trained LSB functions."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Kayvon Mazooji","Title":"Fast Multiple Sequence Alignment via Multi-Armed Bandits","Abstract":"Multiple sequence alignment is an important problem in computational biology with applications that include phylogeny and the detection of remote homology between protein sequences.  UPP is a popular software package that constructs accurate multiple sequence alignments for large datasets based on ensembles of Hidden Markov Models (HMMs).  A computational bottleneck for this method is a sequence-to-HMM assignment step, which relies on the precise computation of probability scores on the HMMs.  In this work, we show that we can speed up this assignment step significantly by replacing these HMM probability scores with alternative scores that can be efficiently estimated. Our proposed approach utilizes a Multi-Armed Bandit algorithm to adaptively and efficiently compute estimates of these scores.  This allows us to achieve similar alignment accuracy as UPP with a significant reduction in computation time."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Seong Woo Han","Title":"Contrasting and Combining Transcriptome Complexity Captured by Short and Long RNA Sequencing Reads","Abstract":"High-throughput short-read RNA sequencing has given researchers unprecedented detection and quantification capabilities of splicing variations across biological conditions and disease states. However, short-read technology is limited in its ability to identify which isoforms are responsible for the observed sequence fragments and how splicing variations across a gene are related. In contrast, more recent long-read sequencing technology offers improved detection of underlying full or partial isoforms but is limited by high error rates and throughput, hindering its ability to accurately detect and quantify all splicing variations in a given condition._x000D_\n_x000D_\nTo better understand the underlying isoforms and splicing changes in a given biological condition, itâ€™s important to be able to combine the results of both short and long-read sequencing, together with the annotation of known isoforms. To address this need, we develop MAJIQ-L, a tool to visualize and quantify splicing variations from multiple data sources. MAJIQ-L combines transcriptome annotation, long reads based isoform detection tools output, and MAJIQ (Vaquero-Garcia et al. (2016, 2023)) based short-read RNA-Seq analysis of local splicing variations (LSVs). We analyze which splice junction is supported by which type of evidence (known isoforms, short-reads, long-reads), followed by the analysis of matched short and long-read human cell line datasets. Our software can be used to assess any future long reads technology or algorithm, and combine it with short reads data for improved transcriptome analysis."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Sergii Strelchuk","Title":"Quantum Computing for Genomic Analysis","Abstract":"Many essential tasks in genomic analysis are extremely difficult for classical computers due to problems inherently hard to solve efficiently with classical (empirical) algorithms. Quantum computing offers novel possibilities with algorithmic techniques capable of achieving provable speedups over existing classical exact algorithms in large-scale genomic analyses. Our work utilizes PhiX174, SARS-CoV-2, and human genome data to explore quantum algorithms and data encoding techniques to pave the way for the analysis with better time and space efficiency._x000D_\n_x000D_\nWe take a two-pronged approach:_x000D_\n_x000D_\n1) Algorithm Development: We will design novel quantum algorithms for MSA subproblems and heuristic methods (QAOA) for de novo assembly._x000D_\n _x000D_\n2) Data Encoding and State Preparation: We develop efficient quantum circuits to encode genomic data and reduce the computational overhead with a variety of techniques, including tensor network methods. It facilitates data encoding into quantum states for Machine Learning applications._x000D_\n_x000D_\nStarting with the PhiX174 genome, we will test our quantum algorithms with provable theoretical speedup compared to classical methods. This allows us to scale the approach to larger and more complex genomes like SARS-CoV-2 and the human genome. We'll develop efficient encoding strategies and optimize quantum circuits to minimize resource needs for the current hardware. To test how noise sources that appear in a variety of hardware implementations affect the computation we are using recently-developed tensor network contraction methods for efficient small-scale classical simulation._x000D_\n_x000D_\nThis project aims to identify problem settings where utilizing quantum computing will be the most beneficial to unlocking the vast potential of genomics in healthcare. By studying classical computational bottlenecks and developing ways to speed them up, we aim to achieve a deeper understanding of human health and pathogens."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Franziska Görtler","Title":"Adaptive Digital Tissue Deconvolution","Abstract":"Motivation: The inference of cellular compositions from bulk and spatial transcriptomics data increasingly complements data analyses. Multiple computational approaches were suggested and recently, machine learning techniques were developed to systematically improve estimates. Such approaches allow to infer additional, less abundant cell types. However, they rely on training data which do not capture the full biological diversity encountered in transcriptomics analyses; data can contain cellular contributions not seen in the training data and as such, analyses can be biased or blurred. Thus, computational approaches have to deal with unknown, hidden contributions. Moreover, most methods are based on cellular archetypes which serve as a reference; e.g., a generic T-cell profile is used to infer the proportion of T-cells. It is well known that cells adapt their molecular phenotype to the environment and that pre-specified cell archetypes can distort the inference of cellular compositions._x000D_\nResults: We propose Adaptive Digital Tissue Deconvolution (ADTD) to estimate cellular proportions of pre-selected cell types together with possibly unknown and hidden background contributions. Moreover, ADTD adapts prototypic reference profiles to the molecular environment of the cells, which further resolves cell-type specific gene regulation from bulk transcriptomics data. We verify this in simulation studies and demonstrate that ADTD improves existing approaches in estimating cellular compositions. In an application to bulk transcriptomics data from breast cancer patients, we demonstrate that ADTD provides insights into cell-type specific molecular differences between breast cancer subtypes._x000D_\nAvailability and implementation: A python implementation of ADTD and a tutorial are available at https:\/\/doi.org\/10.5281\/zenodo.7548362 (doi:10.5281\/zenodo.7548362)."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Jonathan Bard","Title":"Maximizing accuracy of cellular deconvolution. (ACeD)","Abstract":"Bulk RNA-sequencing has been a mainstay for biomedical research since its inception.  In cancer alone, the TCGA project has examined 33 cancer types with over 20,000 samples.  Each sample has a wealth of patient information associated with it, from survival records to several data modalities including copy number, microbiome, methylation and transcriptomic profiling at the bulk tissue level. However, the challenge with bulk tissue profiling, like RNA-seq, is that the assay measures the average expression across all the cells in the sample, thus hiding cellular heterogeneity. Leveraging cellular deconvolution, these datasets can be used to infer cell type composition and molecular heterogeneity. However, accurate deconvolution is contingent upon using a high-quality single-cell reference dataset with proper cell-type cluster resolution. Therefore, there is a fundamental need for methodology to quantify single-cell dataset quality for deconvolution with optimization of cell-type cluster resolution. To address this challenge, we developed a novel computational strategy to identify the optimal cell-type clustering resolution that maximizes deconvolutional performance. Our R-based software package (ACeD) provides the research community with a valuable toolset to evaluate reference set quality and optimize data upstream of reference-based deconvolution algorithms, enhancing our analysis and understanding of the tumor microenvironment."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Marjorie Roskes","Title":"Evolution of genomic and epigenomic heterogeneity in prostate cancer from tissue and liquid biopsy","Abstract":"Castration Resistant Prostate Cancer (CRPC) is an aggressive disease that is highly plastic. Although histologically there are two subtypes of CRPC: adenocarcinoma and neuroendocrine, we have shown it has four distinct molecular subtypes exhibiting differential chromatin and transcriptomic profiles. These are CRPC-AR (androgen receptor dependent), CRPC-WNT (Wnt pathway dependent), CRPC-SCL (stem-cell like), and CRPC-NE (neuroendocrine). During treatment with AR signaling inhibitors, patient tumors can evolve to different subtypes. Clinical identification of these subtypes and mechanistic understanding of the genomic and epigenomic heterogeneity accompanying this evolution is a huge challenge. To address this, we have amassed a unique cohort of 60 CRPC patients with various subtypes from whom cell-free DNA (cfDNA) was collected at various clinically relevant time points and whole-genome sequencing (WGS) was performed. For 24 of these patients, time-matched tissue RNA-seq was performed. We estimated epigenetic\/transcriptomic heterogeneity in tissue by deconvolution of bulk RNA-seq data. We performed nucleosomal profiling from cfDNA WGS to infer tumor chromatin accessibility and estimate each epigenetic subtypeâ€™s fractional contribution. We can detect the different subtypes in cfDNA and find that CRPC-SCL patients exhibit more heterogeneity than other subtypes in both tissue and cfDNA, likely indicating the transitory state of this subtype. We calculated allele-specific, genome-wide copy number alterations in cfDNA, and can track the parallel evolution of genomic and epigenomic events, e.g. AR gains track with increasing CRPC-AR fraction over time. Our study shows that, beyond biomarker development, cfDNA WGS can be used for characterizing the epigenomic and genomic evolution of patient tumors."},{"Track":"HiTSeq","Room":"517d","Weekday":"Sunday","Date":"14 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Rija Zaidi","Title":"Accurate and robust bootstrap inference of single-cell phylogenies by integrating sequencing read counts","Abstract":"Recent single-cell DNA sequencing (scDNA-seq) technologies have enabled the parallel investigation of thousands of individual cells. This is required for accurately reconstructing tumour evolution, during which cancer cells acquire a multitude of different genetic alterations. Although the evolutionary analysis of scDNA-seq datasets is complex due to their unique combination of errors and missing data, several methods have been developed to infer single-cell tumour phylogenies by integrating estimates of the false positive and false negative error rates. This integration relies on the assumption that errors are uniformly distributed both within and across cells. However, this assumption does not always hold; error rates depend on sequencing coverage, which is not constant within or across cells in a sequencing experiment due to, e.g., copy-number alterations and the replication status of a cell, limiting the accuracy of existing methods._x000D_\n_x000D_\nTo address this challenge, we developed a novel single-cell phylogenetic method that integrates raw sequencing read counts into a statistical framework to robustly correct the errors and missing data. Specifically, our method includes bootstrapping to robustly correct for high error frequency genomic positions and a fast probabilistic heuristic based on hypothesis testing to distinguish the remaining errors from truly observed genotypes. We demonstrate the improved accuracy and robustness of our method compared to existing approaches across several simulation settings. To demonstrate its impact, we applied our method to 42,009 breast cancer cells and 19,905 ovarian cancer cells, revealing more accurate phylogenies consistent with larger genetic alterations."},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-10:50","Format":"In Person","Speaker":null,"Title":"Introduction","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"10:50-11:20","Format":"In Person","Speaker":"Barry Honig","Title":"Predicting Protein-Protein Interactions","Abstract":"The PrePPI algorithm is unique in its ability to predict whether and how two proteins interact on a proteomic scale. PrePPI considers all 200 million possible PPIs in the human proteome and over a million high confidence predictions appear in its online database. PrePPI is based primarily on the use of structural alignment to identify templates in the PDB for homology modeling of complexes but incorporates domain-peptide interactions and non-structural sources of information as well. Recent additions to PrePPI include ZEPPI, which uses evolutionary information to score protein-protein interfaces as well as deep-learning-based approaches. PrePPI performance will be described and its large-scale perspective will be discussed in the context of slower AlphaFold based approaches."},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"11:20-11:50","Format":"In Person","Speaker":"John Moult","Title":"Critical assessment in Computational Structural Biology","Abstract":"As science evolves, the old ways of ensuring rigor, particularly the venerable peer-reviewed journal paper system, are breaking down. On the other hand, new technologies are providing opportunities for new approaches and the prospect of more rapid advance. One of these is the community critical assessment experiment. CASP (Critical Assessment of Structure Prediction) was the first and has seen a series of both dead-ends and breakthroughs. In this talk Iâ€™ll review some of the stops and starts, with the goal of drawing conclusions about how efficiently the field has moved forward and the strengths and limitations of the approach.  "},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"11:50-12:10","Format":"In Person","Speaker":"Jianlin Cheng","Title":"Protein Structure Accuracy Estimation Using Geometry-Complete Graph Neural Networks","Abstract":"Estimating the accuracy of protein structural models is a critical task in protein bioinformatics. The need for robust methods in the estimation of protein model accuracy (EMA) is prevalent in the field of protein structure prediction, where computationally-predicted structures need to be screened rapidly for the reliability of the positions predicted for each of their amino acid residues and their overall quality. Current methods proposed for EMA are either coupled tightly to existing protein structure prediction methods or evaluate protein structures without sufficiently leveraging the rich, geometric information available in such structures to guide accuracy estimation. In this work, we propose a geometric message passing graph neural network referred to as the geometry-complete perceptron network for protein structure EMA (GCPNet-EMA), where we demonstrate through rigorous computational benchmarks that GCPNet-EMA's accuracy estimations are faster and more accurate than baseline state-of-the-art methods for both tertiary and quaternary structure EMA. The source code and data for GCPNet-EMA are available on GitHub (https:\/\/github.com\/BioinfoMachineLearning\/GCPNet-EMA). "},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"12:10-12:20","Format":"In Person","Speaker":null,"Title":"The NSF funded iCn3D POSE project","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-14:35","Format":"In Person","Speaker":null,"Title":"The impact of comparative structure analysis on protein classification at NCBI","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"14:35-14:50","Format":"In Person","Speaker":null,"Title":"From Immersive Visualization to Interdisciplinary Communication","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"14:50-15:00","Format":"Live Stream","Speaker":"Marcus De Almeida Mendes","Title":"Visualizing Immune Epitope Data: The Innovations of IEDB-3D 2.0","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"15:00-15:10","Format":"Live Stream","Speaker":"Sandra Porter","Title":"New iCn3D tools for educators: color keys and collections","Abstract":"Molecular modeling programs, such as iCn3D, are increasingly being used in the classroom. Enhancements to accessibility and usability are contributing to adoption. Two such enhancements are color keys and collections.  Color keys help both students and instructors interpret visual information in molecular structure models without having to hunt down user manuals.  Collections make it possible for instructors to gather related models together and provide them to students to support online laboratory investigations. We will discuss examples of using collections to facilitate studying drug resistance in influenza, identifying secondary structure elements in antifreeze proteins, identifying common features of immunoglobulins and T cell receptors, identifying where molecules bind DNA, understanding how mutations can lead to breast cancer, and more.  "},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"15:10-15:25","Format":"In Person","Speaker":"Jiyao Wang","Title":"iCn3D: visualization, analysis & sharing of protein structures and interactions","Abstract":"Itâ€™s challenging to understand 3D structural data. To address this, the web-based iCn3D (I-see-in-3D) viewer uses all kinds of 3D, 2D, and 1D views to make this process easy. Specifically, iCn3D simplifies the 3D structural view, reduces 3D view into synchronized 2D cartoon\/2D interaction network and 1D tracks of annotations, show side by side 3D structure alignment and the corresponding 1D sequence alignment, displays 3D structures with 3D printing, Virtual Reality (VR) or Augumented Reality (AR), and outputs annotations with Node.js\/Python scripts. Furthermore, all iCn3D views can be reproduced in a sharable URL or iCn3D PNG image. The source code of iCn3D is at https:\/\/github.com\/ncbi\/icn3d. "},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"15:25-15:40","Format":"In Person","Speaker":"Henry Jakubowski","Title":"Using iCn3D to address molecular structure\/function and to bridge the biology and chemistry educational communities","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"15:40-15:50","Format":"In Person","Speaker":"Jose Duarte","Title":"RCSB.org: a one-stop shop to explore biology in 3D","Abstract":"We will present the latest developments from the RCSB Protein Data Bank organization in relation to its main web portal RCSB.org. The portal now integrates > 1M Computed Structure Models, calculated by AI methods, alongside the entire corpus of experimental data from the PDB. Thus providing access to all of Structural Biology under a single resource. "},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"15:50-16:00","Format":"In Person","Speaker":"Marcelo Querino Lima Afonso","Title":"Advancing Molecular Graphics: MolviewSpec and Recent Enhancements in Molstar","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-16:50","Format":"In Person","Speaker":null,"Title":"Envisioning the OSE for Biology Research and Education","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"16:50-17:00","Format":"In Person","Speaker":null,"Title":"Community Hackathons","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"17:00-18:00","Format":"In Person","Speaker":null,"Title":"Round Table Discussion","Abstract":null},{"Track":"iCn3D","Room":"524c","Weekday":"Sunday","Date":"14 July","Timespan":"18:00-19:30","Format":"In Person","Speaker":null,"Title":"iCn3D Workshop","Abstract":null},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-10:50","Format":"In Person","Speaker":null,"Title":"Introduction to iRNA track","Abstract":null},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"10:50-11:30","Format":"In Person","Speaker":null,"Title":"SPLASH is a reference-free statistical algorithm, unifying biological discovery in RNA-seq, single cell sequencing and beyond","Abstract":"Myriad mechanisms diversify the sequence content of RNA transcripts and are of great interest to single cell biology. Currently, these events are detected using tools that first require alignment to a necessarily incomplete reference genome alignment in the first step; this incompleteness is especially prominent in diseases such as cancer. Second, today the next step in analysis requires as a custom choice of bioinformatic procedure to follow it: for example, to detect splicing, RNA editing or V(D)J recombination among others. I will present collaborative work based on a new statistics-first analytic method â€”SPLASH (Statistically Primary aLignment Agnostic Sequence Homing)â€” that performs unified, reference-free inference directly on raw sequencing reads without a reference genome or cell metadata. SPLASH is highly efficient and simple to run. As a snapshot of SPLASH, applying to 10,326 primary human single cells in 19 tissues profiled with SmartSeq2, we discover a set of splicing and histone regulators with highly conserved intronic regions that are themselves targets of complex splicing regulation, unreported transcript diversity in the heat shock protein HSP90AA1, and diversification in centromeric RNA expression, V(D)J recombination, RNA editing, and repeat expansions missed by existing methods, as well as unpublished extensions to 10x genomics data."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"11:30-11:50","Format":"In Person","Speaker":"Zachary Wakefield","Title":"Hybrid exons build genome-wide proteomic complexity","Abstract":"Alternative splicing (AS) is a highly regulated process occurring in approximately 95% of encoded proteins, however the global implications on the proteome are largely unknown. To explore how AS impacts the proteome on a genome-wide level, we systematically identified every possible isoform switch annotated in the human genome, resulting from alternative first\/last exons, alternative splice sites, and retained introns, in a pairwise manner. Additionally, we characterized isoform swaps due to the regulation of a newly identified class of exons known as hybrid exons, which can act as terminal or internal exons. We then performed sequence alignment between each protein pair and classified changes as frame shifts, partial expansions\/reductions, and identical proteins. We observed the changing use of hybrid exons between internal and last exons had the most significant impact on protein sequences across all different splicing events. To elucidate the proteomic consequences of AS across phenotypes, we developed SpliceImpactR, a novel open-source R package for protein-domain analysis and isoform-specific domain-derived protein-protein interactions (ISPPI). Using adapted functionality from SpliceImpactR, we quantified the changes in ISPPI and the domain enrichment using the previously identified isoform swaps, revealing varied and unique impacts across each AS type. Applying SpliceImpactR to brain and heart samples from the GTEx database showed over 700 genes with significantly differentially used terminal exons â€“ 33% of the identified swaps classified as strong swaps are caused by changing usage of hybrid exons. Our findings underscore the significance of hybrid exon usage in shaping the proteome diversity expressed in human cells."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"11:50-12:00","Format":"In Person","Speaker":"Priyanka Sehgal","Title":"Splicing-derived neo-epitopes in pediatric high-grade glioma","Abstract":"Pediatric high-grade gliomas (pHGG) respond poorly to standard therapies, and the development of novel immunotherapeutics (such as chimeric antigen receptor (CAR)-armed T cells) is hindered by the paucity of tumor-specific surface antigens. To overcome this problem, we used various algorithms to compare and contrast splicing patterns in 142 pHGGs vs. adult and fetal brain samples, yielding a list of pHGG-specific splice junctions. After prioritizing events corresponding to extracellular domains, we found that ~40% of them mapped to 3-51 nucleotide-long microexons. One salient example is neural cell adhesion molecule (NRCAM) mRNA, which exhibits skipping of the 18-nt microexon 9 and 30-nt microexon 23 (GTEx nomenclature) in ~70% of pHGG samples. Consequently, the corresponding junctions shows much higher expression levels in pHGGs compared to normal tissues of both neural and non-neural origins. Bulk and single-nuclei (SnISOr) long-read RNA-seq of pHGG organoids using the Oxford Nanopore platform revealed coordinated skipping of both microexons and a uniform expression pattern of the Î”ex9Î”ex23 NRCAM isoform across different cell clusters. We validated the surface expression of the corresponding proteoform using live cell biotinylation assay and demonstrated that it increases migration and invasion of KNS42 pHGG cells. We also developed a mouse monoclonal antibody with significantly higher avidity for the Î”ex9Î”ex23 vs. the full-length NRCAM isoform. Therefore, the pHGG-specific NRCAM (and possibly other microexon-derived proteoforms) are highly selective and feasible targets for CAR T cell-based immunotherapies."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Flash talks to advertise the posters","Abstract":null},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Qian Shi","Title":"Accurate Assembly of Multiple RNA-seq Samples with Aletsch","Abstract":"High-throughput RNA sequencing has become indispensable for decoding gene activities, yet the challenge of reconstructing full-length transcripts persists. Traditional single-sample assemblers frequently produce fragmented transcripts, especially in single-cell RNA-seq data. While algorithms designed for assembling multiple samples exist, they encounter various limitations. We present Aletsch, a new assembler for multiple bulk or single-cell RNA-seq samples. Aletsch incorporates several algorithmic innovations, including a â€œbridgingâ€ system that can effectively integrate multiple samples to restore missed junctions in individual samples, and a new graph-decomposition algorithm that leverages â€œsupporting information across multiple samples to guide the decomposition of complex vertices. A standout feature of Aletsch is its application of a random forest model with 50 well-designed features for scoring transcripts. We demonstrate its robust adaptability across different chromosomes, datasets, and species. Our experiments, conducted on RNA-seq data from several protocols, firmly demonstrate Aletschâ€™s significant outperformance over existing meta-assemblers. As an example, when measured with the partial area under the precision-recall curve (pAUC) , Aletsch surpasses the leading assemblers TransMeta by 21.2%-57.4% and PsiCLASS by 21.9%-172.5% on human datasets. Aletsch is freely available at https:\/\/github.com\/Shao-Group\/aletsch."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Beril Erdogdu","Title":"Detecting differential transcript usage in heterogenous populations with SPIT","Abstract":"Differential transcript usage (DTU) plays a crucial role in shaping gene expression diversity across different biological scenarios, influencing cellular functionality and disease development. However, current DTU analysis methods often fail to consider the inherent population heterogeneity seen in complex human traits and diseases. Filling this important gap, our study introduces SPIT, a statistical method specifically designed to identify predominant subgroups and their unique DTU events within populations._x000D_\n_x000D_\nUtilizing an over-smoothed kernel density estimator (KDE), SPIT effectively mitigates technical and biological noise inherent in RNA-Seq data, and detects of multimodality without assumptions about expression pattern distributions. Additionally, SPIT generates an empirical null distribution of isoform abundance variability across datasets, enhancing its accuracy and versatility compared to existing tools._x000D_\n_x000D_\nApplying SPIT to a diverse array of human brain samples, our analysis unveils six significant DTU events associated with Schizophrenia subgroups, underscoring its efficacy in capturing disease heterogeneity. Furthermore, exploration of prenatal and adult brain samples reveals thousands of genes where the dominant isoform undergoes a complete shift between developmental stages and post-birth, providing novel evidence of this phenomenon in human brain development. These findings provide biological significance to specific isoforms previously lacking comprehensive functional understanding, and valuable insights into neurodevelopmental disorders."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:30","Format":"In Person","Speaker":"Ana Victoria Conesa Cegarra","Title":"Bias analysis for long-reads transcriptomics multi-sample datasets","Abstract":"Long-read sequencing technologies such as PacBio and Oxford Nanopore are reshaping transcriptomics. The enhanced precision and depth of sequencing from these methods are proving critical for differential isoform expression studies across various conditions. This shift towards long-reads necessitates new best practices for experimental designs, preprocessing, and normalization tailored to these data types._x000D_\nWe set out to provide analysis guidelines for multi-sample long-read transcriptomics experiments. Utilizing a replicated dataset from mouse tissues and three long-read cDNA protocols, including the newest Pacbio Kinnex, we explored biases when constructing count tables. _x000D_\nWe evaluated two main approaches: Call&Join (call transcript model for each sample and then combine results) and Join&Call (merge reads from different samples, then call transcripts models and re-quantify), finding that each strategy renders a different transcriptome composition depending on the analysis tool. Sequencing depth and replicate number significantly affect transcript identification, with known transcripts quickly stabilizing and novel ones requiring more depth, and most transcripts detected either by all or just one sample._x000D_\nWe detected variable biases in quantification due to read length and GC content across technologies. For instance, PacBio data showed a parabolic length bias and increased expression levels with higher GC content, although this greatly varied by sample, challenging differential analyses._x000D_\nOur study highlights that experimental and preprocessing choices profoundly affect the long-read transcriptome count-tables. Length and GC content biases impact quantification, influenced by sample and technology. The results underscore the importance of thoughtful experimental design and preprocessing to ensure accurate transcriptome dataset composition and comparable quantification."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:30","Format":"In Person","Speaker":"Yu-Jen Lin","Title":"RISE: Relative Impact of Splicing and Expression in RNA-seq studies","Abstract":"RNA-seq has been widely used to quantify expression and splicing changes in transcriptomes. Although biological consequences arise from changes in both expression and splicing aspects, researchers usually use their impressions to choose only one aspect to analyze, potentially overlooking significant impacts of the other. Even if researchers investigate both, the measurement scales of expression and splicing are different, and thus, their impacts are incomparable. To compare the relative impact of expression and splicing, we have developed RISE._x000D_\n_x000D_\nRISE qualifies the relative impact of expression and splicing changes caused by the treatment. To place the impact of expression and splicing changes on the same scale to compare, we developed the Normalized Variation (NV) measure. NV is defined as the proportion of the between-group variation to the total variation. Finally, we assess whether expression NV (eNV) or splicing NV (sNV) is significantly larger to understand the comparative influence of expression versus splicing alternations in the transcriptome._x000D_\n_x000D_\nTo validate our method, we performed RISE analysis on RNA-seq data from knockdown or overexpression experiments of 11 transcription and splicing factors. RISE effectively categorizes transcription and splicing factors by their relative impacts on expression and splicing. As an example application, we applied RISE to 4 studies involving proteins with complex or previously unknown roles in regulating transcriptomes to understand their functions. In summary, RISE enables researchers to systematically compare the relative impact of expression and splicing."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:30","Format":"In Person","Speaker":"Eraj Khokhar","Title":"From Noise to Signal: Quantifying Stochasticity in mRNA Splicing","Abstract":"Splicing is likely a major contributor to noise in mRNA regulation, with errors in splicing leading to reduced transcriptional efficiency and wasted transcriptional output. Cryptic splicing involves use of low-fidelity or infrequently bound splice sites that often leads to non-productive transcripts, likely targeted for degradation. Substantial evidence suggests that splicing noise is prevalent in homeostatic cell conditions, but the extent to which it occurs is likely underappreciated due to the challenges of identifying cryptic, low-fidelity splice site usage in mature mRNA data. Characterizing splicing noise has become increasingly important since blocking or redirecting the use of noisy splice sites in favor of productive splice sites may provide a novel strategy for up-regulating gene expression in healthy or disease contexts with high levels of splicing noise (e.g., cancer). Here, we tackle these challenges by performing high-throughput sequencing on selectively enriched nuclear nascent RNA, which greatly increases the global detection of cryptic splice sites. We further developed a python package to systematically identify and analyze cryptic, low-fidelity sites in high-throughput sequencing data from nascent, nuclear RNA and RNA from cycloheximide-treated cells. We use these experimental and computational methods to analyze cryptic splicing events in cancer cell lines and identify genomic features, sequence elements, and gene properties associated with the occurrence of cryptic splice sites across and between cell types. Our findings uncover a previously under-appreciated role for stochasticity in regulation of mRNA splicing, identify features predictive of splicing noise, and will aid in developing novel disease therapeutics to inhibit cryptic splicing."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"15:30-15:50","Format":"In Person","Speaker":"Teresa Rummel","Title":"Deciphering Transcriptional Bursting Using Single-Cell Metabolic Labeling Data","Abstract":"In single cells, transcription is governed by bursts. The kinetics of transcriptional bursting are defined by the burst frequency, describing how often bursts occur, and the burst size, describing how many mRNA molecules are synthesized during one burst. Knowledge of these parameters of transcriptional bursting is key to gaining a better understanding of gene expression and its regulation._x000D_\n_x000D_\nTranscriptional bursting can be studied in a transcriptome-wide manner using single-cell RNA-seq. However, this approach only works under steady-state conditions and assumes a uniform RNA degradation rate across genes._x000D_\n_x000D_\nTo overcome these limitations, we developed a new mathematical model that utilizes temporal data from single-cell metabolic labeling (scSLAM-seq) to quantitatively assess transcriptional bursting. This model enables the estimation of burst frequencies, sizes, and gene-specific degradation rates, applicable also under dynamic, non-steady state conditions such as upon viral infections or cytokine stimulation._x000D_\n_x000D_\nUsing scSLAM-seq data we studied the role of MYC in transcriptional regulation in two cell lines with different MYC levels, A375 (high) and MaMel63a (low), after MYC depletion via an auxin inducible degron system. Our findings challenge the view that MYC primarily impacts burst size. Instead, we discovered that changes in burst frequency or a combination of both frequency and size are drivers of transcriptional changes, indicating a more complex role for MYC in gene regulation._x000D_\n_x000D_\nThese insights illustrate the benefits of combining advanced sequencing techniques with dynamic modeling to study gene expression, enhancing our understanding of transcriptional mechanisms and providing a framework for analyzing gene responses under various conditions."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"15:50-16:00","Format":"In Person","Speaker":"Pavel Sumazin","Title":"Coordinated regulation by lncRNAs results in tight lncRNA-target couplings","Abstract":"The characterization of long noncoding RNA (lncRNA) function is a major challenge in RNA biology with applications to basic, translational, and medical research. Our efforts to characterize the regulatory roles of lncRNAs in cancer identified lncRNA species that coordinately regulate both the transcriptional and post-transcriptional processing of their targets. This coordinated regulation results in tight couplings between lncRNAs and their targets and is easier to identify and verify. Our analyses suggested that hundreds of cancer genes are coordinately regulated by lncRNAs in multiple tumor types. As proof of principle, we studied the regulation of DICER1â€”a cancer gene that regulates microRNA biogenesisâ€”by the lncRNA ZFAS1. ZFAS1 activates DICER1 transcription and blocks its post-transcriptional repression to control the expression of DICER1 and its target microRNAs. Both genes regulate tumor growth and DNA repair. Our analyses suggested that coordinated lncRNA regulation can propagate genomic alterations at lncRNAs to physiologically dysregulate cancer genes."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Stefan Prodic","Title":"SWARM: Single-molecule Workflow for Analysing RNA Modifications","Abstract":"The epitranscriptome contains over 170 chemical modifications that play a pivotal role in regulating RNA properties and function across various RNA classes. High-throughput methods for RNA modification detection remain limited and current approaches are hindered by extensive protocols that lack isoform-level resolution and restrict studies to a single modification per experiment, limiting comprehensive exploration of the dynamic and diverse epitranscriptome. Here we describe SWARM, a robust approach for the detection of m6A, m5C, pseudouridine, and ac4C from the same sample in individual RNA isoforms. SWARM exploits nanopore direct RNA sequencing  signals that  capture continuous native individual RNA molecules. SWARM attains unmatched accuracy in single-molecule modification detection for multiple RNA modifications through innovative neural network and training strategy applied to a broad array of diverse nanopore signals. We apply SWARM to numerous independent datasets and highlight replicable and accurate detection of modified sites in the transcriptome (messenger RNA and long non-coding RNA) and their modification rates, showing extensive agreement with experimentally validated sites. Our analysis shows that SWARM delivers confident detection of multiple RNA modifications from a single sample and provides a robust framework for comparing RNA modification landscapes between samples. We also provide an efficient workflow that opens a wealth of possibilities towards uncovering diverse RNA modification landscapes in countless contexts. SWARM enables a significant leap towards deciphering the dynamics and functional relevance of the epitranscriptome."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Fatima Mostefai","Title":"Refinement of SARS-CoV-2 Intra-host Mutations Using Explainable Representations","Abstract":"SARS-CoV-2, an RNA virus, has evolved into multiple variants by accumulating mutations during transmission (inter-host) and infection (intra-host). De novo mutations arise in viral genomes during infection, and analyzing these mutations in sequencing data may predict emerging variants. Intra-host single nucleotide variants (iSNVs) can be identified by analyzing RNA sequencing (RNA-seq) reads from infections. However, sequencing artifacts introduced during the RNA-seq process can result in erroneous iSNVs. We aim to identify true intra-host mutations from viral RNA-seq data and propose metrics to refine RNA-seq analysis._x000D_\n_x000D_\nWe developed a two-step workflow to isolate de novo iSNVs, focusing on the SARS-CoV-2 RNA-seq dataset. Initially, we processed a dataset of RNA-seq libraries, ensuring high-quality library preparation through whole-genome quality control. We then used these libraries for iSNV calling, using metrics such as Alternative Allele Frequency (AAF) and Strand Bias Likelihood (S) metrics to distinguish iSNVs from sequencing artifacts. We also used dimensionality reduction representations, such as PHATE and t-SNE, to visualize and analyze library structures complemented with an explainability metric._x000D_\n_x000D_\nWe applied our workflow to a comprehensive SARS-CoV-2 RNA-seq dataset, distinguishing between de novo and consensus iSNVs, which is crucial for understanding viral intra-host evolution. We identified batch effects from sequencing centers and refined the AAF and S metrics for artifact resolution. Analyzing libraries from 2020 to 2023, we observed low intra-host diversity per infection, significant diversity in the spike gene, and strong purifying selection. This workflow enhances the precision and depth of RNA-seq and viral genomic analyses, advancing studies in RNA viruses."},{"Track":"iRNA","Room":"519","Weekday":"Saturday","Date":"13 July","Timespan":"17:20-18:00","Format":"In Person","Speaker":"Ashley Laughney","Title":"Tackling the genotype-to-phenotype problem in cancer evolution","Abstract":"Predicting protein function from sequence, also known as genotype-to-phenotype mapping, remains a central challenge in biology. This is because most proteins are highly pleiotropic; meaning they can perform more than one function and participate in a wide range of biological processes. As such, perturbations to a single gene often affects multiple, independent cellular responses. Integrating innovative systems and synthetic biology approaches with a hypothesis-driven framework, I will describe tools my lab has developed to map genome-encoded components to complex cellular and in vivo functions at scale. We focus on cancer metastasis as our model of a multicellular, evolutionary process and develop approaches that ask how activation of the very same protein or signaling pathway can lead to diverse functional outputs through (i) the evolution of distinct modular domains, (ii) intra-cellular genetic interactions (epistasis) and (iii) inter-cellular signaling networks (multicellular programs). We apply these emerging techniques to understand how highly pleiotropic proteins - such as an immune-related protein called Stimulator of Interferon Genes (STING) - switches from a tumor-suppressor to pro-tumoral function during the evolution of cancer metastasis. "},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Christopher Burge","Title":"Interpretable models to understand regulation of RNA splicing","Abstract":"We are developing fully interpretable models of RNA splicing and its regulation for improved understanding and various applications. We recently described a model called SMsplice that predicts the splicing patterns of primary transcripts in a variety of animal and plant species, using just core splice site motifs, exon and intron length preferences, and learned scores for splicing regulatory elements (SREs) that act locally on splice sites. This model enables automatic learning of candidate SREs from any organism, and achieves accuracy of 83-86% in fish, insects, and plants and about 70% in mammals. A new direction is the inference of the splicing regulatory activity of a splicing factor from just knockdown\/RNA-seq data and a model of its intrinsic binding preferences such as an RNA Bind-n-Seq, RNACompete or SELEX motif, but without using crosslinking data. Application to data from the ENCODE RNA-binding protein dataset and other data yields models that are reproducible across cell lines and species, and which can distinguish direct from indirect regulatory targets and can be used to infer cooperative splicing regulation."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Ruian Shi","Title":"IsoCLR: Contrastive learning for RNA foundation models","Abstract":"In the face of rapidly accumulating genomic data, our understanding of the RNA regulatory code remains incomplete. Recent self-supervised methods in other domains have demonstrated the ability to learn rules underlying the data-generating process such as sentence structure in language. Inspired by this, we extend contrastive learning techniques to genomic data by utilizing functional similarities between sequences generated through alternative splicing and gene duplication. We introduce IsoCLR, a model trained on a novel dataset with a contrastive objective enabling the learning of generalized RNA isoform representations. We validate representation utility on downstream tasks such as RNA half-life and mean ribosome load prediction. Our pre-training strategy yields competitive results using linear probing across 6 tasks, along with up to a two-fold increase in Pearson correlation in low-data conditions. Importantly, our exploration of the learned latent space reveals that our contrastive objective yields semantically meaningful representations, underscoring its potential as a valuable initialization technique for RNA property prediction."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Uwe Ohler","Title":"Explaining Deep Neural Networks for the Prediction of Translation Initiation","Abstract":"Regulation of mRNA translation enables rapid and local control of gene expression. As rate-limiting step, translation initiation is primarily controlled by the 5â€™ untranslated region (5â€™UTR). In it, regulatory sequence elements including RNA structural motifs and upstream open reading frames (uORFs) dictate the efficiency of translation. A recent convolutional neural network model accurately quantifies the relationship between massively parallel synthetic 5â€™ UTRs and translation levels, but the underlying sequence determinants remain elusive. _x000D_\nTo uncover the input features most important for prediction, feature attribution methods compute importance scores for input features, thus allowing to explain prediction output with respect to its input. Hence, model interpretation can be applied as a tool to uncover functional sequence patterns and generate novel biological hypotheses. Applying model interpretation, we extract representations of regulatory logic, revealing a complex interplay of regulatory sequence elements. Guided by insights from model interpretation, we adapt the model by human reporter data to obtain superior performance."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Can Cenik","Title":"Translational efficiency covariation across cell types is a conserved organizing principle of mammalian transcriptomes","Abstract":"Characterization of shared patterns of RNA expression between genes across conditions has led to the discovery of novel biological functions and regulatory networks. These RNA co-expression relationships have illuminated the higher-order organization of transcriptomes, yet we currently do not know if patterns of coordination in other gene expression modalities are similarly informative. In particular, translational covariation across cell types have remained unexplored, primarily due to the scarcity of comprehensive translational measurements across a large compendium of biological contexts. Here, we uniformly analyzed 2277 matched ribosome profiling and RNA-seq data from 90 human and 81 mouse tissues and cell lines. We introduce the concept of Translational Efficiency Covariation (TEC), identifying mRNAs that demonstrate coordinated translation patterns across cell types. We demonstrate that TEC is conserved across human and mouse cells and uncover novel gene functions that rely on translational covariation information alone. Moreover, our observations indicate that proteins exhibiting positive covariation at both translational and transcriptional levels are significantly more likely to physically interact. We finally discover TEC patterns indicative of RNA-binding protein (RBP) involvement, suggesting potential mechanisms of shared translational regulation. Our findings establish translational covariations across various conditions as a pervasive and conserved organizing principle of mammalian transcriptomes."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Yaron Orenstein","Title":"CellRBP: Improving Protein-RNA Binding Prediction In Vivo Using Cell-Type-Specific Features","Abstract":"RNA-binding proteins play important roles in various cellular processes. For this reason, researchers have developed experimental assays to measure proteinâ€“RNA binding in vivo. However, obtaining these measurements for every protein across various cell types is infeasible due to the high cost and long times of these experiments. Thus, researchers rely on computational methods to predict proteinâ€“RNA binding, but so far methods have been limited in their success in predicting RNA binding across cell types. In this work, we present CellRBP, a novel method to accurately and efficiently predict proteinâ€“RNA binding across cell types. CellRBP is based on a convolutional neural network that uniquely receives as input cell-type-specific information, such as experimentally measured RNA structure and RNA abundance, which enable the accurate generalization across cell types (Figure 1). We trained CellRBP on 196 of eCLIP experiments and evaluated prediction performance in both cross-validation and across cell types. CellRBP achieved superior performance compared to the state of the art achieving an average AUROC score of 0.889 in cross-validation and 0.772 across cell types, respectively (Figure 2A,B). We interrogated the trained models for the important features they learned using both local and glocal interpretability techniques and discovered known and novel RNA-binding preferences (Figure 2C). CellRBP is expected to help many researchers in predicting proteinâ€“RNA binding over various cell types and conditions. CellRBP is available via https:\/\/github.com\/OrensteinLab\/CellRBP."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Kaitlin Laverty","Title":"Reconstructing the sequence specificities of RNA-binding proteins across eukaryotes","Abstract":"RNA-binding proteins (RBPs) are key regulators of gene expression. Here, we introduce RBPzoo â€” a resource of RNAcompete-derived in vitro RNA-binding data for 379 RBPs from 33 diverse eukaryotes. We develop a new method, Joint Protein-Ligand Embedding (JPLE), to map specificity-determining peptides to corresponding RNA motifs for 28,667 RBPs from 690 eukaryotes. We illustrate the broad utility of this resource by inferring post-transcriptional function for 12 eukaryotic RBPs in mRNA stability and reconstructing the evolution of 2,568 RNA motifs. For the latter, we identify a universal set of 19 RNA motifs conserved between plants and metazoa and observe rapid motif evolution arising from whole genome duplications in vertebrate ancestors. RBPzoo represents a powerful resource for the study of gene regulation for any organism with an annotated genome."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"15:00-15:10","Format":"In Person","Speaker":"Shaimae Elhajjajy","Title":"A novel NLP-based RBP binding motif and context discovery method using multiple-instance learning","Abstract":"RNA-binding proteins (RBPs) are the primary mediators of mRNA regulation, dynamically governing complex processes such as splicing, cleavage, and degradation. Previous studies have shown that structurally diverse RBPs recognize similar motifs but can still bind distinct sites within the transcriptome. While in vitro evidence suggests that motif context plays an important role in RBP binding specificity, the precise underlying mechanisms remain unclear. Despite recent advances in machine learning models to predict RBP binding, current methods are often difficult to interpret and do not categorically investigate motif contexts. Thus, there remains a need for interpretable predictive models to disambiguate the contextual determinants of RBP binding specificity. Here, we present, to the best of our knowledge, the first formulation of the RBP binding prediction task as an NLP-based multiple-instance learning problem. We introduce a novel sequence decomposition strategy to generate entities termed â€œcontextsâ€, which we use to train and test our deep learning models. We also develop a deterministic motif discovery algorithm that is fast, accurate, and specialized to handle our data structure, recapitulating the motifs of well-characterized RBPs as validation. Importantly, we discover and characterize the in vivo sequence binding contexts for a collection of RBPs. Finally, by integrating motif and context similarity measures with a cross-prediction approach, we propose novel RBP-RBP interaction partners and hypothesize whether these interactions are cooperative or competitive. In summary, we present a comprehensive computational strategy for illuminating contextual determinants of specific RBP binding and demonstrate the implications of our findings in delineating RBP function."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"15:10-15:30","Format":"In Person","Speaker":"Kristina Sungeun Song","Title":"snoFlake: Discovery of a snoRNA-guided splicing regulatory complex via the snoRNA-RBP interactome","Abstract":"Box C\/D small nucleolar RNAs (snoRNAs) are noncoding RNAs crucial for guiding 2â€™-O-ribose methylation in ribosomal RNA during ribosome biogenesis, primarily through the formation of ribonucleoprotein (snoRNP) complexes with core RNA-binding proteins (RBPs). Additional roles were proposed for box C\/D snoRNAs, including the regulation of alternative splicing of protein-coding transcripts, yet few validated examples exist, with unclear mechanisms. Some noncanonical functions are thought to involve interactions with additional RBPs beyond the core snoRNA binders, indicating diverse regulatory roles of snoRNAs by interacting with various RBPs, collectively modulating protein-coding target RNAs. To explore these interactions and their functional implications, we introduce snoFlake, an interaction network of 191 box C\/D snoRNAs and 166 human RBPs, showing direct binding interactions and significant overlap of binding sites on shared protein-coding target RNAs, reinforcing their concerted role in gene regulation. Focusing on snoRNAs targeting groups of functionally-related targets, also bound by snoRNA-associated RBPs led to a hub region composed of SNORD22 and U2 and U5-associated splicing factors: SF3B4, PRPF8, EFTUD2 and AQR. SNORD22, PRPF8 and AQR exhibited an enrichment of overlapping binding sites at both 5â€™ and 3â€™ splice sites with the highest number of shared protein-coding target RNAs, suggesting their involvement in a splicing regulatory model. Knockdown experiments and differential alternative splicing analysis further highlighted the potential role of the SNORD22 complex in splicing, marking the first snoRNP splicing regulatory complex. This reshapes the understanding of snoRNA biology, emphasizing snoFlake's potential as a foundation for unravelling the impact of snoRNA-RBP interactions in gene regulation."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"15:30-15:40","Format":"In Person","Speaker":"Ruiyan Hou","Title":"scTail: precise polyadenylation site detection and its alternative usage analysis from reads 1 preserved 3' scRNA-seq data","Abstract":"Three-prime single-cell RNA-seq (scRNA-seq) has been widely employed to profile cellular transcriptomes, however, its power of analysing polyadenylation sites (PAS) has not been fully utilised. Here, we present a computational method, scTail, to precisely identify PAS by using reads 1 and quantify its expression by leveraging the reads 2, which enables effective detection of alternative PAS usage. When compared with other methods, PAS detected by scTail are more accurate. With various experimental data sets, we have demonstrated that scTail can accurately identify PAS and the detected alternative PAS usages showed strong specificity in different biological processes."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Farica Zhuang","Title":"G4mer: Transcriptome-wide prediction of RNA G-quadruplexes with a deep RNA language model","Abstract":"RNA G-quadruplexes (rG4) are RNA secondary structures known to play an important role in gene regulation. Despite their importance, the effects of genetic variants on rG4 formations and functions remain unexplored. To address this challenge, we introduce G4mer, a deep learning model that predicts transcriptome-wide rG4 formations using high throughput RT-stop experimental data, rG4-seeker. While computational methods have been developed to predict whether rG4s are likely to form on a given sequence, we show that G4mer outperforms other state-of-the-art models, especially for non-canonical rG4 that do not confer to the consensus GGG-{N-1:7}(3)-GGG motif. Additionally, G4mer offers a computational approach to study the effect of variants on rG4 formation and the association of these variants with diseases. With G4mer, we map variants in the 5â€™ and 3â€™ untranslated regions that are predicted to alter rG4 formations. Then using the Penn Medicine BioBank, we identify those associated with diseases such as breast cancer. By carefully interpreting the learned G4mer model, we identify rG4 length as a significant factor that deviates between experimental data and human rG4s. Finally, we validate the effect of disease-associated rG4-altering variants on protein expression using dual luciferase assay, and assess the effect of variants on structure formation using Circular Dichroism and RT-stop assays. These experiments point to a potential interplay between structure and sequence motifs affecting downstream gene translation. Overall, our work offers a compelling framework for detecting and validating the functional effects of rG4-altering variants that are significantly associated with diseases."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:20","Format":"In Person","Speaker":"Jérôme Waldispühl","Title":"Fast and accurate RNA virtual screening using non-canonical RNA base pair interaction networks and graph machine learning","Abstract":"RNAs constitute a vast reservoir of mostly untapped drug targets. Structure-based virtual screening (VS) methods are key to massively screen molecular targets and identify promising candidate molecules binding. However, this strategy does not scale well with the size the small molecule databases and the number of potential RNA targets. Furthermore, this approach is also hampered by the scarcity of RNA 3D structural data._x000D_\nIn this talk, we show that using an augmented classification of RNA base pairs combined with graph machine learning methods enable us to design a new class of algorithms for screening RNAs and promising molecular compounds. We describe a data-driven VS pipeline that deals with the unique challenges of RNA molecules through coarse grained modeling of 3D structures and heterogeneous training regimes. We demonstrate strong prediction and generalizability of our framework and discuss further expansion of this platform._x000D_\n"},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Frederic Runge","Title":"Partial RNA Design","Abstract":"RNA design is a key technique to achieve new functionality in fields like synthetic biology or biotechnology. Computational tools could help to find such RNA sequences but they are often limited in their formulation of the search space. In this work, we propose partial RNA design, a novel RNA design paradigm that addresses the limitations of current RNA design formulations. Partial RNA design describes the problem of designing RNAs from arbitrary RNA sequences and structure motifs with different design goals. By separating the design space from the objectives, our formulation enables the design of RNAs with variable lengths and desired properties, while still allowing precise control over sequence and structure constraints at individual positions. Based on this formulation, we introduce a new algorithm, libLEARNA, capable of efficiently solving different constraint RNA design tasks. A comprehensive analysis of various problems, including a realistic riboswitch design task, reveals the outstanding performance of libLEARNA and its robustness."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"17:40-17:50","Format":"In Person","Speaker":"J. White Bear","Title":"High resolution deconvolution of RNA secondary structure via long read nanopore technology","Abstract":"RNA are known to be highly flexible and take on multiple conformations to perform various tasks and binding in vivo. This makes structural analysis more challenging than with larger, lower entropy molecules.  Structure probing with chemical reagents has been a key tool for developing deeper understandings of secondary RNA structure. Traditional probing methods use an averaged mutational profile to detect modifications and infer secondary structural features using reverse transcription. However, this requires shortened segment lengths which can obfuscate key structural information. Moreover, mutational profiles do not express alternative conformations or indicate optimality and conserved features that may be equally viable or relevant to function. Indeed, the averaged profile may be suboptimal. In our study, we use long read nanopore technology to directly sequence RNA, with the reagent acetlyimadizole (AcIm). Our software, Dashing Turtle (DT), can identify AcIm modifications at high resolution, inferring secondary structural features, and determining diverse conformations. DT applies a unique deconvolution of large RNA samples and examines both conservation and dominance across the sample, potentially yielding optimal conformations. Identifying dominant conformations may further lead to a better understanding of key RNA hybridization strategies that can only be observed in transitional interactions and co-modifications. Additionally, DT can identify conserved states across these conformations that may have key functional implications. Furthermore, our method has the potential for application to time or phase-based strategies that can help us understand intermediate structures that play key roles in binding or other in vivo activities for both drug delivery and pathogenesis."},{"Track":"iRNA","Room":"519","Weekday":"Sunday","Date":"14 July","Timespan":"17:50-18:00","Format":"In Person","Speaker":null,"Title":"Conclusion and awards","Abstract":null},{"Track":"Distinguished Keynotes","Room":"-","Weekday":"Friday","Date":"12 July","Timespan":"18:30-19:30","Format":"In Person","Speaker":null,"Title":"Sensitive Sustainable Science","Abstract":"How do we sustainably maintain and further develop bioinformatics and computational biology (BCB) software, databases and tools, in the face of short <5 year periods of funding support? How do we promote open data and open science in a way that best effects positive change and avoids causing unwitting harm on communities? Using some historical data and also my recent research as examples, Iâ€™ll review how open science is evolving, building on FAIR (findable, accessible, interoperable, reusable) with also, for example, CARE (Collective Benefit, Authority to Control, Responsibility, Ethics) as Principles for Indigenous Data Governance. Iâ€™ll review this and other principles in the context of both microbial data, as well as human cohort data, presenting some approaches to research that can support more sustainable, inclusive science that can potentially better lead to positive change. While there is no one size fits all solution, there are some common themes and considerations that we as a BCB community should discuss - and ideally incorporate into BCB training programs."},{"Track":"Distinguished Keynotes","Room":"-","Weekday":"Saturday","Date":"13 July","Timespan":"09:00-10:00","Format":"In Person","Speaker":null,"Title":"Progress in Large-Scale Phylogenomic Estimation Methods","Abstract":"Over the last several years, interest in computing and then using large-scale phylogenies has increased for multiple reasons, including basic science (how did life evolve on earth) and applications in biomedicine and public health (e.g., understanding the evolution of SARS-Cov-2). The estimation of these large phylogenies, wiith potentially millions of leaves, presents fascinating mathmetical, statistical, and computational challenges, ranging from computing multiple-sequence alignments, developing effective heuristics to NP-hard optimization problems (e.g., maximum likelihood tree estimation) on large datasets), estimating species trees from genome- scale data while addressing biological causes for heterogeneity (e.g., gene duplication and loss and incomplete lineage sorting) across the genome). There are also many fascinating and difficult problems that have to do with â€œpost-treeâ€ analyses, such as rooting gene trees and species trees, or estimating branch lengths in species trees and dates at internal nodes, that are needed for many down-stream analyses. In this talk I will describe progress on these questions, and I will also present some open problems where new techniques are needed."},{"Track":"Distinguished Keynotes","Room":"-","Weekday":"Sunday","Date":"14 July","Timespan":"09:00-10:00","Format":"In Person","Speaker":null,"Title":"Human genome 2.0 : why a pangenome graph is better for genetic and epigenetic analyses","Abstract":"Genomic analyses often start by mapping reads to a reference genome. But, in every individual, there are DNA variants and sequences that are unique to that individual and reads coming from those regions will often be ignored. Thankfully, progress in long-read technologies and assembly can now efficiently deliver telomere-to-telomere genomes. Applying such approaches to a diverse panel of individuals combined with the development of graph-based genomic tools, the Human Pangenome Reference Consortium has just released the first human pangenome reference graph. This new resource is meant to alleviate the limitations of relying on a single linear human genome as the first step of most genetic and epigenetic analyses. In this talk, I will summarize some of the benefits of using the pangenome reference. In particular, I will show how this new reference can be used to extract missing signal when looking for genetic variants in a rare disease cohort called Genomic Answers for Kids. I will also describe the results of a new study using a genome-graph looking at epigenetic changes before and after influenza infection in monocyte-derived macrophages extracted from more than 30 individuals of different ancestry. Finally, considering the importance of data sharing in genomics, I will introduce a project called the Pan-Canadian Genome Library, which will establish the framework for Canadaâ€™s management and sharing of human genomic data."},{"Track":"Distinguished Keynotes","Room":"-","Weekday":"Monday","Date":"15 July","Timespan":"09:00-10:00","Format":"In Person","Speaker":null,"Title":"Supercharged Protein Analysis in the Era of Accurate Structure Prediction","Abstract":"Abstract: Protein analysis has witnessed a revolution through machine-learning methods. At the forefront are highly accurate structure prediction methods such as AlphaFold2 and ESMFold. These have generated an avalanche of publicly available protein structures. The AlphaFold database and ESMatlas contain over 214 and 620 million predicted structures, respectively, covering nearly every protein sequence in our largest protein reference databases. This unprecedented access to structural information is not just critical for structural biology but impacts most fields of biology. In this talk, I will discuss how this data is revolutionizing genomic and proteomic annotations and introduce fast and sensitive methods to search and cluster this data to extract new biological insights."},{"Track":"Distinguished Keynotes","Room":"-","Weekday":"Tuesday","Date":"16 July","Timespan":"16:00-17:00","Format":"In Person","Speaker":null,"Title":"Explainable AI for health: where we are and how to move forward","Abstract":"The first part of my talk delves into various research endeavors conducted by my lab, focusing on explainable AI's application across diverse biomedical domains. I will demonstrate how explainable AI can elucidate novel scientific inquiries, with a primary emphasis on understanding neurodegenerative diseases and biological age._x000D_\n_x000D_\nIn the second part, we will explore the evolving landscape of explainable AI, uncovering its potential to chart new scientific directions in biomedicine, exemplified by our recent work in dermatology, emergency medicine, and precision cancer medicine. This discussion aims to shed light on the necessary enhancements for explainable AI to effectively tackle a wide array of real-world challenges in biomedicine."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-11:25","Format":"In Person","Speaker":"Christopher Quince","Title":"Towards fully genome-resolved metagenomics","Abstract":"I will discuss the impact of long accurate sequencing reads generated by HiFi PacBio on the assembly of microbial genomes directly from metagenomes. I will present our recent assembler metaMDBG based on minimiser de Bruijn graphs for this application. I will also talk about the prospects for the use of the latest ONT reads which are approaching HiFi levels of accuracy. I will conclude with a discussion of Hi-C proximity ligation for metagenome binning and the linking of extra-chromosomal elements to genomes. "},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"11:25-11:40","Format":"In Person","Speaker":"Shanfeng Zhu","Title":"Effective binning of metagenomic contigs using contrastive multi-view representation learning","Abstract":"Contig binning plays a crucial role in metagenomic data analysis by grouping contigs from the same or closely related genomes. However, existing binning methods face challenges in practical applications due to the diversity of data types and the difficulties in efficiently integrating heterogeneous information. Here, we introduce COMEBin, a binning method based on contrastive multi-view representation learning. COMEBin utilizes data augmentation to generate multiple fragments (views) of each contig and obtains high-quality embeddings of heterogeneous features (sequence coverage and k-mer distribution) through contrastive learning. Experimental results on multiple simulated and real datasets demonstrate that COMEBin outperforms state-of-the-art binning methods, particularly in recovering near-complete genomes from real environmental samples. COMEBin outperforms other binning methods remarkably when integrated into metagenomic analysis pipelines, including the recovery of potentially pathogenic antibiotic-resistant bacteria (PARB) and moderate or higher quality bins containing potential biosynthetic gene clusters (BGCs)."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Jim Shaw","Title":"Floria: Fast and accurate strain haplotyping in metagenomes","Abstract":"Shotgun metagenomics allows for direct analysis of microbial community genetics, but scalable computational methods for the recovery of bacterial strain genomes from microbiomes remains a key challenge. We introduce Floria, a novel method designed for rapid and accurate recovery of strain haplotypes from short and long-read metagenome sequencing data, based on minimum error correction (MEC) read clustering and a strain-preserving network flow model. Floria can function as a standalone haplotyping method, outputting alleles and reads that co-occur on the same strain, as well as an end-to-end read-to-assembly pipeline (Floria-PL) for strain-level assembly. Benchmarking evaluations on synthetic metagenomes showed that Floria is > 3x faster and recovers 21% more strain content than base-level assembly methods (Strainberry), while being over an order of magnitude faster when only phasing is required. Applying Floria to a set of 109 deeply sequenced nanopore metagenomes took < 20 minutes on average per sample, and identified several species that have consistent strain heterogeneity. Applying Floriaâ€™s short-read haplotyping to a longitudinal gut metagenomics dataset revealed a dynamic multi-strain Anaerostipes hadrus community with frequent strain loss and emergence events over 636 days. With Floria, accurate haplotyping of metagenomic datasets takes mere minutes on standard workstations, paving the way for extensive strain-level metagenomic analyses."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Mihai Pop","Title":"The impact of transitive annotation on the training of taxonomic classifiers","Abstract":"A common task in the analysis of microbial communities involves assigning taxonomic labels to the sequences derived from organisms found in the communities. Frequently, such labels are assigned using machine learning algorithms that are trained to recognize individual taxonomic groups based on training data sets that comprise sequences with known taxonomic labels. Ideally, the training data should rely on labels that are experimentally verifiedâ€”formal taxonomic labels require knowledge of physical and biochemical properties of organisms that cannot be directly inferred from sequence alone. However, the labels associated with sequences in biological databases are most commonly computational predictions which themselves may rely on computationally-generated dataâ€”a process commonly referred to as â€œtransitive annotationâ€. Here, we focus on taxonomic annotation using a naÃ¯ve Bayes classifier developed for the annotation of 16S rRNA gene sequencesâ€”the Ribosomal Database Project (RDP) classifier. We chose this data set and classifier since they are established resources in microbial ecology, however, our general methodology and conclusions apply more broadly to any sequence-based machine-learning classifier. We demonstrate that even a few computationally-generated training data points can significantly skew the output of the classifier to the point where entire regions of the taxonomic space can be disturbed. To exemplify we note that retraining the classifier with artificial sequences caused changes that spanned microbial phyla. We also discuss key factors that affect the resilience of classifiers to transitively-annotated training data, and propose best practices to avoid the artifacts of transitive annotation."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Patrick Bradley","Title":"Sensitive, specific association of microbial functions with host phenotypes using Phylogenize2","Abstract":"In metagenomics, a key challenge is to explain differences in microbial communities in terms of gene function. Many common approaches to this problem do not account for the fact that related species tend to share both genes and phenotypes. This makes them susceptible to discovering clade markers for differentially-abundant microbes, which often have weak evidence for functional relevance. We have developed a new major version, Phylogenize2, of a tool that allows researchers to link community-level changes to gene content while accounting for phylogeny. This revision leverages large, modern collections of isolate and metagenome-assembled genomes, allowing the method to be applied across a range of biomes. We have also substantially improved Phylogenize2's statistical power by combining new, microbiome-specific differential abundance methods with adaptive shrinkage. As a test case, we apply Phylogenize2 to a human cohort with liver cirrhosis, and discover a link between abundances of the Lachnospiraceae (a prevalent group of commensal Clostridia) and anaerobic oxidative stress. Our preliminary results suggest that Phylogenize2 is both more specific than linear modeling and more sensitive than competing methods. Phylogenize2 is a publicly available, open source tool that can extract specific functional information from a wide variety of environmental and host-associated microbiomes."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Kristen Curry","Title":"Reference-free Structural Variant Detection in Microbiomes via Long-read Co-assembly Graphs","Abstract":"Bacterial genome dynamics are vital for understanding the mechanisms underlying microbial adaptation, growth, and their broader impact on host phenotype. Structural variants (SVs), genomic alterations of 50 base pairs or more, play a pivotal role in driving evolutionary processes and maintaining genomic heterogeneity within bacterial populations. While SV detection in isolate genomes is relatively straightforward, metagenomes present broader challenges due to the absence of clear reference genomes and the presence of mixed strains. In response, our proposed method rhea, forgoes reference genomes and metagenome-assembled genomes (MAGs) by encompassing a single metagenome co-assembly graph constructed from all samples in a series. The log fold change in graph coverage between subsequent samples is then calculated to call SVs that are thriving or declining throughout the series. We show rhea to outperform existing methods for SV and horizontal gene transfer (HGT) detection in two simulated mock metagenomes, which is particularly noticeable as the simulated reads diverge from reference genomes and an increase in strain diversity is incorporated. We additionally demonstrate use cases for rhea on series metagenomic data of environmental and fermented food microbiomes to detect specific sequence alterations between successive time and temperature samples, suggesting host advantage. Our approach leverages previous work in assembly graph structural and coverage patterns to provide versatility in studying SVs across diverse and poorly characterized microbial communities for more comprehensive insights into microbial gene flux."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-14:55","Format":"In Person","Speaker":"Paweł P. Łabaj","Title":"Targeted Sequencing and Triplet Loss classification allow for microbiome-based inference of soil sample origin","Abstract":"Microbiome characterization has been successfully applied in forensic studies. However, from MetaSUB Consortium and CAMDA we know that the full forensic potential of environmental metagenomic data is yet to be unveiled. Thus the aim of our SMAFT project was to develop a complete (wet-lab + dry-lab) solution for forensic laboratories in Poland._x000D_\nBased on earlier gained experience we first have analyzed climate and geographical properties of Poland to select 80 locations. From those the samples have been collected throughout four seasons in triplicates resulting in almost 1000 total. They were then profiled with WMS with about 120 million read-pairs per sample. Which were then fed (and MetaSUB ones as non-Polish negative reference) to MetaGraph to obtain the set of metagenomic features (unitigs) to be used to distinguish between respective locations._x000D_\nWe have ultimately identify 1015 All Relevant Features. which in wet-lab part were used for designing probes for Targeted Metagenomics Sequencing panel, while in dry-lab part were source of data for classification\/prediction model._x000D_\nFor sample origin prediction the Triplet Loss based solution was used to reduce the dimensionality of the metagenomic profiles and then Deep Neural Network to obtain probabilities of the origin of the sample. The overall performance was: LRAP: 0.87; Weighted F1: 0.86; Balanced Accuracy: 0.94 on all validation samples (n=547)._x000D_\nOur work has confirmed that targeted microbiome sequencing of soil sample together with appropriate data science and AI processing allows for accurate sample origin classification even in such climate-homogeneous country as Poland."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"14:55-15:10","Format":"In Person","Speaker":"Samantha Graham","Title":"Integration and analysis of 168,000 human gut microbiome samples","Abstract":"The human microbiome, or the collection of microorganisms within the human body, plays an important role in modulating human health and disease. While some health-relevant patterns may only be discoverable with thousands of samples, the average gut microbiome study contains fewer than 100 samples; fortunately, hundreds of thousands of samples are available via public repositories such as NCBI SRA. Here, we leverage this publicly available data by uniformly processing and integrating 168,464 16S rRNA gene sequencing human gut microbiome samples. This resource, the Human Microbiome Compendium, is freely available via our website (MicroBioMap.org) and R package (MicroBioMap). We use this dataset to characterize global patterns of variation in microbiome composition. _x000D_\nWe classified our samples into eight world regions to investigate patterns of variation, and found distinct differences in microbiome composition between regions. We sought to understand specific microbes driving these differences and found that the 65 most abundant genera were differentially abundant between at least one pair of regions. We also note the disparity in sampling between world regions. Over 90,000 of the samples originate from Europe and Northern America, while we identified only 4 samples from Oceania. We show that some regions likely have many taxa that remain undiscovered due to undersampling. _x000D_\nHere, we present a new, large-scale collection of human gut microbiome data, which we use to study global microbiome variation. We expect this compendium will be a valuable resource for the community and enable novel insights into the microbial ecology of the human gut."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"15:10-15:30","Format":"In Person","Speaker":"Ananthan Nambiar","Title":"MC-Funcformer: A foundational model of microbial community metabolism","Abstract":"Microbial communities are remarkably complex and encompass an incredible diversity of bacteria, archaea, fungi, and viruses. The metabolic functions of individual community members are at the core of this complexity. Despite the immense taxonomic diversity of microorganisms, there is a notable degree of functional redundancy and universality across taxa. Here, we leverage the recent advances in large language models (LLMs) and a large database of microbial community profiles across a wide range of environments to learn the principles of functional universality. In particular, we introduce MC-Funcformer, a novel approach that pretrains a language model on information extracted from microbial functional abundance data. By using functional information in lieu of taxonomic information, we are able to represent microbial community profiles derived from diverse environments. Our findings highlight the utility of MC-Funcformer in predicting various metadata associated with microbial communities, including host phenotypes and environmental properties. Notably, the embeddings generated by MC-Funcformer outperform traditional approaches based solely on functional abundance vectors, improving predictions of host diseases, diet, and environmental parameters. Furthermore, our analysis reveals the universal nature of the embeddings, enabling generalization across different microbiomes and prediction tasks."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"15:10-15:30","Format":"In Person","Speaker":"Serghei Mangul","Title":"Assessing Microbial Genome Representation Across Various Reference Databases: A Comprehensive Evaluation","Abstract":"Metagenomics research can provide significant insights into the composition, diversity and functions of mixed microbial communities found in various environments. To identify bacterial species, reads from samples are mapped to references that are found in bacterial reference databases. Multiple references may be assigned the same taxonomic identifiers yet these references may contain different genomic information. This project was designed to uncover and correct inconsistencies in bacterial reference databases by comparing species names and genomic representation for the two most commonly used bacterial reference databases PATRIC and Refseq. NCBIâ€™s taxonomic identifiers were utilized to assess the agreement of reference databases at the species rank. Same species across two databases were identified by finding the same taxID in two databases. Comparison of genomic representation across databases was performed using the BLAST tool. After finding the exact same strain, all the contigs from one database were aligned to all contigs from another. This analysis was extended to all overlapping species where strain information was available. The results of the study revealed substantial discrepancies across the databases in the presence of bacterial species. 12.4% of species are present in all three databases, 29.49% are found only in two databases, and 58.46% are found only in one database. To compare genomic representation, we visualized gathered data on all observed alignment cases showing that quality of reference genomes can be improved through consolidation of contigs.  This evaluation is a fundamental step towards creating a comprehensive reference database that will substantially improve the accuracy of metagenomics research."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"15:30-16:00","Format":"Live Stream","Speaker":"Victor Gordeev","Title":"A rigorous benchmarking of methods for SARS-CoV-2 lineage abundance estimation in wastewater","Abstract":"Wastewater genomic surveillance of SARS-CoV-2 has emerged as a scalable, cost-effective, passive surveillance tool to monitor viral variants circulating in the human population. However, accurate estimation of viral lineage prevalence in communities relies on the performance of computational methods for analyzing wastewater sequencing data. We perform a comprehensive benchmarking of bioinformatics methods designed for estimating the relative abundance of SARS-CoV-2 (sub)lineages from wastewater sequencing data, along with RNA-Seq and metagenomics methods repurposed for this task. We systematically compare the accuracy of these computational methods in estimating the relative abundances of the (sub)lineages present in a sample, including closely related and low-abundance (sub)lineages. Our preliminary results on simulated data and a few computational methods show that RNA-Seq methods RSEM (most accurate), Kallisto, and Salmon consistently achieve lower L1 errors for lineages and particularly for sublineages when compared to wastewater-surveillance methods Alcov and PiGx. In particular, while the distribution of absolute errors for lineages is similar, for sublineages, roughly 80% of the absolute error values for RSEM, Kallisto, and Salmon are lower than 0.04%, compared to roughly 75% for Alcov and only 25% for PiGx. In addition to extensive simulated data, we will use in vitro mixtures of (sub)lineages of various complexity prepared from synthetic RNA genomes or inactivated viral particles and sequenced using short-read and long-read technologies. Using different experimental strategies, we will also investigate how the performance of these computational methods is impacted by the wastewater matrix or wastewater nucleic acid background, but also by the design of the sequencing experiment. Our study will inform the selection of the most accurate, robust, and sensitive methods for SARS-CoV-2 lineage prevalence estimation to enable effective wastewater-based genomic surveillance."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"15:30-16:00","Format":"Live Stream","Speaker":"Nitesh Kumar Sharma","Title":"MetaViz: Realistic assortment of novel metagenomics benchmarks with diverse biological and technological characteristics","Abstract":"Metagenomics research relies heavily on bioinformatics methods for analyzing complex microbial communities, necessitating rigorous validation through benchmarking. However, creating high-quality experimental benchmarks can be costly and challenging. Current benchmarking efforts often rely on limited gold-standard samples or synthetic data, hindering comprehensive evaluations. To address this, we propose MetaViz, a tool for generating semi-real novel metagenomics benchmarks through in silico modification of existing experimental data. MetaViz offers a cost-effective alternative, combining elements of real data with simulated modifications, surpassing the limitations of purely simulated datasets. Our tool allows precise control over sample composition, diversity, and technological characteristics, enhancing benchmarking accuracy and applicability. We applied MetaViz to over 27 real metagenomics benchmarks, including in-vitro viral mock communities and intra-host clinical samples. Our tool allowed us to precisely control the composition and the abundance of  microbial genomes in the in-vitro mixtures (mock community). We were also able to adjust their relative abundance with varying frequency ranging from 0.1% to 10%. Leveraging reference mapping, we introduced varying errors within the read data, thereby enhancing reliability. Our method introduces a novel approach to benchmarking in metagenomics, particularly valuable where traditional gold-standard creation is impractical. By capturing the complexity of actual datasets, MetaViz produces semi-real benchmarks that encompass a broader range of clinical and technological characteristics, ultimately enhancing benchmarking comprehensiveness. Adoption of our approach promises to significantly improve benchmarking studies' robustness and accuracy, advancing our understanding of microbial communities across diverse biological contexts."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"15:30-16:00","Format":"In Person","Speaker":"Jonas Grove","Title":"Phage Host Prediction Using Novel Global-Scale Phage-Host Interaction Atlas and Genomic Language Models","Abstract":"Viruses, including bacteriophages and archaeal viruses, are the most abundant form of life on earth (1031), interacting with all life and shaping the global ecosystem. However, phage-host relationships have proven challenging to identify without culture-based experiments to generate unambiguous evidence for a phageâ€™s presence in a given host. These experiments inherently require that all hosts are culturable, restricting the microbial diversity that can be surveyed._x000D_\n _x000D_\nProximity ligation sequencing is a powerful metagenomic method for associating viruses with their hosts directly in native microbial communities. Proximity ligation captures, in vivo, physical interactions between the host microbial genome and the genetic material of both lytic and lysogenic phages. These linkages offer direct evidence that phage sequences were present within an intact host cell, establishing a phage-host pair without the propagation of living bacterial cells. The combination of intra-phage and phage-host signal enables us to simultaneously deconvolve viral and microbial genomes directly from metagenomes, and to assign microbial hosts to large numbers of viruses without culturing._x000D_\n _x000D_\nOur application of this technology to thousands of complex microbiome samples has yielded host assignments for hundreds of thousands of novel phage and archaeal viruses. Utilizing our expanded phage-host interaction training data, and leveraging advancements made in the field of natural language processing (NLP) and genomic large language models (LLMs), we have developed deep learning networks that model the dynamics between phages and microbial hosts at sequence-level resolution. We will report published and unpublished work highlighting the power of this approach in the field of metagenomic discovery."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Yanni Sun","Title":"Towards more accurate microbial source tracking via non-negative matrix factorization (NMF)","Abstract":"Motivation: The microbiome of a sampled habitat often consists of microbial communities from various sources, including potential contaminants. Microbial source tracking (MST) can be used to discern the contribution of each source to the observed microbiome data, thus enabling the identification and tracking of microbial communities within a sample. Therefore, MST has various applications, from monitoring microbial contamination in clinical labs to tracing the source of pollution in environmental samples. Despite promising results in MST development, there is still room for improvement, particularly for applications where precise quantification of each sourceâ€™s contribution is critical._x000D_\nResults: In this study, we introduce a novel tool called SourceID-NMF towards more precise microbial source tracking. SourceID-NMF utilizes a non-negative matrix factorization (NMF) algorithm to trace the microbial sources contributing to a target sample, without assuming specific probability distributions. By leveraging the taxa abundance in both available sources and the target sample, SourceID-NMF estimates the proportion of available sources present in the target sample. To evaluate the performance of SourceID-NMF, we conducted a series of benchmarking experiments using simulated and real data. The simulated experiments mimic realistic yet challenging scenarios for identifying highly similar sources, irrelevant sources, unknown sources, low abundance sources, and noise sources. The results demonstrate the superior accuracy of SourceID-NMF over existing methods. Particularly, SourceID-NMF accurately estimated the proportion of irrelevant and unknown sources while other tools either over- or under-estimated them. Additionally, the noise sources experiment also demonstrated the robustness of SourceID-NMF for MST."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:15","Format":"In Person","Speaker":"Yanbin Yin","Title":"Carbohydrate-active enzyme annotation in microbiomes using dbCAN3","Abstract":"Carbohydrate active enzymes (CAZymes) are made by various organisms for complex carbohydrate metabolism. Genome mining of CAZymes has become a routine data analysis in microbiome sequencing projects, owing to the importance of CAZymes in bioenergy, microbiome, nutrition, agriculture, and global carbon recycling. In 2012, dbCAN was provided as an online web server for automated CAZyme annotation. dbCAN2 (https:\/\/bcb.unl.edu\/dbCAN2) was further developed in 2018 as a meta server to combine multiple tools for improved CAZyme annotation. dbCAN2 also included CGC-Finder, a tool for identifying CAZyme gene clusters (CGCs) in (meta-)genomes. We have updated the meta server to dbCAN3 with the following new functions and components: (i) dbCAN-sub as a profile Hidden Markov Model database (HMMdb) for substrate prediction at the CAZyme subfamily level; (ii) searching against experimentally characterized polysaccharide utilization loci (PULs) with known glycan substates of the dbCAN-PUL database for substrate prediction at the CGC level; (iii) a majority voting method to consider all CAZymes with substrate predicted from dbCAN-sub for substrate prediction at the CGC level; (iv) improved data browsing and visualization of substrate prediction results on the website. In summary, dbCAN3 not only inherits all the functions of dbCAN2, but also integrates three new methods for glycan substrate prediction in microbiome sequencing data. _x000D_\n_x000D_\nPublication: https:\/\/academic.oup.com\/nar\/article\/51\/W1\/W115\/7147496"},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"17:15-17:30","Format":"In Person","Speaker":"Adrian Martin-Segura","Title":"NUTRIclock, NEURAL NETWORKS ANALYSIS OF MICROBIOME FOR IMPLEMENTING PRECISSION NUTRITION IN AGING.","Abstract":"Aging is the greatest risk factor for the development of chronic diseases like neurodegenerative disorders or cancer. The increase in life expectancy makes extremely urgent to understand and develop mechanisms to promote healthy aging. In that sense, the study of microbiome has emerged as an important factor in aging processes. The gut microbiome comprises all gut microorganisms, and different scientific evidence points towards microbiome alterations as important events in disease onset. Moreover, different nutritional interventions have shown their efficiency to modulate human microbiome and physiology due to their versatility. However, their efficiency is highly dependent on the individualâ€™s genomic and metagenomic background. We propose using neural networks (NN) algorithms and ~3700 whole genome shotgun samples from public repositories, to elucidate microbiome changes with aging. Using samplesâ€™ microbial profiles (relative abundance and gene families), we are developing NUTRIclock, an aging clock based on microbiome. We are trying different NN architectures (convolutional, GraphNN) to find the one that better accommodates to microbiome particularities, encoding in it microbial and metabolic changes observed along aging. NUTRIclock will serve i) as a tool to calculate the biological age of a patient, in opposition to his\/her chronological age; ii) to find microbiome patterns altered in the patients that could be driving potential effects of that biological age. Thus, allowing to implement personalized nutritional interventions according to a patient microbiome profile, to influence on its dynamics, enhancing the development of precision nutrition and personalized medicine fields."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"17:30-17:40","Format":"In Person","Speaker":"Yifan Jiang","Title":"MIOSTONE: Modeling microbiome-trait associations with taxonomy-adaptive neural networks","Abstract":"The human microbiome, a complex ecosystem of microorganisms inhabiting the body, plays a critical role in human health. Investigating its association with host traits is essential for understanding its impact on various diseases. Although shotgun metagenomic sequencing technologies have produced vast amounts of microbiome data, analyzing such data is highly challenging due to its sparsity, noisiness, and high feature dimensionality. Here we propose MIOSTONE, a novel machine learning method that leverages the intercorrelation of microbiome features due to their phylogeny-based taxonomic relationships. MIOSTONE employs a novel taxonomy-encoded deep neural network (DNN) architecture that harnesses the capabilities of DNNs with mitigated concerns of overfitting. In addition, MIOSTONE has the ability to determine whether taxa within the corresponding taxonomic group provide a better explanation in a data-driven manner. We empirically assessed MIOSTONE's accuracy and interpretability on various real microbiome datasets, demonstrating its competitive performance and interpretability compared to existing methods."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Saturday","Date":"13 July","Timespan":"17:40-18:00","Format":"Live Stream","Speaker":"Alice McHardy","Title":"Critical Assessment of Metagenome Interpretation - Updates and Future Benchmarking Challenges","Abstract":null},{"Track":"MICROBIOME","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:25","Format":"In Person","Speaker":"Susannah Tringe","Title":"Sequence-based interrogation of soil microbiomes and their ecosystem benefits","Abstract":"Plants roots and the soil they grow in are heavily colonized with microbes that play critical roles in nutrient cycling and transport as well as influencing plant growth and health. Molecular methods including DNA sequencing have begun to elucidate the forces governing the assembly and maintenance of plant and soil microbial communities, offering the opportunity for these microbial communities to be nurtured and manipulated to promote plant growth and health as well as soil health and ecosystem functions.  _x000D_\nWe have combined omics methods, biogeochemical assays, and gas flux measurements to investigate the factors influencing greenhouse gas emissions from natural and managed wetland systems. By integrating these datasets we find that gas fluxes represent a complex interplay of biological, chemical, and physical factors that vary across habitats. Our results suggest considerable heterogeneity in fluxes even in physically proximate locations that have implications for the success of wetland preservation and restoration as a carbon storage strategy, particularly in the context of sea level rise. _x000D_\nIn agricultural systems, we find that different plant compartments (e.g. rhizosphere and root endosphere) harbor unique and dynamic microbial communities heavily influenced by the soil, surrounding environment and host genotype.  Abiotic stress, such as drought and low nitrogen, can alter both the composition of these communities and their interactions with each other and the plant. Our sequence-based characterizations of plant-associated communities, leveraging a variety of bioinformatic tools, have identified key populations that structure the community and respond dynamically to environmental changes, representing potential targets for improvement of plant resilience."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"11:25-11:40","Format":"In Person","Speaker":"Luis Pedro Coelho","Title":"Understanding the small proteins from the global microbiome","Abstract":"Small proteins, crucial across all life domains, have been overlooked in large-scale microbiome studies due to limitations in both wet lab and bioinformatics techniques. In particular, it is difficult to predict them without generating numerous false positives, and functional predictions based on homology fail without closely-related homologs. Recently, studies have begun addressing these challenges, with improved methods for managing small protein data in metagenomic analyses._x000D_\n_x000D_\nWe tackled this by analyzing sequences shared across multiple metagenomes, to increase confidence in predictions. This method was applied in creating the Global Microbial smORF Catalogue, which includes almost one billion sequences. This is accessible online for users to identify homologs to smORFs identified in their own studies._x000D_\n_x000D_\nAdditionally, we used machine learning to filter out false positives effectively, particularly in identifying active sequences within specific functional classes like antimicrobial peptides (AMPs). For this task, we designed macrel to optimize for high precision, albeit at the potential cost of lower recall. Macrel was used to generate a catalog of one million potential AMPs from extensive genomic and metagenomic data, a dataset we termed AMPsphere._x000D_\n_x000D_\nExperimental validation of these methods included synthesizing and testing 100 AMPs. In total, 79 showed activity against pathogens or commensals. Some peptides also demonstrated efficacy comparable to the clinical antimicrobial, polymyxin B, in a preclinical mouse model, underscoring the potential of these novel bioinformatic approaches to contribute significantly to discovering novel antibiotics."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"11:40-11:55","Format":"In Person","Speaker":"James Morton","Title":"Multi-level analysis of the gutâ€“brain axis shows autism spectrum disorder-associated molecular and microbial profiles","Abstract":"Autism spectrum disorder (ASD) is a neurodevelopmental disorder characterized by heterogeneous cognitive, behavioral and communication impairments. Disruption of the gutâ€“brain axis (GBA) has been implicated in ASD although with limited reproducibility across studies. In this study, we developed a Bayesian differential ranking algorithm to identify ASD-associated molecular and taxa profiles across 10 cross-sectional microbiome datasets and 15 other datasets, including dietary patterns, metabolomics, cytokine profiles and human brain gene expression profiles. We found a functional architecture along the GBA that correlates with heterogeneity of ASD phenotypes, and it is characterized by ASD-associated amino acid, carbohydrate and lipid profiles predominantly encoded by microbial species in the genera Prevotella, Bifidobacterium, Desulfovibrio and Bacteroides and correlates with brain gene expression changes, restrictive dietary patterns and pro-inflammatory cytokine profiles. The functional architecture revealed in age-matched and sex-matched cohorts is not present in sibling-matched cohorts. We also show a strong association between temporal changes in microbiome composition and ASD phenotypes. In summary, we propose a framework to leverage multi-omic datasets from well-defined cohorts and investigate how the GBA influences ASD."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"11:55-12:10","Format":"In Person","Speaker":"Xiaofang Jiang","Title":"Metagenomic Mining Reveals Niche-Specific Bilirubin Reductases in the Gut Microbiome","Abstract":"The gut microbiome plays crucial roles in animal health and metabolism, including the biotransformation of host and diet-derived metabolites. The microbial reduction of bilirubin, a heme degradation product, to urobilinogen is a key process in maintaining bilirubin homeostasis in animals. This study employs phylogenetic and metagenomic mining approaches to unveil a novel family of gut-adapted bilirubin reductase enzymes within the Old Yellow Enzyme (OYE) family. _x000D_\n_x000D_\nThrough an integrated analysis combining experimental screening, comparative genomics, and advanced computational methodologies, we identified and characterized a putative bilirubin reductase enzyme family in anaerobic microbes associated with the gut. Through structural modeling, ancestral sequence reconstruction, and targeted mutation experiments, we confirmed the specificity and function of these enzymes, delineating them from other members of the OYE family. Our findings reveal three distinct forms of bilirubin reductase, characterized by unique domain compositions, that form separate clades within the enzyme's phylogeny._x000D_\n_x000D_\nOur analysis of 1373 gut metagenomes across 132 animal species illuminated the evolutionary divergence and niche-specific associations of the bilirubin reductase clades. We found that bilirubin reductase was significantly enriched in the anaerobic niche of the lower gut in multiple animals, being nearly absent in their upper gastrointestinal tracts. The broader distribution of bilirubin reductase clades highlights clear patterns of co-evolution with their animal hosts, underscoring the ecological and evolutionary interplay between gut microbes and their vertebrate hosts._x000D_\n"},{"Track":"MICROBIOME","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Christina Boucher","Title":"Scalable de novo Classification of Antimicrobial Resistance of Mycobacterium Tuberculosis","Abstract":"We develop a robust machine learning classifier using both linear and nonlinear models (i.e., LASSO logistic regression (LR) and random forests (RF)) to predict the phenotypic resistance of \\emph{Mycobacterium tuberculosis} (MTB) for a broad range of antibiotic drugs. We use data from the CRyPTIC consortium to train our classifier, which consists of whole genome sequencing and antibiotic susceptibility testing (AST) phenotypic data for 13 different antibiotics.  To train our model, we assemble the sequence data into genomic contigs, identify all unique 31-mers in the set of contigs, and build a feature matrix M, where M[i,j] is equal to the number of times the i-th 31-mer occurs in the j-th genome.  Due to the size of this feature matrix (over 350 million unique 31-mers), we build and use a sparse matrix representation. Our method, which we refer to as MTB++, leverages compact data structures and iterative methods to allow for the screening of all the 31-mers in the development of both LASSO LR and RF. MTB++ is able to achieve high discrimination (F-1 greater than 80%) for the first-line antibiotics.  Moreover, MTB++ had the highest F-1 score in all but three classes and was the most comprehensive since it had an F-1 score greater than 75% in all but four (rare) antibiotic drugs. We use our feature selection to contextualize the 31-mers that are used for the prediction of phenotypic resistance, leading to some insights about sequence similarity to genes in MEGARes."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Youping Deng","Title":"Genomic analysis reveals dysregulation of the intratumor microbiome related to immune response in lung cancer","Abstract":"Background: Identifying factors underlying resistance to immune checkpoint therapy (ICT) is still challenging. Intratumor microbes (bacteria, fungi, and viruses) are found in multiple tumor tissues of many cancers.  In this study, we examined the intratumor microbes of lung patients under immune checkpoint inhibitors. _x000D_\nMethods: We downloaded the whole exome sequencing (WXS) that contains primary tumor and non-tumor non-small cell lung cancer (NSCLC) with clinical data downloaded from Genomes and Phenotypes (dbGaP) databases. The data was collected from three NSCLC with ICT response cohorts (phs002244, phs000980 and phs001940), including 44 response (R) samples and 51 nonresponse (NR) samples. The microbesâ€™ abundance, diversity and significant microbes were extracted through machine learning and microbiome analysis._x000D_\nResults:  The whole exome sequencing (WXS) of 95 patientâ€™s data (NR, 51; R,44) were obtained and analyzed from Feb 2023 to April 2024. After cleaning up, and microbiome analysis data, we found significant significantly higher alpha diversity in response group compare with non-response group in three type microbes in tumor samples (p<0.05) while not significant found in non-tumor tissues. Through different microbesâ€™ analysis, we found bacterium (Lactobacillus gasseri), fungi (Aspergillus_versicolor, GS01_phy_Incertae_sedis_sp) and viruses (Alphabaculovirus, and Mardivirus) are top significant species and genus in response group compared to non-response group in tumor tissues. We found top abundance species and genus of bacteria (Lactobacillus gasseri, and Ralstonia solanacearum), fungi (Aspergillus, Fungi_gen_Incertae_sedis) and viruses (Alphabaculovirus and Betapartitivirus) in tumor samples. _x000D_\nConclusion: Together, these microbes data provide important implications for the treatment of lung cancer with immune checkpoint inhibitors."},{"Track":"MICROBIOME","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Jithin Sunny","Title":"Bioinformatics exploration of bacterial communities and plastic-degrading laccase from the gut microbiomes of plastic degrading beetle larvae","Abstract":"This study utilizes comprehensive bioinformatics approaches to investigate the gut bacterial population of mealworms and superworms and to mine for enzymes potentially involved in plastic degradation. A total of 46 metagenomes were assembled, annotated, and analyzed to characterize the bacterial population and identify taxa differentially abundant between insects fed plastics and those not fed plastics. Alpha and beta diversity metrics were first used to examine global differences between the diet groups followed by non-symmetric analysis to explain the variation in data. Binning of the metagenome assemblies led to the generation of 153 metagenome-assembled genomes (MAGs). Metabolic pathway analysis was performed for these MAGs to observe the gene counts involved in aromatic compound degradation genes belonging to the butanoate, and propanoate metabolic pathways amongst others. To further explore genes potentially associated with plastic biodegradation, we annotated all the metagenomes and extracted a non-redundant set of ~105,000 proteins. The non-redundant set of exported proteins included 129 putative laccases, which were of interest as previous studies have implicated this protein family in plastic degradation. We therefore performed sequence and structural analyses to explore the properties of the putative laccases identified in our study. Features were computed using site and domain-based information along with residue and enzyme backbone based structural similarity. Three different clustering methods along with evaluation metrics were employed to evaluate enzymes showing high similarity to laccases previously suggested to be active on plastics. Overall, this research employs different bioinformatics techniques to understand the bacterial groups and enzymes involved in plastic degradation."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:30","Format":"In Person","Speaker":null,"Title":"How generative AI can transform biomedical research ","Abstract":"This talk will explore how we can develop and use generative AI to help researchers. I will first discuss how generative AI can act as research co-advisors. Then I will first discuss how we use genAI to expand researchers' creativity by designing and experimentally validating new drugs. Finally, I will explore the role of language as the foundational data modality for biomedicine. "},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"11:30-11:40","Format":"In Person","Speaker":"Eric Sun","Title":"SPRITE: improving spatial gene expression imputation with gene and cell networks","Abstract":"Spatially resolved single-cell transcriptomics have provided unprecedented insights into gene expression {\\it in situ}, particularly in the context of cell interactions or organization of tissues. However, current technologies for profiling spatial gene expression at single-cell resolution are generally limited to the measurement of a small number of genes. To address this limitation, several algorithms have been developed to impute or predict the expression of additional genes that were not present in the measured gene panel. Current algorithms do not leverage the rich spatial and gene relational information in spatial transcriptomics. To improve spatial gene expression predictions, we introduce SPRITE (Spatial Propagation and Reinforcement of Imputed Transcript Expression) as a meta-algorithm that processes predictions obtained from existing methods by propagating information across gene correlation networks and spatial neighborhood graphs. SPRITE improves spatial gene expression predictions across multiple spatial transcriptomics datasets. Furthermore, SPRITE predicted spatial gene expression leads to improved clustering, visualization, and classification of cells. SPRITE is available as a software package and can be used in spatial transcriptomics data analysis to improve inferences based on predicted gene expression."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Charles Broadbent","Title":"Deciphering High-order Structures in Spatial Transcriptomes with Graph-guided Tucker Decomposition","Abstract":"Spatial transcripome (ST) profiling can reveal cellsâ€™ structural organizations and functional roles in tissues. However, deciphering the spatial context of gene expressions in ST data is a challengeâ€”the high-order structure hiding in whole transcriptome space over 2D\/3D spatial coordinates requires modeling and detection of interpretable high-order elements and components for further functional analysis and interpretation. This paper presents a new method GraphTuckerâ€”-graph-regularized Tucker tensor decomposition for learning high-order factorization in ST data. GraphTucker is based on a non-negative Tucker decomposition algorithm regularized by a high-order graph that captures spatial relation among spots and functional relation among genes. In the experiments on several Visium and Stereo-seq datasets, the novelty and advantage of modeling multi-way multilinear relationships among the components in Tucker decomposition are demonstrated as opposed to the Canonical Polyadic Decomposition (CPD) and conventional matrix factorization models by evaluation of detecting spatial components of gene modules, clustering spatial coefficients for tissue segmentation and imputing complete spatial transcriptomes. The results of visualization show strong evidences that GraphTucker detect more interpretable spatial components in the context of the spatial domains in the tissues. Availability: https:\/\/github.com\/kuanglab\/GraphTucker."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:10","Format":"In Person","Speaker":null,"Title":"CellPie: a fast spatial transcriptomics factor discovery method via joint factorization of gene expression and imaging data","Abstract":"Spatially resolved transcriptomics has revolutionised the study of the gene expression within tissues, allowing researchers to maintain the spatial context. Accompanying these spatial transcriptomics datasets are often histology images, providing rich information on tissue architecture, organisation and pathology, complementing the spatial gene expression. However, in traditional pipelines, histological information is typically discarded during tasks such as dimensionality reduction of the spatial transcriptomics data. _x000D_\nTo address this limitation, we propose Cellpie, a novel approach based on fast, joint non-negative matrix factorisation (NMF). Cellpie simultaneously decomposes spatial gene expression and histology image features into interpretable components. Through joint NMF, CellPie generates non-negative factor matrices representing parts-based representation (factors) of the data, facilitating the identification of biologically relevant patterns of variation. In addition, CellPie extracts the corresponding leading genes and image features that are strongly associated with each factor. These genes and features serve as marker genes and morphological characteristics, respectively, providing insights into the biological processes underlying the observed patterns in the spatial gene expression data. Furthermore, they enable the discoverer of links between molecular signalling and tissue morphology._x000D_\nWe demonstrated CellPie on two distinct tissue types, showcasing its improved accuracy in downstream analysis tasks compared to published dimensionality reduction methods."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"12:10-12:30","Format":"In Person","Speaker":"Sachin Mathur","Title":"Integrating patients in time series clinical transcriptomics data","Abstract":"Motivation: Analysis of time series transcriptomics data from clinical trials is challenging. Such studies usually profile very few time points from several individuals with varying response patterns and dynamics. Current methods for these datasets are mainly based on linear, global orderings using visit times which do not account for the varying response rates and subgroups within a patient cohort._x000D_\n_x000D_\nResults We developed a new method that utilizes multi-commodity flow algorithms for trajectory inference in large scale clinical studies. Recovered trajectories satisfy individual-based timing restrictions while integrating data from multiple patients. Testing the method on multiple drug datasets demonstrated an improved performance compared to prior approaches suggested for this task, while identifying novel endotypes that correspond to heterogeneous patient response patterns._x000D_\n_x000D_\nAvailability: The source code and instructions to download the data have been deposited on GitHub at https:\/\/github.com\/euxhenh\/Truffle"},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"14:20-15:10","Format":"In Person","Speaker":null,"Title":"Learning the Language of Biology: Transforming Biomedical Discovery with Foundation Models and Causal Inference","Abstract":"In this talk, I will showcase the work of my lab in revolutionizing biomedical data analysis through foundation models and large language models (LLMs). First, we introduce CINEMA-OT, a causal-inference-based approach using optimal transport for single-cell perturbation analysis. CINEMA-OT allows individual treatment-effect analysis, response clustering, and synergy analysis, revealing potential mechanisms in airway antiviral response and immune cell recruitment. Next, we present CaLMFlow, combining flow matching with integral equations and causal language models. By fine-tuning LLMs on flow matching and conditioning on natural language prompts, CaLMFlow predicts single-cell perturbation responses and performs protein backbone generation. We then explore \"Cell2Sentence\" (C2S), a technique translating single-cell transcriptomics into a language for LLMs. C2S automates the generation of natural language insights directly from biological data and generates cells based on textual prompts, enhancing data interpretation and synthesis. Additionally, I will discuss \"BrainLM,\" the first fMRI foundation model to decode brain activity, predict clinical variables, and improve our understanding of brain function and disease. Finally, I will present some of our efforts to integrate foundation models with graphs with the aim to leverage pre-trained textual and non-textual foundation models for graph-based tasks."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"15:10-15:20","Format":"In Person","Speaker":"Jakub Zarzycki","Title":"Deep Reinforcement Learning for Controlled Traversing of the Attractor Landscape of Boolean Models in the Context of Cellular Reprogramming","Abstract":"Cellular reprogramming can be used for both the prevention and cure of different diseases. However, the efficiency of discovering reprogramming strategies with classical wet-lab experiments is hindered by lengthy time commitments and high costs. In this study, we develop a novel computational framework based on deep reinforcement learning that facilitates the identification of reprogramming strategies. For this aim, we formulate a control problem in the context of cellular reprogramming for the frameworks of BNs and PBNs under the asynchronous update mode. Furthermore, we introduce the notion of a pseudo-attractor and a procedure for identification of pseudo-attractor state during training. Finally, we devise a computational framework for solving the control problem, which we test on a number of different models."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Ding Bai","Title":"AttentionPert: Accurately Modeling Multiplexed Genetic Perturbations with Multi-scale Effects","Abstract":"Genetic perturbations (e.g. knockouts, variants) have laid the foundation for our understanding of many diseases, implicating pathogenic mechanisms and indicating therapeutic targets. However, experimental assays are fundamentally limited by the number of measurable perturbations. Computational methods can fill this gap by predicting perturbation effects under novel conditions, but accurately predicting the transcriptional responses of cells to unseen perturbations remains a significant challenge. We address this by developing a novel attention-based neural network, AttentionPert, which accurately predicts gene expression under multiplexed perturbations and generalizes to unseen conditions. AttentionPert integrates global and local effects in a multi-scale model, representing both the non-uniform system-wide impact of the genetic perturbation and the localized disturbance in a network of gene-gene similarities, enhancing its ability to predict nuanced transcriptional responses to both single and multi-gene perturbations. In comprehensive experiments, AttentionPert demonstrates superior performance across multiple datasets outperforming the state-of-the-art method in predicting differential gene expressions and revealing novel gene regulations. AttentionPert marks a significant improvement over current methods, particularly in handling the diversity of gene perturbations and in predicting out-of-distribution scenarios."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"Live Stream","Speaker":"Wei Huang","Title":"Predicting single-cell cellular responses to perturbations using cycle consistency learning","Abstract":"Phenotype-based screening has emerged as a powerful approach for identifying compounds that actively interact with cells.  Transcriptional and proteomic profiling of cell population and single cell provide insights into the cellular changes that occur at the molecular level in response to external perturbations, such as drugs or genetic manipulations. In this paper, we propose cycleCDR, a novel deep learning framework to predict cellular response to drugs or gene perturbations. We leverage the power of autoencoders to maps the unperturbed cellular states to a latent space, in which we postulate the effects of drug perturbations on cellular states follow a linear additive model. Next, we introduce the cycle consistency constraints to ensure that unperturbed cellular state subjected to drug perturbation in the latent space would produce the perturbed cellular state through the decoder. Conversely, removal of perturbations from the perturbed cellular states could restore the unperturbed cellular state. The cycle consistency constraints and linear modeling in latent space enable to learn transferable representations of external perturbations, so that our model can generalize well to unseen drugs. We validate our model on four different types of datasets, including bulk transcriptional responses, bulk proteomic responses, and single-cell transcriptional responses to drug\/gene perturbations. The experimental results demonstrate that our model consistently outperforms existing state-of-the-art methods, indicating our method is highly versatile and applicable to a wide range of scenarios."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Asa Ben-Hur","Title":"The role of chromatin state in intron retention:  a case study in leveraging large scale deep learning models","Abstract":"Complex deep learning models trained on very large datasets have become key enabling tools for current research in  natural language processing and computer vision._x000D_\nBy providing pre-trained models that can be fine-tuned for specific applications, they enable researchers to create accurate models with minimal effort and computational resources._x000D_\nLarge scale genomics deep learning models come in two flavors:  the first are large language models of DNA sequences trained in a self-supervised fashion, similar to the corresponding natural language models; the second are supervised learning models that leverage large scale genomics datasets from ENCODE and other sources. _x000D_\nWe argue that these models are the equivalent of foundation models in natural language processing in their utility, as they encode within them chromatin state in its different aspects, providing useful representations that allow quick deployment of accurate models of gene regulation._x000D_\nWe demonstrate this premise by leveraging the recently created Sei model to develop simple, interpretable models of intron retention, and demonstrate their advantage over models based on the DNA langauage model DNABERT-2._x000D_\nOur work also demonstrates the impact of chromatin state on the regulation of intron retention._x000D_\nUsing representations learned by Sei, our model is able to discover the involvement of transcription factors and chromatin marks in regulating intron retention, providing better accuracy than a recently published model trained from scratch for this purpose."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Anupama Jha","Title":"Predicting interchromosomal Hi-C contacts from DNA sequence with TwinC","Abstract":"The 3D nuclear DNA architecture is composed of intrachromosomal and interchromosomal contacts. Despite the functional relevance of interchromosomal contacts, existing predictive models for 3D genome folding have focused on modeling intrachromosomal contacts from nucleotide sequences, mainly ignoring the contributions of interchromosomal contacts. To remedy this, we propose TwinC, an interpretable convolutional neural network model that uses a paired sequence design to model Hi-C interchromosomal contacts from replicate Hi-C experiments. TwinC accepts two 100~kb nucleotide sequences as input and predicts interchromosomal contacts between them. We use Hi-C experiments from 20 human donor heart samples from the ENCODE project to show that TwinC achieves high predictive accuracy (AUROC=0.80) on a cross-chromosomal test set. Furthermore, despite TwinC's computational simplicity and faster training time, it performs at par with the state-of-the-art orca model. Subsequently, we show that TwinC learns the importance of local chromatin accessibility features in the formation of interchromosomal contacts and identifies transcription factories located on different chromosomes that cluster in the nucleus. Our results suggest that by leveraging pooled contacts from multiple donors and employing a twin sequence design, TwinC can learn to accurately predict interchromosomal contacts and identify sequence signatures relevant to their 3D structure in the nucleus."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Mogan Gim","Title":"MolPLA: A Molecular Pre-training Framework for Learning Cores, R-Groups and their Linker Joints","Abstract":"Motivation: Molecular core structures and R-groups are essential concepts especially in compound analysis and lead optimization. Integration of these concepts with conventional graph pre-training approaches can promote deeper understanding in both local and global properties of molecules. We propose MolPLA, a dual molecular pre-training framework that promotes understanding in a molecule's core structure with peripheral R-groups and extends it with the ability to help chemists find replaceable R-groups in lead optimization scenarios._x000D_\nResults: Experimental results on molecular property prediction show that MolPLA exhibits predictability comparable to current state-of-the-art models. Qualitative analysis implicate that MolPLA is capable of distinguishing core and R-group sub-structures, identifying decomposable regions in molecules and contributing to lead optimization scenarios by rationally suggesting R-group replacements given various query core templates."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Yoseph Barash","Title":"Deep generative models for RNA splicing predictions and design","Abstract":"Alternative splicing (AS) of pre-mRNA is a highly regulated process with significant splicing changes occurring across human tissues. The tissue-specific changes in splicing, combined with the fact splicing defects are related to numerous disease made the ability to predict or manipulate AS a  long-term goal, with applications ranging from identifying novel regulatory mechanisms to designing therapeutic targets. Here, we take advantage of generative model architectures to address the prediction and design of tissue-specific RNA splicing outcomes. First, we construct a predictive model, TrASPr, which combines multiple localized transformers to predict splicing in a tissue-specific manner. Then, we exploit TrASPr as an Oracle to produce labeled data for a Bayesian Optimization (BO) algorithm with a custom loss function for RNA splicing outcome design. We demonstrate TrASPr significantly outperforms recently published models and identifies relevant regulatory features also captured by the BO generative process."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Daniel Olson","Title":"NEAR: Neural Embeddings for Amino acid Relationships","Abstract":"The homology search tool HMMER is extremely sensitive and can identify homologous protein pairs even when there is very little %id between them. Search tools such as MMSeqs2 and DIAMOND are extremely efficient and capable of rapidly searching extremely large protein databases like TrEMBL, but are less sensitive than HMMER. The varying performance (speed and accuracy) of these tools is largely influenced by the choice of filtering strategies that are used to eliminate candidate alignments before running more expensive alignment algorithms. Motivated by a desire to retain full HMM sensitivity with greater speed, we have developed a new filtering method, called NEAR (Neural Embeddings for Amino acid Relationships). NEAR is a method based on representation learning that is designed to rapidly identify good sequence alignment candidates from a large protein database. NEAR's neural embedding model computes per-residue embeddings for target and query protein sequences and identifies alignment candidates with a pipeline consisting of k-NN search, filtration, and neighbor aggregation._x000D_\nNEARâ€™s ResNet embedding model is trained using an N-pairs loss function guided by sequence alignments generated by the widely used HMMER3 tool._x000D_\nBenchmarking results reveal improved performance relative to state-of-the-art neural embedding models specifically developed for protein sequences, as well as enhanced speed relative to the alignment-based filtering strategy used in HMMER3â€™s sensitive alignment pipeline. We present NEAR as a standalone filter, but have plans to integrate NEAR into our search tool NAIL."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Jiri Reinis","Title":"Machine learning-enabled highly multiplexed monitoring of subcellular protein localization in live cells","Abstract":"Imaging-based methods are widely used for studying the subcellular localization of proteins in living cells. While routine for individual proteins, global monitoring of protein dynamics following chemical or genetic perturbation typically relies on arrayed panels of fluorescently tagged cell lines, limiting throughput and scalability. Here, we describe a strategy that combines high-throughput microscopy, computer vision, and machine learning to detect perturbation-induced changes in multicolor tagged visual proteomics cell (vpCell) pools._x000D_\n_x000D_\nWe use genome-wide and cancer-focused intron-targeting sgRNA libraries to generate vpCell pools and a large arrayed collection of clones (4,576 clones, 1,158 unique fluorescently tagged proteins). Each vpCell clone expresses two different endogenously tagged fluorescent proteins. Individual clones can be identified in the pool by image analysis alone, training a machine learning model on localization patterns and expression levels of the tagged proteins. This enables simultaneous live-cell monitoring of large sets of proteins._x000D_\n_x000D_\nTo demonstrate broad applicability and scale, we test the effects of antiproliferative compounds on a pool with cancer-related proteins, on which we identify widespread protein localization changes and novel inhibitors of the nuclear import\/export machinery. The time-resolved characterization of changes in subcellular localization and abundance of proteins upon perturbation in pooled format highlights the power of the vpCell approach for drug discovery and mechanism of action studies._x000D_\n_x000D_\nFinally, we present an interactive online web atlas of 1,158 fluorescently labeled proteins in clonal cell lines, available at https:\/\/vpcells.cemm.at."},{"Track":"MLCSB","Room":"517d","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Zhangzhi Peng","Title":"PTM-Mamba: A PTM-Aware Protein Language Model with Bidirectional Gated Mamba Blocks","Abstract":"Proteins serve as the workhorses of living organisms, orchestrating a wide array of vital functions. Post-translational modifications (PTMs) of their amino acids greatly influence the structural and functional diversity of different protein types and uphold proteostasis, allowing cells to swiftly respond to environmental changes and intricately regulate complex biological processes. To this point, efforts to model the complex features of proteins have involved the training of large and expressive protein language models (pLMs) such as ESM-2 and ProtT5, which accurately encode structural, functional, and physicochemical properties of input protein sequences. However, the over 200 million sequences that these pLMs were trained on merely scratch the surface of proteomic diversity, as they neither input nor account for the effects of PTMs. In this work, we fill this major gap in protein sequence modeling by introducing PTM tokens into the pLM training regime. We then leverage recent advancements in structured state space models (SSMs), specifically Mamba, which utilizes efficient hardware-aware primitives to overcome the quadratic time complexities of Transformers. After adding a comprehensive set of PTM tokens to the model vocabulary, we train bidirectional Mamba blocks whose outputs are fused with state-of-the-art ESM-2 embeddings via a novel gating mechanism. We demonstrate that our resultant PTM-aware pLM, PTM-Mamba, improves upon ESM-2's performance on various PTM-specific tasks. PTM-Mamba is the first and only pLM that can uniquely input and represent both wild-type and PTM sequences, motivating downstream modeling and design applications specific to post-translationally modified proteins."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"08:40-09:00","Format":"In Person","Speaker":"Natalie Davidson","Title":"Domain adaptation for cell-free DNA fragmentomics","Abstract":"Cell-free DNA (cfDNA) is an emerging minimally-invasive biomarker that could detect cancer, indicate transplant rejection, and predict autoimmune disease severity. A critical application of cfDNA is identifying the cfDNAâ€™s tissue-of-origin, a presumed disease source. The most established cfDNA strategies rely on identifying disease-specific mutations but can only be applied to diseases with a known variant.  However, the recent discovery that cfDNA fragmentation patterns reflect nucleosome positioning and active transcription factor binding sites (TFBSs) indicates that the fragmentation patterns alone can predict the tissue of origin and open the door for applications to a broader range of diseases. _x000D_\n_x000D_\nCurrently, to predict tissue-of-origin, one needs to gather large cohorts to sample their cfDNA, which is commonly infeasible. In contrast, we propose that we instead use domain adaptation to train a model on a complementary data type, ATAC-Seq, such that it can also be used on cfDNA._x000D_\n_x000D_\nTo do this, we must address two key problems: 1) generating a tissue prediction model that can translate across the domains of cfDNA and ATAC-Seq; 2) that the majority of cfDNA reads will come from blood. _x000D_\n_x000D_\nWe address both problems through the use of data augmentation strategies and the utilization of our previously preprinted domain invariant method, BuDDI. We apply this approach first to ATAC-Seq alone, to ensure our model can detect the tissue of origin, even when 99% of total reads come from blood and not the tissue of interest. Finally, we apply our approach to real cfDNA."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"08:40-09:00","Format":"In Person","Speaker":"Yang Lu","Title":"DeepROCK: Error-controlled interaction detection in deep neural networks","Abstract":"The complexity of deep neural networks (DNNs) makes them powerful but also makes them challenging to interpret, hindering their applicability in error-intolerant domains. Existing methods attempt to reason about the internal mechanism of DNNs by identifying feature interactions that influence prediction outcomes. However, such methods typically lack a systematic strategy to prioritize interactions while controlling confidence levels, making them difficult to apply in practice for scientific discovery and hypothesis validation. In this paper, we introduce a method, called DeepROCK, to address this limitation by using knockoffs, which are dummy variables that are designed to mimic the dependence structure of a given set of features while being conditionally independent of the response. Together with a novel DNN architecture involving a pairwise-coupling layer, DeepROCK jointly controls the false discovery rate (FDR) and maximizes statistical power. In addition, we identify a challenge in correctly controlling FDR using off-the-shelf feature interaction importance measures. DeepROCK overcomes this challenge by proposing a calibration procedure applied to existing interaction importance measures to make the FDR under control at a target level. Finally, we validate the effectiveness of DeepROCK through extensive experiments on simulated and real datasets."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"09:00-09:20","Format":"In Person","Speaker":"Stefan Schrod","Title":"CODEX: COunterfactual Deep learning for the in-silico EXploration of cancer cell line perturbations","Abstract":"Motivation: High-throughput screens (HTS) provide a powerful tool to decipher the causal effects of chemical and genetic perturbations on cancer cell lines. Their ability to evaluate a wide spectrum of interventions, from single drugs to intricate drug combinations and CRISPR-interference, has established them as an invaluable resource for the development of novel therapeutic approaches. Nevertheless, the combinatorial complexity of potential interventions makes a comprehensive exploration intractable. Hence, prioritizing interventions for further experimental investigation becomes of utmost importance._x000D_\nResults: We propose CODEX as a general framework for the causal modeling of HTS data, linking perturbations to their downstream consequences. CODEX relies on a stringent causal modeling strategy based on counterfactual reasoning. As such, CODEX predicts drug-specific cellular responses, comprising cell survival and molecular alterations, and facilitates the in-silico exploration of drug combinations. This is achieved for both bulk and single-cell HTS. We further show that CODEX provides a rationale to explore complex genetic modifications from CRISPR-interference in silico in single cells._x000D_\nAvailability and Implementation: Our implementation of CODEX is publicly available at https:\/\/github.com\/sschrod\/CODEX. All data used in this article are publicly available._x000D_\nSupplementary information: Supplementary materials are available at Bioinformatics online."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Divya Koyyalagunta","Title":"A statistical method for migration history inference reveals alternative patterns of metastatic dissemination, clonality and phyleticity","Abstract":"Although metastasis is the cause of 90% of cancer deaths, little is known about its clonal evolution, genetic drivers, and seeding patterns. Identifying these patterns from DNA sequencing data requires solving a challenging mixed-variable combinatorial optimization problem to reconstruct the history of metastatic migrations. Current methods, based on integer linear programs, are slow, restricted to unrealistic assumptions, and cannot report uncertainty in their reconstructions. Furthermore, a fundamental problem with these methods is their inability to choose between multiple equally or similarly likely metastatic migration histories. To address these challenges, we propose a novel statistical framework for migration history inference, Metient, which uses recent machine learning advancements in discrete variable gradient estimation and metastasis specific priors. Rather than requiring a metastatic seeding dissemination model to be known a priori, Metient aims to answer this question by evaluating all possible migration history hypotheses and choosing the best model as informed by biologically motivated data. On simulated data, Metient outperforms the state-of-the-art, and can sample up to 64 possible solutions in 1% of the time. The migration histories inferred by Metient on 167 patients with four cancer types recover expert-assigned parsimony models in 84% of cases, but find notable differences where more plausible histories are proposed. We find that parallel gains of metastatic potential are much less common than previously proposed, and that polyclonal seeding occurs more in lymph nodes than in distant metastases. Along with significantly improving existing methodology, Metient provides a means to better model metastasis across different cancer types."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Sungjoon Park","Title":"A deep learning model of tumor cell architecture elucidates response and resistance to CDK4\/6 inhibitors","Abstract":"Cyclin-dependent kinase 4 and 6 inhibitors (CDK4\/6is) have revolutionized breast cancer therapy. However, <50% of patients have an objective response, and nearly all patients develop resistance during therapy. To elucidate the underlying mechanisms, we constructed an interpretable deep learning model of the response to palbociclib, a CDK4\/6i, based on a reference map of multiprotein assemblies in cancer. The model identifies eight core assemblies that integrate rare and common alterations across 90 genes to stratify palbociclib-sensitive versus palbociclib-resistant cell lines. Predictions translate to patients and patient-derived xenografts, whereas single-gene biomarkers do not. Most predictive assemblies can be shown by CRISPRâ€“Cas9 genetic disruption to regulate the CDK4\/6i response. Validated assemblies relate to cell-cycle control, growth factor signaling and a histone regulatory complex that we show promotes S-phase entry through the activation of the histone modifiers KAT6A and TBL1XR1 and the transcription factor RUNX1. This study enables an integrated assessment of how a tumorâ€™s genetic profile modulates CDK4\/6i resistance."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"In Person","Speaker":"Monica-Andreea Baciu-Dragan","Title":"oncotree2vec â€“ A method for embedding and clustering of tumor mutation trees","Abstract":"Understanding the genomic heterogeneity of tumors is an important task in computational oncology, especially in the context of finding personalized treatments based on the genetic profile of each patientâ€™s tumor. Tumor clustering that takes into account the temporal order of genetic events, as represented by tumor mutation trees, is a powerful approach for grouping together patients with genetically and evolutionarily similar tumors and can provide insights into discovering tumor subtypes, for more accurate clinical diagnosis and prognosis. Here, we propose oncotree2vec, a method for clustering tumor mutation trees by learning vector representations of mutation trees that capture the different relationships between subclones in an unsupervised manner. Learning low-dimensional tree embeddings facilitates the visualization of relations between trees in large cohorts and can be used for downstream analyses, such as deep learning approaches for single-cell multi-omics data integration. We assessed the performance and the usefulness of our method in three simulation studies, and on two real datasets: a cohort of 43 trees from six cancer types with different branching patterns corresponding to different modes of spatial tumor evolution and a cohort of 123 AML mutation trees."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:30","Format":"In Person","Speaker":"Sara Mostafavi","Title":"Deep learning of personal genomes","Abstract":null},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"11:30-11:40","Format":"In Person","Speaker":"Linh Tran","Title":"ConfuseNN: Interpreting convolutional neural network inferences in population genomics with data shuffling","Abstract":"Convolutional neural network (CNN) is an increasingly popular supervised machine learning approach that has been applied to many inference tasks in population genomics. Under this framework, population genomic variation data are typically represented as 2D images with sampled haplotypes as rows and segregating sites as columns. While many published studies reported promising performance of CNNs on various inference tasks, understanding which features in the data meaningfully contributed to the CNN's reported performance remains challenging. Here we propose a novel approach to interpreting CNN performance motivated by population genetic theory on genomic data. Specifically, we designed a suite of scramble tests where each test deliberately disrupts a feature in the genomic image data (e.g. allele frequency, linkage disequilibrium, etc.) to assess how each feature affects the CNN performance. We applied these tests to three networks designed to infer demographic history and natural selection from genetic variation data, identifying the fundamental population genomic features that drive inference for each network."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:30","Format":"In Person","Speaker":null,"Title":"Trustworthy AI in the life sciences","Abstract":null},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-15:10","Format":"In Person","Speaker":null,"Title":"Towards spatiotemporal design principles in multicellular systems","Abstract":"Gene expression profiles of a cellular population, generated by single-cell RNA sequencing, contain rich, 'hidden' information about biological state and collective multicellular behavior that is lost during the experiment or not directly accessible, including cell type, cell cycle phase, gene regulatory patterns, cell-cell communication, and location within the tissue-of-origin. In this talk I will discuss several methods, based on a combination of spectral, machine learning, and dynamical systems approaches, to disentangle and enhance particular spatiotemporal signals that cellular populations encode and interpret their manifestation across space and time in tissues. We will further discuss how we can computationally transfer knowledge across biological datasets and systematically identify gaps in our knowledge."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"15:10-15:30","Format":"In Person","Speaker":"Alexander Immer","Title":"Probabilistic Pathway-based Multimodal Factor Analysis","Abstract":"Multimodal profiling strategies promise to produce more informative insights into biomedical cohorts via the integration of the information each modality contributes. In order to perform this integration, however, the development of novel analytical strategies are needed. Multimodal profiling strategies often come at the expense of lower sample numbers, which can challenge methods to uncover shared signals across a cohort. Thus, factor analysis approaches are commonly used for the analysis of high-dimensional data in molecular biology, however they typically do not yield representations that are directly interpretable, whereas many research questions often center around the analysis of pathways associated with specific observations._x000D_\n_x000D_\nWe develop PathFA, a novel approach for multimodal factor analysis over the space of pathways. PathFA produces integrative and interpretable views across multimodal profiling technologies, which allow for the derivation of concrete hypotheses. PathFA combines a pathway-learning approach with integrative multimodal capability under a Bayesian procedure that is efficient, hyper-parameter free, and able to automatically infer observation noise from the data. We demonstrate strong performance on small sample sizes within our simulation framework and on matched proteomics and transcriptomics profiles from real tumor samples taken from the Swiss Tumor Profiler consortium. On a subcohort of melanoma patients, PathFA recovers pathway activity that has been independently associated with poor outcome. We further demonstrate the ability of this approach to identify pathways associated with the presence of specific cell-types as well as tumor heterogeneity. Our results show that we capture known biology, making it well suited for analyzing multimodal sample cohorts."},{"Track":"MLCSB","Room":"517d","Weekday":"Tuesday","Date":"16 July","Timespan":"15:30-15:40","Format":"In Person","Speaker":"Jishnu Das","Title":"SLIDE: Significant Latent Factor Interaction Discovery and Exploration across biological domains","Abstract":"Modern multi-omic technologies can generate deep multi-scale profiles. However, differences in data modalities, multicollinearity, and large numbers of irrelevant features make the analyses and integration of high-dimensional omic datasets challenging. Here, we present Significant Latent factor Interaction Discovery and Exploration (SLIDE), a first-in-class interpretable machine learning technique for identifying significant interacting latent factors underlying outcomes of interest from high-dimensional omic datasets. SLIDE makes no assumptions regarding data-generating mechanisms, comes with theoretical guarantees regarding identifiability of the latent factors\/corresponding inference, and has rigorous FDR control. SLIDE outperforms a wide range of state-of-the-art approaches, including other latent factor approaches, in terms of prediction. More importantly, it provides biological inference beyond prediction that other methods do not afford. Using SLIDE on scRNA-seq data from systemic sclerosis (SSc) patients, we first uncovered significant interacting latent factors underlying SSc pathogenesis. In addition to outperforming existing benchmarks for prediction, SLIDE uncovered significant factors that included well-elucidated altered transcriptomic states in myeloid cells and fibroblasts and a novel keratinocyte-centric signature validated by protein staining. SLIDE also worked well on a wide range of spatial modalities spanning transcriptomic and proteomic data and was able to accurately identify significant interacting latent factors underlying immune cell partitioning by 3D location in different contexts. Finally, SLIDE leveraged paired scRNA-seq and TCR-seq data to elucidate novel latent factors underlying extents of clonal expansion of CD4 T cells in a nonobese diabetic model of T1D. Overall, SLIDE is a versatile engine for biological discovery from modern multi-omic datasets."},{"Track":"NetBio","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"15:10-15:20","Format":"In Person","Speaker":null,"Title":"NetBio Opening","Abstract":null},{"Track":"NetBio","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"15:20-16:00","Format":"In Person","Speaker":"Sergio Baranzini","Title":"Towards semantic representation and causal inference in biomedicine.  Challenges and applications","Abstract":"Massive amounts of data and information are available for analysis in biomedicine. However, integration of these resources in a powerful statistical but also biologically meaningful framework poses a considerable challenge. SPOKE is a large knowledge graph containing information from more than 40 specialized databases and spanning multiple disciplines within biomedicine. Currently SPOKE contains 50 million concepts and more than 130 million relationships organized in a semantic graph.  This talk will cover the creation of SPOKE and some of its cutting-edge applications. Some examples will include the embedding of more than 2 million electronic health records onto SPOKE, which led to training of machine learning models to predict diagnosis and outcomes in multiple sclerosis (MS), Parkinsonâ€™s disease (PD) and Alzheimerâ€™s (AD). In addition, efforts directed towards applications in drug development and repurposing will be presented. Finally, automated integrative strategies are needed to fully harness the power of biomedical information. To that end, a novel method of knowledge graph-based retrieval augmentation (KG-RAG) implemented over SPOKE will be discussed."},{"Track":"NetBio","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Kevin Rupp","Title":"Modeling metastatic progression from cross-sectional cancer genomics data","Abstract":"Metastasis formation is a hallmark of cancer lethality. Yet, metastases are generally unobservable during their early stages_x000D_\nof dissemination and spread to distant organs. Genomic datasets of matched primary tumors and metastases may offer_x000D_\ninsights into the underpinnings and the dynamics of metastasis formation. We present metMHN, a cancer progression_x000D_\nmodel designed to deduce the joint progression of primary tumors and metastases using cross-sectional cancer genomics_x000D_\ndata. The model elucidates the statistical dependencies among genomic events, the formation of metastasis, and the clinical_x000D_\nemergence of both primary tumors and their metastatic counterparts. metMHN enables the chronological reconstruction_x000D_\nof mutational sequences and facilitates estimation of the timing of metastatic seeding. In a study of nearly 5000 lung_x000D_\nadenocarcinomas, metMHN pinpointed TP53 and EGFR as mediators of metastasis formation. Furthermore, the study_x000D_\nrevealed that post-seeding adaptation is predominantly influenced by frequent copy number alterations. All datasets and_x000D_\ncode are available on GitHub at https:\/\/github.com\/cbg-ethz\/metMHN."},{"Track":"NetBio","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":null,"Title":"Multi-omics systems biology approach identifies novel signature genes for neuropsychiatric disorders","Abstract":"The complex nature of mental disorders has long fascinated scientists, driving them to uncover the shared genetic factors that link these conditions. Much is still unknown about the genetic overlap across mental  disorders nor the  specificities of their genetic underpinning. We create a gene-disease network using genes associated to disorder from multiple curated sources, which revealed clusters of highly genetically related diseases, corroborating with the main chapters of the DSM-5. Interestingly, psychiatric disorders formed a tight cluster with neurodegenerative disorders. This prompts us to investigate that cluster using a combination of gene coexpression networks and protein-protein interaction networks. To this end, we constructed 61 independent coexpression networks, focusing on Transcription Factors, from studies including data from patients with autism spectrum disorder, Bipolar Disorder, Major Depressive Disorder, Schizophrenia, Alzheimerâ€™s Disease and Parkinson's Disease, as well as control individuals, employing rigorous statistical methods to reduce bias between studies and the number of false positive links, and performed a differential network analysis to compare networks across diseases. Our analysis allowed pinpointing signature TF genes for each disorder that could help improve disease diagnosis. Taken together, our discoveries not only advance our understanding of the interconnectedness of the investigated mental disease but also offer the possibility of improving diagnostic approaches to distinguish between diseases, ultimately benefiting individuals affected by these challenging disorders."},{"Track":"NetBio","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Luis Augusto Eijy Nagai","Title":"Improved community detection through signed graphs in single-cell co-expression networks","Abstract":"Recent advances in single-cell RNA sequencing (scRNA-seq) have highlighted the limitations of traditional gene co-expression network analysis in capturing the full spectrum of gene relationships, particularly in terms of negative correlations. Our study introduces an improved community detection method leveraging signed graphs in single-cell gene co-expression networks (scGCNs) to address this gap. We compared the traditional Louvain algorithm with our proposed Louvain Signed method across three distinct tests: a simulated dataset with inherent subgroups, a real dataset of CD4 cell subtypes, and a challenging dataset of ventral midbrain cells exhibiting stemness properties. The Louvain Signed approach demonstrated superior capability in distinguishing nested gene groups, identifying crucial marker genes, and discerning gene communities linked to specific biological functions, even in datasets where cell types were not clearly defined. Our findings suggest that incorporating both positive and negative gene correlations significantly enhances the resolution and relevance of community detection in scGCNs, offering a more nuanced understanding of cellular functions in single-cell studies. This approach promises to refine our understanding of gene dynamics and cellular heterogeneity, complementing existing methods in single-cell analysis."},{"Track":"NetBio","Room":"520c","Weekday":"Sunday","Date":"14 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Hao Zhu","Title":"Fast Gene Regulatory Network Inference in Single-cell RNA-Seq with RegDiffusion","Abstract":"Understanding gene regulatory networks (GRNs) is crucial for elucidating cellular mechanisms and advancing therapeutic interventions. Many existing methods often struggle with the high dimensionality and inherent noise of single-cell data. Inspired by our previous work on dropout augmentation, here, we introduce RegDiffusion, a new class of Denoising Diffusion Probabilistic Models for fast and accurate GRN inference. RegDiffusion introduces Gaussian noise to the input gene expression data following a diffusion schedule and the neural network with a parameterized adjacency matrix is trained to predict the added noise. This approach eliminates costly matrix inversion and significantly accelerates the inference process. Analyzing real world single-cell data with over 14,000 genes now completes in under five minutes, in contrast to the hours required by previous deep learning methods. Further, to verify the biological validity of the inferred networks, we visualized the inferred local regulatory neighborhood around well-studied key genes in mouse microglia cells. We found that genes identified in those neighborhoods are consistent with prior biological knowledge, and genes from the same functional groups are often topologically clustered together. Finally, we would like to demonstrate the regdiffusion package, which includes a straightforward interface to this model and a set of tools to analyze and visualize the inferred GRNs. Overall, with its capacity for rapid inference on large scale data and the explainability of the inferred networks, we believe RegDiffusion will be a useful tool in computational biology and help deliver new insights into complex biological data. Project site: https:\/\/tuftsbcb.github.io\/RegDiffusion\/"},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Anne-Claude Gingras","Title":"Using proximity-dependent biotinylation to understand dynamic cell organization","Abstract":"Compartmentalization is essential for all complex forms of life. In eukaryotic cells, membrane-bound organelles and a multitude of protein- and nucleic acid-rich subcellular structures maintain boundaries and serve as enrichment zones to promote and regulate protein function, including signalling events. Consistent with the critical importance of these boundaries, alterations in the machinery that mediates protein transport between these compartments have been implicated in several diverse diseases. Understanding the composition of each cellular â€œcompartmentâ€ (be it a classical organelle or a large protein complex) remains a challenging task.  Using the proximity-dependent biotinylation approach BioID, we systematically mapped the composition of various subcellular structures, using well-characterized subcellular markers for a specified location as baits proteins. We defined how relationships between â€œpreyâ€ proteins detected through this approach can help understand the protein organization inside a cell, further facilitated by newly developed computational tools. We will first discuss our map of a human cell containing major organelles and non-membrane bound structures at steady-state, and illustrate how this map can be leveraged to devise â€œcompartment sensorsâ€ to explore dynamic cell signalling. We will then describe a computational and experimental strategy to generate multiple contextual maps of subcellular organization."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Merel Kuijs","Title":"GraphCompass: Spatial metrics for differential analyses of cell organization across conditions","Abstract":"Spatial omics technologies are increasingly leveraged to characterize how disease disrupts tissue organization and cellular niches. While multiple methods to analyze spatial variation within a sample have been published, statistical and computational approaches to compare cell spatial organization across samples or conditions are mostly lacking. We present GraphCompass, a comprehensive set of omics-adapted graph analysis methods to quantitatively evaluate and compare the spatial arrangement of cells in samples representing diverse biological conditions. GraphCompass builds upon the Squidpy spatial omics toolbox and encompasses various statistical approaches to perform cross-condition analyses at the level of individual cell types, niches, and samples. Additionally, GraphCompass provides custom visualization functions that enable effective communication of results. We demonstrate how GraphCompass can be used to address key biological questions, such as how cellular organization and tissue architecture differ across various disease states and which spatial patterns correlate with a given pathological condition. GraphCompass can be applied to various popular omics techniques, including, but not limited to, spatial proteomics (e.g. MIBI-TOF), spot-based transcriptomics (e.g. 10x Genomics Visium), and single-cell resolved transcriptomics (e.g. Stereo-seq). In this work, we showcase the capabilities of GraphCompass through its application to three different studies that may also serve as benchmark datasets for further method development. With its easy-to-use implementation, extensive documentation, and comprehensive tutorials, GraphCompass is accessible to biologists with varying levels of computational expertise. By facilitating comparative analyses of cell spatial organization, GraphCompass promises to be a valuable asset in advancing our understanding of tissue function in health and disease."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Nadezhda T. Doncheva","Title":"Functional analysis of MS-based proteomics data: from protein groups to networks","Abstract":"In high-throughput mass spectrometry (MS), proteins are digested into peptides and the peptide MS signals are then used to infer protein relative quantities across samples. Proteins that cannot be unambiguously distinguished based on the available set of peptides are reported as protein groups containing several protein accessions. However, typical follow-up analysis such as gene set enrichment and protein interaction networks are based on gene-level annotation. Thus, they can only be performed on single proteins or genes, rendering such analysis incompatible with protein group outputs. Currently, there is no best practice on how to handle this and its impact on functional analysis is unknown._x000D_\nHere, we investigate the composition of protein groups identified in 14 published proteomics data sets, including deep proteomes, phosphoproteomics data, single-cell proteomics, and pull down experiments from different species. We show that the arbitrary choice of one protein from each group affects gene set enrichment and network analysis to varying degrees, and that this issue should especially not be ignored in network analysis. We thus developed the Cytoscape app â€œProteo Visualizerâ€ that can complement the widely used stringApp by creating STRING networks from protein groups input instead of single protein accessions. In the resulting networks, each protein group is represented as a single node that inherits all existing edges of the group members. In addition, all relevant node and edge attributes are aggregated. This app opens new avenues for performing network analysis with protein groups from MS studies."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Kevin Drew","Title":"Direct Contacts 2: identification of direct physical interactions from > 25,000 mass spectrometry experiments","Abstract":"Protein complexes are essential to biological function and when disrupted can cause adverse health outcomes such as neurodegenerative disease, developmental disease, and cancer. Many research efforts have identifying thousands of protein complexes, yet we are severely limited in our knowledge of direct physical interactions among complex subunits. Further, knowledge of the three-dimensional (3D) structure of protein complexes illuminates their function but unfortunately the 3D structures of the vast majority of protein complexes are unsolved. These include many implicated in disease limiting our ability to interpret human pathogenic mutations. Here we describe Direct Contacts 2, our machine-learning model for predicting direct physical interactions among pairs of proteins. Our highly accurate network consists of >15k protein pairs predicted to directly interact within identified protein complexes. We developed the Direct Contacts 2 model using > 25,000 high throughput mass spectrometry experiments, including affinity purification and co-fractionation mass spectrometry experiments. Our model was built using the AutoGluon model selection framework and trained on physically interacting proteins from Protein Data Bank (PDB) structures. We evaluated our method using leave out sets of structures from the PDB, a large set of AlphaFold2 structure predictions, and chemical cross-linking data. We illustrate the usefulness of our model in investigating complexes associated with developmental disease, including using our predictions to build AlphaFold-multimer models of Oral-facial-digital syndrome associated proteins and modeling specific mutations. This work informs future research on human genetic disease and allows a framework to place disease mutations into their structural context."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Chao Deng","Title":"Identifying new cancer genes based on the integration of annotated gene sets via hypergraph neural networks","Abstract":"Motivation: Identifying cancer genes remains a significant challenge in cancer genomics research. Annotated gene sets encode functional associations among multiple genes, and cancer genes have been shown to cluster in hallmark signaling pathways  and biological processes. The knowledge of annotated gene sets is critical for discovering cancer genes but remains to be fully exploited._x000D_\nResults: Here, we present the DIsease-Specific Hypergraph neural network (DISHyper), a hypergraph-based computational method that integrates the knowledge from multiple types of annotated gene sets to predict cancer genes. First, our benchmark results demonstrate that DISHyper outperforms the existing state-of-the-art methods and highlight the advantages of employing hypergraphs for representing annotated gene sets. Second, we validate the accuracy of DISHyper-predicted cancer genes using functional validation results and multiple independent functional genomics data. Third, our model predicts 44 novel cancer genes, and subsequent analysis shows their significant associations with multiple types of cancers. Overall, our study provides a new perspective for discovering cancer genes and reveals previously undiscovered cancer genes._x000D_\nAvailability: DISHyper is freely available for download at https:\/\/github.com\/genemine\/DISHyper."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Mehmet Koyutürk","Title":"Are under-studied proteins under-represented? How to fairly evaluate link prediction algorithms in network biology","Abstract":"For biomedical applications, new link prediction algorithms are continuously being developed and these algorithms are typically evaluated computationally, using test sets generated by sampling the edges uniformly at random. However, as we demonstrate, this evaluation approach introduces a bias towards â€œrich nodesâ€, i.e., those with higher degrees in the network. More concerningly, this bias persists even when different network snapshots are used for evaluation, as recommended in the machine learning community. This creates a cycle in research where newly developed algorithms generate more knowledge on well-studied biological entities while under-studied entities are commonly overlooked. To overcome this issue, we propose a weighted validation setting specifically focusing on under-studied entities and present AWARE strategies to facilitate bias-aware training and evaluation of link prediction algorithms. These strategies can help researchers gain better insights from computational evaluations and promote the development of new algorithms focusing on novel findings and under-studied proteins."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Joseph Szymborski","Title":"Protein Large Language Models are Effective, Generalized Protein-Protein Interaction Predictors","Abstract":"Advancements in large language model (LLM) training have led to their widespread use across various applications, including predicting protein secondary structure and function using protein LLMs (pLLMs). Few studies characterize the suitability of pLLMs for PPI inference and none yet have investigated the role of data leakage. Training models for PPI prediction poses challenges related to data leakage and generalization, and due to pre-trained pLLMs frequently containing proteins found in testing set's PPI pairs, pLLMs are a potential source of data leakage. This study evaluates pLLM suitability for PPI inference, focusing on data leakage and optimal architectural considerations for accuracy._x000D_\n_x000D_\nTo test hypotheses regarding pLLMs and their effects on hyperparameters, we trained numerous pLLMs using an efficient transformer architecture (SqueezeBERT) and a high-quality dataset of amino acid sequences (SWISS-PROT), significantly improving the problemâ€™s tractability by reducing both training time and equipment costs. We tested two datasets, \"\"strict\"\" and \"\"non-strict\"\", to investigate the role of data leakage in the performance of pLLM-based PPI inference models. The \"\"strict\"\" dataset excluded proteins found in the testing set of our PPI dataset, while the \"\"non-strict\"\" dataset had no such restrictions. Our experiments revealed that data leakage plays a negligible role in performance and may be mitigated by tuning architecture._x000D_\n_x000D_\nOur benchmarking of various pre-trained pLLMs against previous state-of-the-art methods, which do not use pLLMs, showed that all pLLM-based methods outperformed traditional PPI inference methods. Together, our results demonstrate for the first time that pLLM embeddings are both generalized and effective features for PPI inference."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Iker Núñez Carpintero","Title":"Addressing data scarcity in biomedical research using Multilayer Networks","Abstract":"Realization of the new paradigm brought by Precision Medicine heavily relies on the development of integrative and cost-effective methodologies for analyzing biomedical data. However, its application to rare diseases and precision oncology faces a major challenge: data scarcity._x000D_\n_x000D_\nIn order to solve data-scarce biomedical scenarios, it is crucial to explore the potential of integrating complementary biomedical knowledge with patient-specific data. Here, we illustrate how network biology, and in particular, multilayer network models, can play a pivotal role in addressing this challenge provided its advantages in molecular interpretability. _x000D_\n_x000D_\nWe delve into three distinct applications of multilayer network models for the analysis of biomedical scenarios heavily impacted by data scarcity: The first article  presents a personalized medicine study of the molecular determinants of severity in congenital myasthenic syndromes (CMS), a group of rare disorders of the neuromuscular junction (NMJ), recently published (Feburary 2024) in Nature Communications. _x000D_\nIn the second publication, our focus shifts to the application of multilayer networks in prioritizing genes in medulloblastoma, a childhood brain tumor. This extends and refines concepts introduced in our previous research. _x000D_\nLastly, we present ongoing efforts aimed at applying the developed methodologies to provide a comprehensive molecular understanding of the existing subtypes of rare PIK3CA\/TEK vascular malformations._x000D_\n_x000D_\nThis work presents major advances on the use of multilayer network-based approaches for the application of precision medicine to data-scarce scenarios, exploring the potential of integrating extensive available biomedical knowledge with patient-specific data."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Milan Picard","Title":"Target repositioning using multi-layer networks and machine learning: the case of prostate cancer","Abstract":"The discovery of novel drug targets typically represents the first and most important step of drug discovery. One solution for target discovery is target repositioning, a strategy which relies on the repurposing of known targets for new diseases, leading to new treatments, less side effects and potential drug synergies. Biological networks have emerged as powerful tools for integrating heterogeneous data and facilitating the prediction of biological or therapeutic properties. Consequently, they are widely employed to predict new therapeutic targets by characterizing potential candidates, often based on their interactions within a Protein-Protein Interaction (PPI) network, and their proximity to genes associated with the disease. However, over-reliance on PPI networks and the assumption that potential targets are necessarily near known genes can introduce biases that may limit their effectiveness. We propose to address these limitations in two ways. First, by creating a multi-layer network which incorporates additional information such as gene regulation, metabolite interactions, metabolic pathways, and several disease signatures. Second, several network-based approaches were exploited including proximity to disease-associated genes, but also unbiased approaches such as propagation-based methods, topological metrics, and module detection algorithms. Using prostate cancer as a case study, each approach extracted relevant features from the network and their predictive power were evaluated independently. Using the best features identified machine learning algorithms were exploited to predict novel promising therapeutic targets for prostate cancer."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Dongmin Bang","Title":"Transfer Learning of Condition-Specific Perturbation in Gene Interactions Improves Drug Response Prediction","Abstract":"Drug response is conventionally measured at the cell level, often quantified by metrics like IC50. However, to gain a deeper understanding of drug response, cellular outcomes need to be understood in terms of pathway perturbation. This perspective leads us to recognize a challenge posed by the gap between two widely used large-scale databases, LINCS L1000 and GDSC, measuring drug response at different levels â€“ L1000 captures information at the gene expression level, while GDSC operates at the cell line level. Our study aims to bridge this gap by integrating the two databases through transfer learning, focusing on condition-specific perturbations in gene interactions from L1000 to interpret drug response integrating both gene and cell levels in GDSC. This transfer learning strategy involves pretraining on the transcriptomic-level L1000 dataset, with parameter-frozen fine-tuning to cell line-level drug response. Our novel Condition-Specific Gene-Gene Attention (CSG2A) mechanism dynamically learns gene interactions specific to input conditions, guided by both data and biological network priors. The CSG2A network, equipped with transfer learning strategy, achieves state-of-the-art performance in cell line-level drug response prediction. In two case studies, well-known mechanisms of drugs are well represented in both the learned gene-gene attention and the predicted transcriptomic profiles. This alignment supports the modeling power in terms of interpretability and biological relevance. Furthermore, our model's unique capacity to capture drug response in terms of both pathway perturbation and cell viability extends predictions to the patient level using TCGA data, demonstrating its expressive power obtained from both gene and cell levels."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Rachel Melamed","Title":"Draphnet: Learning the drug and phenotype network linking drug effects to disease genetics","Abstract":"Medications can have unexpected effects on disease, including not only harmful drug side effects, but also beneficial drug repurposing. These effects on disease may result from hidden influences of drugs on disease networks. Discovering how biological effects of drugs relate to disease biology can both provide insight into mechanism of latent drug effects, and can help predict new effects. _x000D_\nHere, we aim to learn how drug impact on disease can be explained by the relationship between 1) biological effects of the drug and 2) genetic alterations driving disease. We propose that simple linear models connecting a drug's molecular effects to a disease's genetic drivers can explain the drug's effect on phenotype. Our design learns a network linking the biological processes altered by each drug to the gene drivers of phenotype risk, where the model is trained to predict the (incomplete) matrix of drug-phenotype association from SIDER. To estimate this interaction network, we take a supervised approach, training the model based on known drug impacts on disease. By simultaneously training this model to predict relationships between tens of thousands of drug-disease pairs in a multitask fashion, we aim to learn an interpretable network connecting drugs to phenotypes. We call this method Draphnet, or Drug and Phenotype Network._x000D_\n_x000D_\nThe network's interpretability provides a rationale for predictions, increasing confidence in these predictions. As well, this model can provide testable hypotheses for future analysis of drug-disease biology. Finally, it can allow a new classification of drugs based on their downstream effects on disease biology."},{"Track":"NetBio","Room":"520c","Weekday":"Monday","Date":"15 July","Timespan":"17:20-18:00","Format":"In Person","Speaker":"Patrick Aloy","Title":"Blending Biology, Chemistry and AI through network embeddings","Abstract":"Biological data is accumulating at an unprecedented rate, escalating the role of data-driven methods in computational drug discovery. The urge to couple biological data to cutting-edge machine learning has_x000D_\nspurred developments in data integration and knowledge representation, especially in the form of heterogeneous, multiplex and semantically-rich biological networks. Today, thanks to the propitious rise in knowledge embedding techniques, these large and complex biological networks can be converted to a vector format that suits the majority of machine learning implementations. In this computational framework, complex connections between entities can be unveiled by means of simple arithmetic operations. Indeed, we demonstrate and experimentally validate that these descriptors can be used to reverse and mimic biological signatures of disease models and genetic perturbations in vitro and in vivo. However, only a tiny_x000D_\nfraction of the possible chemical space has been so far explored, meaning that most compounds able to modulate biological activities (i.e. drugs) are yet to be discovered. Thus, we are currently developing_x000D_\nstrategies to couple bioactivity signatures and inverse design algorithms to generate new chemical entities with a desired functionality."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-10:50","Format":"In Person","Speaker":null,"Title":" Opening Remarks","Abstract":null},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"10:50-11:10","Format":"Live Stream","Speaker":"Aisha Montgomery","Title":"AI\/ML to Identify and Stratify Non-Clinical Factors Contributing to Cancer Health Disparity in Rural Appalachia","Abstract":"Introduction: In the medically underserved area of rural Appalachia, cancer mortality rates are 32% higher than the US average. Social determinants of health (SDOH) are known barriers to cancer care in rural regions, however, their importance is not well demonstrated in AI\/ML._x000D_\n _x000D_\nBackground: In our pilot study, cancer registry data was used to build an ML model to predict 5-year colorectal cancer (CRC) survival in Appalachians. Analyses showed that the model was less accurate in predicting survival in Appalachian vs. non-Appalachian patients. Limited data from underserved populations in public datasets may increase biases and using only clinical factors may decrease predictability of AI\/ML methods. These findings led to the current project._x000D_\n_x000D_\nMethods: We hypothesized that SDOH factors were important to cancer survival in Appalachia and SDOH data features would improve ML model performance. A combined EHR dataset was created from community-based cancer centers in Appalachia which included both clinical and SDOH data features. SDOH features were stratified and added to the ML model to evaluate their effect. _x000D_\n_x000D_\nResults: Patients were average age 67Â±13.2 years, 49% female, and 66% rural. Stratification identified marital, employment, and insurance statuses as SDOH features with the highest impact on model output. Combining clinical and SDOH features in the ML model increased the area under the receiver operating curve (0.791) as compared to using clinical (0.758) or SDOH (0.662) features alone. _x000D_\n_x000D_\nDiscussion: These findings demonstrate the importance of SDOH factors on health outcomes in an underserved population. Further, the data methods highlight the need for diverse, community based EHR datasets in AI\/ML research. We built an ML model that can be used to improve cancer-related health disparities within rural and other medically underserved populations. Expansion of the current work will contribute to best practices in the creation of diverse, representative clinical and SDOH datasets to improve AI\/ML-based outcomes. _x000D_\n"},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"10:50-11:10","Format":"In Person","Speaker":"Jeremias Sulam","Title":"Estimating and Controlling for Fairness in Radiology with Missing Sensitive Information","Abstract":"As the use of machine learning models in real world high-stakes decision settings continues to grow, it is highly important that we are able to audit and control for any potential fairness violations these models may exhibit towards certain groups. For example, in automated screening protocols in radiology, one may wish to certify that a predictor achieves comparable performance for different demographic groups. To do so, one naturally requires access to these sensitive attributes, such as demographics, biological sex, or other potentially sensitive features that determine group membership. Unfortunately, in many settings, this information is often unavailable either because of inadequacies of existing datasets, or because of legal and privacy constraints. In this presentation, we will focus on the well-known equalized odds (EOD) definition of fairness. In a setting without sensitive attributes, we will show how to provide tight and computable upper bounds for the EOD violation of a predictor, thus being able to guarantee fairness in such missing-data scenarios. Second, we demonstrate how one can provably control the worst-case EOD by a new post-processing correction method. Our results characterize when directly controlling for EOD with respect to the predicted sensitive attributes is--and when is not--optimal when it comes to controlling worst-case EOD. Our results hold under assumptions that are milder than previous works, and we illustrate these results with experiments on synthetic and real datasets, including on chest radiographs."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"11:10-11:30","Format":"In Person","Speaker":"Rina Khan","Title":"Ethical Development of Imaging Biomarkers for Colorectal Biomarkers","Abstract":null},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"11:10-11:30","Format":"Live Stream","Speaker":"Deborah Mudali","Title":"Examining how social and behavioral determiants affect the prevalence, severity, and outcomes of Long-COVID-19 and health disparity","Abstract":"This research examines how social and behavioral determinants affect the prevalence, severity, and outcomes of Long-COVID-19, and their role in worsening health disparities among affected populations. Previous studies have shown that Long-COVID-19 disproportionately impacts various demographic groups, deepening existing disparities. By applying advanced computational techniques, this study aims to identify relevant variables and analyze their associations with Long-COVID-19 using statistical, machine learning, and deep learning models. Using OCHIN datasets, the study focuses on socioeconomic status, race\/ethnicity, education, healthcare access, and substance abuse._x000D_\nThe methods involve data collection and preprocessing, feature selection using R and Python tools, and model development employing logistic regression, decision trees, random forests, gradient boosting, neural networks, and principal component regression. The models are evaluated and validated using metrics like accuracy, precision, recall, and F1-score, with cross-validation techniques ensuring generalizability and robustness. The analysis aims to uncover complex relationships between the social determinants and Long-COVID-19 outcomes, contributing to understanding health disparities._x000D_\nPreliminary results indicate significant disparities in Long-COVID-19 outcomes based on demographic factors. The bimodal distribution in the density plot suggests generational differences in diagnosis, while boxplots indicate age-related trends within racial groups. Scatter plots reveal age-related patterns relevant to Long-COVID incidence or severity. Bar plots highlight unequal representation of racial groups, pointing to potential disparities in healthcare access or exposure risk. These visualizations suggest that certain racial and demographic groups might be disproportionately affected by Long-COVID-19, leading us to the need for targeted interventions._x000D_\nThis study aims to provide insights into health disparities associated with Long-COVID-19 to promote equitable healthcare strategies and reduce disparities across diverse populations. By leveraging advanced analytical techniques, it seeks to inform public health policies and resource allocation, improving healthcare outcomes for all demographic groups affected by Long-COVID-19._x000D_\nKeywords: Long-CoVID-19; Social and Behavioral Determinants; Health Disparity; Machine Learning_x000D_\n"},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"11:30-11:50","Format":"In Person","Speaker":"Bankole Olatosi","Title":"An Ethical Framework-Guided Tool for Assessing Bias in EHR based Big Data Studies ","Abstract":"Background: Current literature describes bias using electronic health records (EHR) for data science research. An important but under-researched ethical issue is the risk of potential biases prevalent in healthcare datasets (e.g., electronic health records [EHR] data) during data curation, acquisition, and processing cycles. To advance our understanding of bias and equity issues in data science applied to EHR data, we developed an ethical framework to guide data scientists using EHR data. This framework articulates the ethically meaningful biases in large EHR data studies affecting People with HIV (PWH), how these biases intersect in reproducing bias in HIV-related studies, and the strategies needed to break this vicious cycle.   _x000D_\nMethod: The ethical framework development was implemented through an iterative process comprised of literature\/policy review, content analysis, and interdisciplinary dialogues and discussion. We interviewed data curators, end-user researchers, healthcare workers, government agencies, and patient representatives throughout all iterative cycles using various formats, including in-depth interviews of 20 key stakeholders, and conducted panel discussions in a conference and a charette workshop._x000D_\nResults: The draft ethical framework was designed to be fair and transparent. It includes meaningful biases representing statistical\/computational biases, social biases (e.g., interpersonal bias, institutional bias, structural bias), and representativeness biases (e.g., underrepresented in the EHR data due to care access, affordability, availability, and acceptability). _x000D_\nConclusion: The developed framework illustrates the actions and steps that healthcare providers, health systems, data scientists, and researchers can collectively take to reduce opportunities that cumulatively work to produce and reproduce biases within EHR data and the resulting data science research products\/interventions. Interdisciplinary collaboration within the public health research area and intersectional efforts across government and the healthcare system in policies, capacity building, and patient engagement\/involvement are needed to manage and address the biases and protect patients from the threats of unfairness and inequality during data science research.    _x000D_\n"},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"11:30-11:50","Format":"In Person","Speaker":"Victor Nwankwo","Title":"Modern, Intuitive Tools for Managing AI\/ML Data in Health Equity-Focused Multiomic and Population Studies","Abstract":"The evolving use of AI and ML in multiomic and population-wide studies presents researchers with the challenge of managing vast and complex datasets. This necessitates modern tools that streamline data management and enhance interpretability. We model a scalable framework for AI\/ML-driven studies that integrates heterogeneous data across various study types, emphasizing translational strategies for health equity. This pipeline incorporates robust information visualization technology packages, offering a zoomable interface to coherently display diverse data types of varied scopes. By utilizing dynamic visualization frameworks and real-time interactive tools, our solution addresses critical gaps in data exploration and decision-making processes. This integration fosters a cohesive understanding of complex research datasets, facilitates hypothesis generation, and accelerates clinical and translational research outcomes, particularly those aimed at reducing health disparities."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"11:50-12:00","Format":"In Person","Speaker":"Senait Tekle","Title":"Use Explainable AI to Improve the Trust of and Detect the Bias of AI Models","Abstract":"Background: Artificial Intelligence (AI) presents promising advancements to improve healthcare outcomes, yet it also raises new ethical concerns. AI systems trained on biased data can perpetuate discrimination against disadvantaged patients, based on factors such as race, gender, or socioeconomic status. Using Explainable AI to describe AI models is expected to enable detection and correction of biases, thereby enhancing the confidence and trustworthiness of AI models._x000D_\n_x000D_\nObjectives: To use Explainable AI to improve trust in AI models and detect bias. _x000D_\n_x000D_\nMethods: Guided by the National Institute of Standards and Technology (NIST) framework on trustworthy AI, we conducted virtual interviews with clinicians, patients, IT and ethics experts, healthcare administrators, and policymakers. Questions included inquiries regarding participantsâ€™ general understanding of AI, perceptions of bias, levels of trust, and familiarity and thoughts on Explainable AI._x000D_\n_x000D_\nResults: Study participants (N=17); 53% White; 35% Asian; 12% African American; 41% female; 59% male. Participants felt Explainable AI provides valuable assistance, giving them a deeper understanding of the decision-making process and boosting confidence in the fairness and reliability of the AI system's output. Many respondents emphasize the importance of understanding the reasoning behind an AI system's decision-making process, particularly in clinical decision-making settings. They believe that transparency and comprehensibility are crucial for building trust and confidence in AI. Many respondents emphasize the need for clear explanations of how AI arrives at its decisions._x000D_\n_x000D_\nConclusions: Our findings underscore the importance of transparency and comprehensibility in AI systems, emphasizing their role in building trust and confidence among users. Additionally, they highlight the critical need for Explainable AI methods, particularly in sensitive domains like clinical decision-making, to ensure accountability and mitigate biases._x000D_\n_x000D_\nSignificance: Our study highlights the importance of ethical data science and the role of Explainable AI in enhancing trust, transparency, and detecting bias within AI systems, particularly in high-impact domains._x000D_\n"},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Discussion","Abstract":null},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Toru Shirakawa","Title":"Deep LTMLE: Scalable Causal Survival Analysis with Transformer","Abstract":"Causal inference under dynamic interventions from longitudinal data with high dimensional variables such as omics and images which potentially vary across time is a central problem in precision medicine. We developed and implemented Deep Longitudinal Targeted Minimum Loss-based Estimation (Deep LTMLE), a novel approach to address this problem (Shirakawa et al. 2024). Following the roadmap of causal inference (Petersen and van der Laan 2014), Deep LTMLE provides an efficient estimator for a class of functionals identified through the g-formula (Robins 1986) in continuous time with monitoring process, right-censoring, and competing risks. Our architecture used Transformer to handle long-range dependencies and heterogenous variables. The method is based on the theory of causal survival analysis (Rytgaard et al. 2022) combined with a scalable architecture of deep neural networks, bridging traditional biomedical statistics and emerging methods in data science. Our method could incorporate high dimensional variables such as omics, texts, images, and videos, and thus could integrate the data from molecular biology and clinical practice to evaluate their causal impacts in relation to clinically significant events such as patientsâ€™ survival. This feature would foster both translational and reverse-translational research. Within the framework of targeted learning (van der Laan and Rose 2011, 2018), we corrected the bias commonly associated with machine learning algorithms and built an asymptotically efficient estimator. In a simulation with a simple synthetic data, Deep LTMLE demonstrated comparable statistical performance and superior computational performance to an asymptotically efficient estimator, LTMLE with a super learner of multiple machine learning algorithms. As the complexities of the synthetic data and the length of time horizon increase, Deep LTMLE tended to outperform LTMLE. Furthermore, Deep LTMLE is implemented in Python and scalable with more computational resources such as graphics processing units (GPUs). We will demonstrate an application of Deep LTMLE with real world data."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Yanbin Yin","Title":"Cloud exploration and AI\/ML-readiness of CAZyme annotation in human gut microbiome","Abstract":"We developed dbCAN as a software system in 2012 and actively maintain it for automated CAZyme (carbohydrate-active enzymes ) annotation. With an R01 award, we have developed dbCAN-seq and dbCAN3, two new web-based bioinformatics tools, to enable dietary fiber utilization prediction given any human gut microbiome sequencing data as queries to assist the development of personalized nutrition. Funded by the NIH-ODSS cloud exploration grant, we have deployed the dbCAN3 web server on Amazon Web Services (AWS) to offer competitive performance, especially when scaling up to handle more job submissions. We also compared the AWS solution with our on-prem solution, which uses a standalone desktop server. Also funded by the NIH-ODSS AI\/ML-readiness grant, we have converted ~250k CAZyme gene clusters (CGCs) of dbCAN-seq identified from ~10k metagenome assembled genomes (MAGs) into a ML\/AI ready vector representations using word2vec. These unsupervised CGC data (without substrate labels) are further used to generate embeddings of supervised data (401 CGCs with substrate labels). An Recurrent Neural Network (RNN) based Multiclass Classification Model is built to allow prediction of glycan substrates for CGCs. Overall, our CAZyme bioinformatics research has benefited from the NIH-ODSS support, which helped us get one step closer to the microbiome-based personalized recommendation of dietary fiber intake._x000D_\n"},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Matthew Goodwin","Title":"Wearable Biosensing to Predict Imminent Aggressive Behavior in Psychiatric Inpatient Youths with Autism","Abstract":"Aggressive behavior is a prevalent and challenging issue in individuals with autism, especially for those who have limited verbal ability and intellectual impairments. This presentation investigates whether changes in peripheral physiology recorded by a wearable biosensor and machine learning can be used to predict imminent aggressive behavior before it occurs in inpatient youths with autism from 4 primary care psychiatric inpatient hospitals. Research staff performed live behavioral coding of aggressive behavior while 70 inpatient study participants wore a commercially available biosensor that recorded peripheral physiological signals (cardiovascular activity, electrodermal activity, and motion). Logistic regression, support vector machines, neural networks, and domain adaptation were used to analyze time-series features extracted from biosensor data. Area under the receiver operating characteristic curve (AUROC) values were used to evaluate the performance of population- and person-dependent models. A total of 429 naturalistic observational coding sessions were recorded, totaling 497 hours, wherein 6665 aggressive behaviors were documented, including self-injury (3983 behaviors [59.8%]), emotion dysregulation (2063 behaviors [31.0%]), and aggression toward others (619 behaviors [9.3%]). Logistic regression was the best-performing overall classifier across all experiments; for example, it predicted aggressive behavior 3 minutes before onset with a mean AUROC of 0.80 (95% CI, 0.79-0.81). Further research will explore clinical implications and the potential for personalized interventions."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Mitzi Morris","Title":"Improve speed, scalability, inter-operability of core C++ modules for Stan - a tool doing Bayesian modeling and statistical inference","Abstract":"Software Engineering for Research Software_x000D_\n_x000D_\nStan is a tool doing Bayesian modeling and statistical inference.  In September 2021, the Stan project received an award from program NOT-OD-21-091 to improve speed, scalability, inter-operability of core C++ modules. The Stan developer process was used for this work and subsequent development initiatives._x000D_\n_x000D_\nReproducible science requires reliable tools.  Rapid science requires tools which are easy to learn and use.  Reliability is enforced through design, code review, and extensive test suites.  Good documentation supports learnability and usability. These activities  inform and reinforce one another;  writing good tests leads to more informative docs and vice versa._x000D_\n_x000D_\nThese activities require input from researchers, developers, and end-users.  The Stan project has been very much a collaboration between computer scientists and applied statisticians.  This talk will examine the Stan developer community as well as the Stan developer process."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Nomi L Harris","Title":"LinkML: A FAIR data modeling framework for the biosciences and beyond","Abstract":"Open science depends on open data. LinkML (Linked data Modeling Language; linkml.io) is an open, extensible modeling framework that makes it easy to model, validate, and distribute reusable, interoperable data._x000D_\nThe quantity and variety of data being generated in scientific fields is increasing rapidly, but is often captured in unstructured, unstandardized formats like publications, lab notebooks, or spreadsheets. Many data standards are defined in isolation, causing siloization; lack of data harmonization limits reusability and cross-disciplinary applications. A confusing landscape of schemas, standards, and tools leaves researchers struggling with collecting, managing, and analyzing data._x000D_\nLinkML addresses these issues, weaving together elements of the Semantic Web with aspects of conventional modeling languages to provide a pragmatic way to work with a broad range of data types, maximizing interoperability and computability across sources and domains. LinkML supports all steps of the data analysis workflow: data generation, submission, cleaning, annotation, integration, and dissemination. It enables even non-developers to create data models that are understandable and usable across the layers from data stores to user interfaces, reducing translation issues and increasing efficiency._x000D_\nProjects across the biomedical spectrum and beyond are using LinkML to model their data, including the NCATS Biomedical Data Translator, Alliance of Genome Resources, Bridge2AI, Neurodata Without Borders, Reactome, Critical Path Institute, iSample, National Microbiome Data Collaborative, Center for Cancer Data Harmonization, INCLUDE project, Open Microscopy Environment, and Genomics Standards Consortium. _x000D_\nUltimately, LinkML democratizes data, helping to bridge the gap between people of diverse expertise and enabling a shared language with which to express the critically important blueprints of each projectâ€™s data collection._x000D_\n_x000D_\nThis work is supported in part by an NIH supplement under NOT-OD-22-068, and by the Genomic Science Program in the U.S. Department of Energy, Office of Science, Office of Biological and Environmental Research (BER) under contract number DE-AC02-05CH11231."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Robert Leaman","Title":"PubTator 3.0: an AI-powered Literature Resource for Unlocking Biomedical Knowledge","Abstract":"PubTator 3.0 (https:\/\/www.ncbi.nlm.nih.gov\/research\/pubtator3\/) is an advanced biomedical literature resource featuring search capabilities enabled with state-of-the-art AI methods. It extracts six key types of biomedical entities (genes, diseases, chemicals, genetic variants, species, and cell lines) and 12 common types of relationships between entities._x000D_\nPubTator 3.0â€™s online interface facilitates literature exploration through semantic, relation, keyword, and Boolean queries. Search results are prioritized based on the depth of the relationship between the query terms in each article and the importance of the section where these matches appear. PubTator 3.0 provides a single, unified, search system across both abstracts and full text, comprising approximately 36 million PubMed abstracts and over 6 million full-text articles from the PMC Open Access Subset._x000D_\nPubTator 3.0 utilizes deep-learning transformer models for named entity recognition and relation extraction. These models, AIONER and BioREx, were recently developed with computational resource support from ODSS. It currently contains 1.6 billion entity annotations and 33 million relationship annotations, with new articles added weekly. PubTator 3.0 offers programmatic access through its API and bulk download._x000D_\nCompared to its predecessor, PubTator 3.0 exhibits enhanced entity recognition and normalization performance. Its new relation extraction feature shows substantially higher performance than previous state-of-the-art systems. PubTator 3.0 retrieves a greater number of articles for entity pair queries than either PubMed or Google Scholar, with higher precision in the top 20 results. Integrating ChatGPT (GPT-4) with the PubTator APIs dramatically improves the factuality and verifiability of its responses._x000D_\nPrevious versions of PubTator have supported a wide range of research applications, fulfilling over one billion API requests. With an improved and expanded set of features and tools, PubTator 3.0 is designed to allow researchers to navigate the ever-expanding wealth of biomedical literature, expediting research and unlocking valuable insights for scientific discovery."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Nancy Li","Title":"Leveraging Language Models for Enhanced Biocuration and User Interaction in Reactome: A Pathway Towards Community-Driven Knowledge Enrichment","Abstract":"The Reactome Pathway Knowledgebase, supported by NIH NHGRI and ODSS, stands as a cornerstone database renowned for its meticulous human curation practices. Reactome is currently the most comprehensive open-source, human biological pathway knowledgebase. Reactome's curation process faces inherent challenges in handling the vastness and complexity of biological data. Human-driven curation struggles with scale and efficiency. Therefore, we are developing a web-based curation tool that incorporates Large Language Models (LLM) technology, guided by Reactomeâ€™s data schema and curation requirements, to significantly improve steps that are major bottlenecks for the curation workflow. To do so, we adopted the RAG (retrieval-augmented generation) technology and developed an API to associate previously unannotated genes with Reactome pathways. Leveraging our prior work with the NIH IDG program, the API can find potential pathways for a query gene, search PubMed for supportive literature evidence, create text summaries, and extract functional relationships between the query gene and biological concepts._x000D_\n_x000D_\nIn addition, Reactome is developing a conversational chatbot that facilitates user interactions toward improving comprehension of Reactome content. The chatbot is designed to facilitate a natural, interactive user experience, and more intuitive navigation through Reactome's extensive database. This interface will allow users to query complex pathway information and receive rich, informative responses that encourage deeper engagement with the knowledgebase. Future endeavors involve integrating LLMs into the chatbot for data analysis, empowering users with diverse technical backgrounds to perform sophisticated analyses using Reactome's. Integration of multi-source data retrieval systems and the incorporation of gene analysis tools are expected to enhance the platform's utility and interactivity, thereby streamlining the user experience and facilitating the exploration and understanding of complex biological datasets. Our LLM-focused approaches will improve user engagement and also lays the groundwork for improved curation workflows within Reactome, potentially offering a means for community curation practices. "},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Trey Saddler","Title":"ToxPipe: Harnessing AI and Cloud Computing for Toxicological Data Exploration and Interpretation  ","Abstract":"ToxPipe is an innovative platform that harnesses expert entrained AI and cloud computing to revolutionize the exploration and interpretation of diverse toxicological data streams. By leveraging large language models (LLMs), AI automation platforms like Auto-GPT, and hybrid deployment approaches, ToxPipe enables semi-autonomous data integration, toxicological characterization, and knowledge discovery through natural language instructions._x000D_\n_x000D_\nThe platform offers a user-friendly web application and API, allowing scientists and toxicologists to interact with AI agents and explore various toxicologically relevant datasets. ToxPipe's open-source codebase provides opportunities for customization and integration with additional data sources._x000D_\n_x000D_\nWe present the design, implementation, and evaluation of ToxPipeâ€™s outputs, showcasing its capabilities in tasks such as gene expression interpretation, toxicological data summarization, and exploration of disparate datasets through text-to-SQL queries, among other capabilities. Performance is evaluated using objective statistical metrics compared to expert assessment. _x000D_\n_x000D_\nToxPipe incorporates advanced tools and libraries, including promptfoo and ragas for automated evaluation of LLM prompt and retrieval-augmented generation (RAG) pipelines, and Langchain and Llamaindex for agent orchestration and data ingestion. Documents like National Toxicology Program Technical Reports are parsed and converted to vector embeddings, enabling semantic similarity search for user and agent queries. ToxPipe also leverages the data contained in ChemBioTox, a Postgres database that aims to compile toxicologically relevant information across a variety of publicly available data sources and QSAR models for over 1 million chemicals and their metabolites, to provide expert-curated annotations._x000D_\n_x000D_\nThrough expert entrainment, ToxPipe generates context to guide users in arriving at integrative interpretation from diverse data streams. The ToxPipe interface offers ready access to powerful agentic discovery tools and is poised to significantly accelerate toxicological research and discovery."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-16:00","Format":"In Person","Speaker":null,"Title":"Discussion","Abstract":null},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Luciano Nocera","Title":"Multi-Context Graph Neural Networks for Enhanced Multivariate Time-Series Analysis in Healthcare","Abstract":"Effective multivariate time-series (MTS) analysis in healthcare is crucial for various medical tasks and requires capturing complex inter-variable relationships accurately. Previous methods often fail to model these relationships adequately, leading to poor predictions, especially when training data size is limited. Our research introduces a series of Graph Neural Networks (GNNs) designed to overcome these limitations by explicitly modeling MTS as graphs, where each variable is a node connected by edges representing multi-context inter-variable relationships._x000D_\n_x000D_\nOur first work, Busyness Graph Neural Network (BysGNN), is a temporal GNN that integrates semantical, temporal, spatial, and taxonomical data to model interactions between Points of Interest (POIs). This model was particularly effective during the COVID-19 pandemic for forecasting POI visits to set occupancy restrictions in urban settings, demonstrating substantial improvements over previous forecasting models by capturing complex multi-context correlations._x000D_\n_x000D_\nBuilding upon BysGNN, we developed NeuroGNN, which extends the graph-based approach to EEG data analysis. NeuroGNN dynamically constructs graphs that reflect the evolving relationships between EEG electrodes and associated brain regions, significantly enhancing seizure detection and classification. The modelâ€™s ability to incorporate diverse contextual data improves its capability to classify rare seizure types with limited samples, addressing a significant challenge in medical diagnostics._x000D_\n_x000D_\nOur latest work, WaveGNN, further extends the application of GNNs by addressing irregular MTS data common in healthcare (e.g., unaligned and incomplete measurements of vital signs). WaveGNN integrates additional data modalities, including Electronic Health Record (EHR) notes, to build a robust graph representation that maintains accuracy nearly equivalent to scenarios with complete data, even in the presence of significant data gaps. This substantially improves over existing methods, which typically suffer performance degradation under similar conditions."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"Live Stream","Speaker":"Yasin Khadem Charvadeh","Title":"Clustering-Informed Shared-Structure Variational Autoencoder for Missing Data Imputation in Large-Scale Healthcare Data","Abstract":"Despite advancements in managing healthcare data, missing data challenges persist in Electronic Health Records (EHR) and patient-reported health data, compromising their usability in various healthcare analytics, including telehealth. As Artificial Intelligence (AI) modeling techniques evolve, conventional and contemporary methods for handling missing data encounter notable limitations that hinder their effectiveness. Established methods such as Multiple Imputation by Chained Equations (MICE), MissForest, and Generative Adversarial Imputation Nets (GAIN) demonstrate limitations in handling the complexities inherent in healthcare data. These challenges involve capturing complex non-linear relationships, extended computation times, and constraints in addressing various types of missing data mechanisms. In response, we propose a novel model building on the Variational Autoencoder (VAE) architecture, a powerful generative model using Bayesian neural networks. Our proposed method differs from existing VAE-based imputation strategies by providing a robust framework specifically designed for handling missing values within healthcare data. This framework can effectively accommodate various missing data mechanisms, including missing not at random (MNAR). By identifying missing data patterns and leveraging shared structures across VAEs for different patterns, our model effectively captures complex associations, thus enhancing generalizability and learning efficiency. Through comprehensive simulation studies, we showcase the adaptability of our approach across different missing mechanisms, demonstrating its superiority over traditional and popular imputation methods in terms of imputation accuracy. We apply our proposed method to EHR data from patients diagnosed with early-stage breast cancer who are at high risk of recurrence after surgery at Memorial Sloan Kettering Cancer Center, specifically among those treated with the FDA-approved drug abemaciclib, which necessitates routine blood monitoring at specific time intervals due to potential side effects. Given the variability in clinical practices for ordering these tests, our approach aims to mitigate the impact of missing data on patient health monitoring and subsequent analyses."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Ansaf Salleb-Aouissi","Title":"Predicting and Preventing Adverse Pregnancy Outcomes in Nulliparous Women","Abstract":null},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Juexin Wang ","Title":"SCH: Graph-based Spatial Transcriptomics Computational Methods in Kidney Diseases","Abstract":"_x000D_\n"},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Bengie L Ortiz","Title":" AI\/ML Ready mHealth and Wearable Data for Dyadic HCT","Abstract":"Hematopoietic cell transplantation (HCT) is a potent form of immunotherapy_x000D_\nfor high risk blood diseases. Given the high risk associated with HCT, a dedicated_x000D_\ncaregiver is necessary and expected for at least the first 100 days post transplant,_x000D_\nhowever HCT caregivers experience adverse physical and mental health during this_x000D_\nperiod. Dyadic methods to identify t he status of the physical and mental health of both_x000D_\ncaregivers and patients can potentially improve the intervention and support needed to_x000D_\nimprove the quality of life of both patients and caregivers. The goal of this research is to_x000D_\nbuild a high quality, co mprehensive, standardized, AI\/ML ready, and clinically_x000D_\nmeaningful mHealth dataset of HCT patients and their caregivers to develop novel_x000D_\ninterventions in HCT using mHealth and wearables. We have developed a novel_x000D_\npreprocessing pipeline to build the AI\/ML ready mHealth and wearable data which_x000D_\nincludes data extraction, data standardization, data cleaning, feature extraction, and_x000D_\ndata integration . The data is collected from Michigan Medicine from an existing mHealth_x000D_\nrandomized clinical trial from Septe mber 2020 to July 2023 for a total of 323 subjects_x000D_\n(166 adult patients and caregivers). This data consists of physiological variables_x000D_\ncaptured from Fitbit such as heart rate, steps, and sleep and survey, patient reported_x000D_\noutcomes, and mood data from the mo bile application. The data demographics include_x000D_\n69.6% females and 28% males for caregivers, however 66% males and 34% females_x000D_\nfor HCT patients. In terms of race, 88.3% are white caregivers and 86.8% are white_x000D_\npatients. There are a total of 417,003,290 steps, 3,075,687 sleep , 381,164 mood , and_x000D_\n86,057,538 heart rate observations in the available date. A n overall 45.67% of data is_x000D_\nduplicate with the maximum duplicate values in steps and heart rate data. The_x000D_\ngenerated AI\/ML ready mHealth data is unique in HCT domain that will h elp research_x000D_\ncommunity to validate novel hypothesis in HCT research."},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Alex Wagner","Title":"Enhancing the AI-readiness of gnomAD with GA4GH Genomic Knowledge Standards","Abstract":"The clinical interpretation of genomes is a labor-intensive process that remains a barrier to scalable genomic medicine. Efforts to improve this â€œinterpretation bottleneckâ€ have resulted in the development of clinical classification guidelines and databases for genomic variants in Mendelian diseases and cancers. The development of AI-augmented genome interpretation systems is a solution to scale the interpretation process, and relies upon expert-defined clinical interpretation frameworks developed by expert communities in clinical interpretation (e.g. ClinGen). Development of such interpretation systems will benefit from aggregation and collation of evidence that is in a computationally-described, AI-ready state. _x000D_\nThe NIH-supported Genome Aggregation Database (gnomAD) is currently the largest and most widely used public resource for population allele frequency data. These data are commonly used by variant interpretation frameworks as strong evidence against variant causality, making this a highly impactful resource for filtering out variants that are unlikely to be causative for Mendelian diseases or cancer development. The importance and scale of the gnomAD population allele frequency data to clinical interpretation systems makes this resource an ideal candidate resource for AI-ready data._x000D_\nUnder the auspices of the Global Alliance for Genomics and Health (GA4GH), we designed and applied standard data models for cohort allele frequency evidence in collaboration with the broader genomic knowledge community. We normalized the ~1.89 billion alleles of the gnomAD resource, following the conventions of the GA4GH Variation Representation Specification (VRS), providing globally unique computed identifiers that are accessible on the gnomAD Hail platform and the associated gnomAD Hail utilities. We also designed a GA4GH draft standard for cohort allele frequency data, and built an gnomAD API to apply these standardized data in genomic interpretation support systems. We conclude with an overview of this effort in the context of interoperability with other genomic evidence repositories using GA4GH genomic knowledge standards. _x000D_\n"},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":"Vida Abedi","Title":"Enhancing Imputation for Clinical Research: The Path for aÂ Flexible Toolkit","Abstract":"Background: Missing data in clinical research restricts robust analysis and AI\/ML model training. This project addresses this challenge by presenting a Python package for efficient and intelligent missing value imputation, designed specifically for clinical research data. Method: The algorithm provides a \"Flexible\" functionality; it dynamically selects the optimal imputation method for each variable based on the unique variableâ€™s characteristics and missing level. The presented algorithm facilitates the incorporation of complementary data sources for improved imputation, including Socioeconomic Determinants of Health (SDoH), medical history, family history, and biospecimen data, when available. The algorithm also allows creation of simulated patients to enhance data diversity, patient representation, and sample size. Finally, our algorithm provides both CPU and graphics processing unit (GPU) acceleration capabilities. Results: Our comprehensive evaluation using real-world clinical datasets from three distinct sources and simulated data demonstrates the superiority of the Flexible functionality implemented as part of our imputation algorithm. This study also corroborates that data distribution and skewness have an important effect on both algorithm and imputation performance; findings from this study highlight the importance of considering data normality for optimal imputation outcomes. Conclusion: Missing data in clinical research limits the AI\/ML readiness and can cause significant algorithmic bias due to bias introduced during model training. Improving imputation can improve our AI\/ML readiness and help improve data quality, diversity, and patient representation. "},{"Track":"NIH ODSS","Room":"520a","Weekday":"Saturday","Date":"13 July","Timespan":"17:40-18:00","Format":"In Person","Speaker":null,"Title":"Closing Remarks","Abstract":null},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Carl de Boer","Title":"Continual improvement of cis-regulatory models","Abstract":"Gene expression is regulated by transcription factors that work together to read cis-regulatory DNA sequences. A primary aim of my group is to decipher the â€œcis-regulatory codeâ€ - the rules that cells use to determine when, where, and how much genes should be expressed. While cis-regulation has proven to be exceedingly complex, recent advances in our ability to query the activity of DNA, combined with Machine Learning have enabled significant progress towards deciphering this code. Here, I will focus on several of our recent efforts to improve cis-regulatory models. First, I will describe a recent DREAM Challenge, where competitors from across the globe competed to create the best sequence-expression models using a dataset of random yeast promoter sequences and their experimentally determined expression levels, which resulted in state-of-the-art model architectures, even for human cis-regulatory data. Next, I will describe an ongoing effort to make cis-regulatory models and evaluation tasks interoperable, streamlining model evaluation and enabling model comparison. Then, I will describe an alternate strategy for dividing the genome into training and test datasets, which substantially mitigates the homology-driven data leakage common in genome-trained models. Finally, I will give a perspective on where the field needs to go to crack the cis-regulatory code. Namely, profiling the regulatory activities of non-genomic DNA sequences in very high-throughput, and using these data to train models that understand genome regulation without ever having seen genomic sequences."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Peter Koo","Title":"Interpreting Cis-Regulatory Interactions from Large-Scale Deep Neural Networks for Genomics","Abstract":"The rise of large-scale, sequence-based deep neural networks (DNNs) for predicting gene expression has introduced challenges in their evaluation and interpretation. Current evaluations align DNN predictions with experimental perturbation assays, which provide insights into the generalization capabilities within the studied loci but offer a limited perspective of what drives their predictions. Moreover, existing model explainability tools focus mainly on motif analysis, which becomes complex when interpreting longer sequences. Here we introduce CREME, an in silico perturbation toolkit that interrogates large-scale DNNs to uncover rules of gene regulation that it learns. Using CREME, we investigate Enformer, a prominent DNN in gene expression prediction, revealing cis-regulatory elements (CREs) that directly enhance or silence target genes. We explore the intricate complexity of higher-order CRE interactions, the relationship between CRE distance from the transcription start sites on gene expression, as well as the biochemical features of enhancers and silencers learned by Enformer. Moreover, we demonstrate the flexibility of CREME to efficiently uncover a higher-resolution view of functional sequence elements within CREs. This work demonstrates how CREME can be employed to translate the powerful predictions of large-scale DNNs to study open questions in gene regulation."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Melanie Weilert","Title":"Chromatin accessibility is driven by intra-nucleosomal pioneer cooperativity that includes low affinity motifs","Abstract":"The regulation of chromatin accessibility at cis-regulatory DNA sequences is a key rate-limiting step for enhancer activation and thus is an important element of the cis-regulatory code. Pioneer transcription factors (TFs) that induce nucleosome remodeling mediate chromatin opening, but the sequence rules by which pioneer or other TFs cooperate to make chromatin accessible are not well understood. To identify these sequence rules in an unbiased manner, we trained and interpreted BPNet-derived deep learning models that predict base-resolution TF binding data and bias-corrected chromatin accessibility data in mouse embryonic stem cells. By comparing the interpretations from both models, we can distinguish between TFs that are strong pioneers, weak pioneers and non-pioneers. Furthermore, we find that pioneering depends on low-affinity TF motifs, which increase in importance when they cooperate with other motifs. This reliance on cooperativity is observed to be important at low TF concentrations, when high-affinity motifs have a decreased effect on chromatin accessibility, confirming our model predictions. By probing the cooperativity in more detail, we find that it generally occurs at intra-nucleosomal distances, supporting a nucleosome-mediated mechanism of cooperativity. This highlights the ability of deep learning models to learn complex sequence rules, suggesting that widespread cooperativity and involvement of low affinity motifs could explain why the context-dependent function of pioneer TFs has been difficult to decipher."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Characterizing transcription factor binding with multi-omics sequence model","Abstract":"The linkage between transcription factors (TFs) and cis-regulatory regions (CREs) is crucial to under- standing gene regulation. Conventionally, it is determined by a step-wise processâ€”motif enrichment and correlation\/regression-based analysis. As the presence of motifs does not always imply binding, and cor- relation analysis may miss low-expression TFs, this process can suffer from false positive and negatives. Here we propose a holistic model that takes joint single-cell RNA sequencing (scRNA-seq) data and single-cell assay for transposase-accessible chromatin sequencing (scATAC-seq) data to delineate TF-CRE linkage. In- spired by multi-omics factor analysis and sequence modeling, our model decomposes peaksâ€™ accessibility into cell factors, encoded from TF expression, and peak factors, encoded from DNA sequences._x000D_\nWe demonstrate our model on an embryonic mouse brain dataset. Both modalities are accurately recon- structed on held-out cells and sequences . Cell factors preserve cell type distinction and trajectory structure, while sequence factors motifs moderately localize some motifs, such as that of Neurod2 and Sox11, indicating the regulatory information is captured._x000D_\nTo delineate TF-CRE linkage, we take gradients with respect to the two inputs. High gradient times TF expression values (gradTF) are assigned to high correlation TF-CREs pairs, whereas low-correlation, high gradTF pairs may correspond to low-expression TFs, though systematic evaluation remains to be done. As an example, Runx1, a low-expression TF, correlates poorly with almost all peaksâ€™ accessibility; however, its potential target CREs (compiled from ChIP-Atlas) have a higher absolute gradTF. On the other hand, gradient times sequence (gradSeq) highlights regulatory motifs."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Protein Language Models improve the target prediction of nucleic acid-binding proteins","Abstract":"Unraveling the DNA and RNA-binding preferences of regulatory proteins, like transcription and splicing factors, is important for understanding gene regulatory networks. In vitro binding assays, including Protein Binding Microarrays (PBMs) and RNAcompete, have been conducted for hundreds of nucleic-acid-binding proteins (NBPs) and provide training data for homology models that can predict the binding preferences of unmeasured NBPs. However, to date, these homology models have used simple rules to predict motifs._x000D_\n_x000D_\nProtein Language Models have emerged as effective models for downstream protein property prediction; however, their value in predicting protein-ligand interactions is less clear. To evaluate this, we extracted diverse NBP representations from four PLMs (AlphaFold2, AminoBERT, ESM-2, ProteinBERT) and compared their performance against baseline representations when used as inputs for different target prediction models, including unsupervised methods and neural networks._x000D_\n_x000D_\nExtensive evaluations across diverse datasets revealed that PLM-learned representations consistently outperformed baseline methods. Further analysis demonstrated the particular value of PLM-learned representations in scenarios on proteins with distant homologs. Feature attribution analyses demonstrated that PLM-learned representations capture global and local structural properties, showcasing their efficacy in predicting binding preferences._x000D_\n_x000D_\nESM-2 emerged as a top performer across predictive models. We further evaluated its performance in predicting the targets of unmeasured RBPs and unmeasured RNAs after fine-tuning. By introducing new tokens for nucleic acids and using concatenated RBP-RNA sequences as inputs, we demonstrated that a fine-tuned ESM-2 model matches the SOTA approach in generalizing to unseen RNA sequences and outperforms the SOTA approach in generalizing to unseen RBPs."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Pedro Tomaz da Silva","Title":"DNA language models reveal the architecture of nucleotide dependencies in genomes","Abstract":"While the genome is composed of individual nucleotides, functional elements such as cis-regulatory elements and structural interactions are formed from sets of interdependent nucleotides. In principle, these dependencies are reflected in coevolutionary relationships. However, their detection beyond coding sequences is challenging with classical approaches._x000D_\n_x000D_\nDNA language models (LMs), which are trained by predicting nucleotides given their sequence context, have recently been proposed as foundational models for sequence-based prediction problems. DNA LMs implicitly capture functional elements from genomic sequences alone. However, which dependencies DNA LMs learn and whether they reflect known or even novel biology remains an open question. _x000D_\n_x000D_\nHere we introduce nucleotide dependency maps to systematically study nucleotide dependencies captured by DNA LMs in a purely unsupervised setup._x000D_\nWe compute these maps genome-wide and show that they reveal and clearly delineate known functional genomic features such as transcription factor binding motifs, functional interactions between splice sites, RNA tertiary structures, and coding sequences. Additionally we uncover novel and conserved dependency structures suitable for experimental validation. _x000D_\n_x000D_\nWe furthermore investigate dependency maps from in silico manipulated sequences, revealing the ability of DNA LMs to capture operations such as copying and reverse complementarity without memorization. _x000D_\n_x000D_\nLastly, we compare dependency maps from openly available DNA LMs, showcasing the drawbacks and advantages of different models. We find stark differences in the ability of models to accurately learn conserved but infrequent features._x000D_\n_x000D_\nAltogether, by leveraging the flexibility of DNA language models, nucleotide dependency mapping emerges as a general methodology to discover and study functional interactions in genomes."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"LoopHunter: Enhancing Chromatin Loop Annotation by Focusing on Larger Regions in Hi-C Data","Abstract":"Chromatin loops, which bring distant loci into close contact, play a crucial role in gene expression and regulation. Although several methods have been developed for annotating loops from Hi-C contact maps, these methods remain unsatisfactory, particularly in accurately identifying loops from low coverage or single-cell Hi-C contact maps at high resolutions. Chromatin loops manifest as small blob-shaped patterns on Hi-C contact maps, encouraging existing tools to focus on analyzing contact pairs within small areas, such as a 21x21 window. However, these blob-shaped patterns are often indistinct in sparse regions, providing insufficient data for precise loop detection. Meanwhile, many chromatin loops exhibit broader patterns, including stripes, particularly in loops associated with the formation of Topologically Associating Domains (TADs), which current tools largely ignore. In this study, we introduce LoopHunter, a axial attention-based deep learning model to annotate loops from Hi-C contact maps at high resolutions across various coverages. LoopHunter utilizes a 224x224 sub-matrix as input and employs a combination of axial attention transformer and convolutional blocks to capture multi-scale data, facilitating robust loop prediction within the input region. Unlike traditional approaches that focus only on the center of the input matrix, we propose to train LoopHunter based on knowledge distillation, enabling it to make dense predictions. Our comparisons of LoopHunter against alternative tools demonstrate that LoopHunter significantly enhances loop annotation across both low and high coverage Hi-C contact maps."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"A systematic comparison of Machine learning methods for the prediction of enhancer-gene interactions from epigenomic data","Abstract":"Understanding the complex interaction between histone modifications, enhancers, and gene regulation is pivotal in deciphering the mechanisms governing cellular identity and function. This study investigates the critical task of predicting enhancer-gene interactions, essential for unraveling non-coding variation and DNA-binding factor-mediated gene regulation. Leveraging the comprehensive EpiATLAS dataset, encompassing high-quality histone ChIP-seq and RNA-seq data from a wide variety of cell types curated by IHEC, we embarked on a systematic comparison of various machine learning methods tailored to gene-specific prediction of gene expression from epigenome data._x000D_\nOur investigation extends beyond traditional approaches by incorporating the large EpiATLAS dataset and exploring different state-of-the-art Machine learning methods. Notably, we optimized novel Convolutional Neural Network (CNN), and Multi-Layer Perceptron (MLP) architectures, and Random Forest-based (RF) methods in comparison to established linear models. By harnessing H3K27ac histone mark signatures within megabase genomic windows surrounding each gene, our models, especially RF and CNN, demonstrated exceptional performance in predicting gene expression. Many different aspects of a gene, such as gene structure, and expression variance across cell types dictate the success of building an accurate model._x000D_\nThrough comprehensive validation using CRISPRi screens and eQTL data, we investigate the efficacy of the learned models in predicting enhancer-gene interactions using an in silico perturbation setup. _x000D_\nIn summary, our work offers a comprehensive framework for understanding enhancer-mediated gene regulation, supported by rigorous validation methods. These findings provide valuable insights into the regulatory landscape of the human genome, advancing our understanding of cellular function and disease mechanisms."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Q&A for Flash Talks","Abstract":null},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-15:00","Format":"In Person","Speaker":"Jian Ma","Title":"Integrative modeling of multiscale single-cell spatial epigenome","Abstract":"Despite significant advancements in high-throughput data acquisition in genomics and cell biology, our understanding of the diverse cell types within the human body remains limited. In particular, the principles governing intracellular molecular spatial organization and interaction, as well as cellular spatial organization within complex tissues, are still largely unclear. A major challenge lies in developing computational methods capable of integrating heterogeneous and multiscale molecular, cellular, and tissue information. In this talk, I will discuss our recent work on creating integrative approaches to advance regulatory genomics using single-cell spatial epigenomics. These methods hold the potential to reveal new insights into fundamental genome structure, gene regulation, and cellular function within complex tissues, across a wide range of biological contexts in both health and disease."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"William Noble","Title":"Enhancing Hi-C contact matrices for loop detection with Capricorn, a multi-view diffusion model","Abstract":"Motivation: High-resolution Hi-C contact matrices reveal the detailed three-dimensional architecture of the genome, but high-coverage experimental Hi-C data are expensive to generate. On the other hand, chromatin structure analyses struggle with extremely sparse contact matrices. To address this problem, computational methods to enhance low-coverage contact matrices have been developed, but existing methods are largely based on resolution enhancement methods for natural images and hence often employ models that do not distinguish between biologically meaningful contacts, such as loops, and other stochastic contacts._x000D_\n_x000D_\nResults: We present Capricorn, a machine learning model for Hi-C resolution enhancement that incorporates small-scale chromatin features as additional views of the input Hi-C contact matrix and leverages a diffusion probability model backbone to generate a high-coverage matrix. We show that Capricorn outperforms the state of the art in a cross-cell-line setting, improving on existing methods by 17% in mean squared error and 26% in F1 score for chromatin loop identification from the generated high-coverage data. We demonstrate that Capricorn performs well in the cross-chromosome setting and cross-chromosome, cross-cell-line setting. We further show that our multi-view idea can also be used to improve several existing methods, HiCARN and HiCNN, indicating the wide applicability of this approach. Finally, we use DNA sequence to validate discovered loops and find that the fraction of CTCF-supported loops from Capricorn is similar to those identified from the high-coverage data. Capricorn is a powerful Hi-C resolution enhancement method that enables scientists to find chromatin features that cannot be identified in the low-coverage contact matrix."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Gabriel Dolsten","Title":"Ultra-long-range and interchromosomal loops link T cell superenhancers","Abstract":"Functional enhancer-promoter interactions are typically thought to occur at distances less than two megabases. To explore the role of long-range regulatory interactions, we generated two Hi-C libraries in regulatory (Treg) and conventional (Tcon) CD4+ T cells. We found that interactions beyond 10Mb dramatically improved prediction of gene expression from Hi-C, suggesting that long-range interactions may play an important role in gene regulation. To analyze the role of long-range interactions, we examined differential contact frequency between Treg and Tcon genome-wide. This analysis revealed 78,089 differential interactions at distances greater than two megabases. Differential long-range contact was especially common at critical T cell genes regulated by superenhancers, such as Ikzf2. These interactions often presented as focal contacts (â€œmegaloopsâ€), such as the Treg-specific 9Mb megaloop between Ikzf2 and Ctla4. A second 18Mb megaloop between Ikzf2 and Arl4c was confirmed by DNA-FISH. We developed a novel algorithm and package, LONGSHOT, to find megaloops and identified 33,791 intrachromosomal and 23,003 interchromosomal megaloops in the T cell connectome. Clustering of megaloops revealed three distinct interchromosomal megaloop hubs. Two of the hubs were highly enriched for superenhancers, capturing 50% of all Treg cell superenhancers. Analysis of a published Hi-C dataset with an Ets1 superenhancer knockout revealed changes in megalooping after superenhancer knockout and changes in gene expression at the megalooped sites. Together, these results suggest that ultra-long-range chromatin contacts, partly mediated by superenhancers, are an important component of T cell gene regulation."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Ghulam Murtaza","Title":"scGrapHiC: Deep learning-based graph deconvolution for Hi-C using single cell gene expression","Abstract":"Single-cell Hi-C (scHi-C) protocol helps identify cell-type-specific chromatin interactions and sheds light on cell differentiation and disease progression. Despite providing crucial insights, scHi-C data is often underutilized due to the high cost and the complexity of the experimental protocol. We present a deep learning framework, scGrapHiC, that predicts pseudo-bulk scHi-C contact maps using pseudo-bulk scRNA-seq data. Specifically, scGrapHiC performs graph deconvolution to extract genome-wide single-cell interactions from a bulk Hi-C contact map using scRNA-seq as a guiding signal. Our evaluations show that scGrapHiC, trained on 7 cell-type co-assay datasets, outperforms typical sequence encoder approaches. For example, scGrapHiC achieves a substantial improvement of 23.2% in recovering cell-type-specific Topologically Associating Domains over the baselines. It also generalizes to unseen embryo and brain tissue samples. scGrapHiC is a novel method to generate cell-type-specific scHi-C contact maps using widely available genomic signals that enables the study of cell-type-specific chromatin interactions."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Emily Maciejewski","Title":"Cross-species and tissue imputation of species-level DNA methylation samples across mammalian species","Abstract":"DNA methylation data offers valuable insights into various aspects of mammalian biology. However, the availability of such data for many mammals has been historically limited due to a lack of applicable microarrays in species other than human and mouse. The recent introduction and large-scale application of the mammalian methylation array has significantly expanded the availability of such data across conserved sites in many mammalian species. In our study, we consider 13,245 samples profiled on this array encompassing 348 species and 59 tissues from 746 species-tissue combinations. While having some coverage of many different species and tissue types, this data captures only 3.6% of potential species-tissue combinations. To address this gap, we developed CMImpute (Cross-species Methylation Imputation) which uses a Conditional Variational Autoencoder (CVAE), a conditional generative model implemented via neural networks, to impute DNA methylation of non-profiled species-tissue combinations. CMImpute specifically conditions the CVAE on species and tissue labels, allowing for direct control over the combination to be imputed. In cross-validation, we demonstrate that CMImpute achieves a strong correlation with actual observed values, surpassing several baseline methods in terms of agreement across methylation array probes with a mean correlation of 0.92 and across samples with a mean correlation of 0.69. Using CMImpute we imputed methylation data for 19,786 new species-tissue combinations representing the remaining 96.4% of potential combinations. We believe that both CMImpute and our imputed data resource will be useful for DNA methylation analyses across a wide range of mammalian species."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Mirae Kim","Title":"Ontology-aware prediction of tissue-specific DNA methylation","Abstract":"DNA methylation (DNAm) has shown tremendous potential in distinguishing physiological states such as aging and cancer progression, and epigenetic clocks, in particular, have had far-reaching applications. Though DNAm is also highly tissue-specific, no pan-tissue classifier currently exists. Here, we manually curate 3,145 healthy human DNA methylation samples across 116 studies spanning 50 tissue types and combine this data compendium with a novel framework that combines Minipatch feature selection with ontology-aware classification. Through this study, we identify a minimal set of 741 CpG sites that can accurately distinguish between different tissue types. A deeper examination of the CpG sites also reveals underlying biological mechanisms that underpin the tissue-specificity of DNA methylation. Furthermore, we demonstrate that this ontology-aware learning structure enables effective zero-shot learning for tissues not seen in training."},{"Track":"RegSys","Room":"518","Weekday":"Saturday","Date":"13 July","Timespan":"17:20-18:00","Format":"In Person","Speaker":"Michael Hoffman","Title":"Virtual ChIP-seq: predicting transcription factor binding by learning from the transcriptome","Abstract":"We will discuss a new method, Virtual ChIP-seq, which predicts binding of individual transcription factors in new cell types using an artificial neural network that integrates ChIP-seq results from other cell types and chromatin accessibility data in the new cell type. Virtual ChIP-seq also uses learned associations between gene expression and transcription factor binding at specific genomic regions. This approach outperforms methods that use transcription factor sequence preferences in the form of position weight matrices, predicting binding for 33 transcription factors."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Maria Colomé Tatché","Title":"Single-cell and single-molecule computational epigenomics","Abstract":"Recent breakthroughs in high-throughput sequencing of single cells are revolutionizing the biological and biomedical sector. Among the different -omics layers that can be measured at the single-cell level, single-cell epigenomic measurements present a rich layer of regulatory information that stands between the genome and the transcriptome. These measurements can be obtained for large heterogeneous samples of single cells to profile tissues, organs and whole organisms, and to study dynamic processes like cellular differentiation, reprogramming or cancer evolution. These data types provide an unprecedented level of measurement resolution. _x000D_\nIn this talk I will discuss how single-cell ATAC-seq and single-cell DNA methylation data can be used to study cell identity [1,2]. I will introduce and compare multiple feature space constructions for epigenetic data analysis and show the feasibility of common clustering, dimension reduction, batch integration and trajectory learning techniques for both single-cell DNA methylation data and scATAC-seq data. _x000D_\nStudying single-cell DNA methylation heterogeneity using single-cell DNA methylation measurements is however complicated, as experimental protocols are costly and difficult to implement. I will present an alternative strategy, which involves minION sequencing combined with deconvolution of single-molecule methylation signals to reconstruct cell-type methylation profiles. I will show how, using this method, it is possible to deconvolve the methylomes of different cell types from an in-silico mix of cells._x000D_\nAnother level of genomic information that can be extracted from single-cell data are single-cell copy number variations (CNVs). I will present a novel algorithm, epiAneufinder [3], which exploits the read count information from scATAC-seq data to extract genome-wide CNVs for individual single-cells, and I will show how the obtained CNVs are comparable to the ones obtained from single-cell whole genome sequencing data. Thanks to epiAneufinder it is therefore possible to add a relevant extra layer of genomic information, namely single-cell copy number variation, to every scATAC-seq dataset without the need of additional experiments._x000D_\n_x000D_\n[1] A. Danese, M.L. Richter, D.S. Fischer, F.J. Theis and M. ColomÃ©-TatchÃ©*. EpiScanpy: integrated single-cell epigenomic analysis. Nature Communications, 12, 5228 (2021). _x000D_\n[2] M.D. Luecken, M. BÃ¼ttner, K. Chaichoompu, A. Danese, M. Interlandi, M.F. Mueller, D.C. Strobl, L. Zappia, M. Dugas, M. ColomÃ©-TatchÃ©*, F.J. Theis*. Benchmarking atlas-level data integration in single-cell genomics. Nat. Methods 19, 41â€“50 (2022)._x000D_\n[3] A. Ramakrishnan, A. Symeonidi, P. Hanel, K. T. Schmid, M. L. Richter, M. Schubert, M. ColomÃ©-TatchÃ©*. epiAneufinder identifies copy number alterations from single-cell ATAC-seq data. Nat. Commun. 14, 5846 (2023)."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"11:20-11:40","Format":"Live Stream","Speaker":"Yang Yang","Title":"REUNION: transcription factor binding prediction and regulatory association inference from single-cell multi-omics data","Abstract":"Motivation: Profiling of gene expression and chromatin accessibility by single-cell multi-omics approaches can help to systematically decipher how transcription factors (TFs) regulate target gene expression via cis-region interactions. However, integrating information from different modalities to discover regulatory associations is challenging, in part because motif scanning approaches miss many likely TF binding sites._x000D_\nResults: We develop REUNION, a framework for predicting genome-wide TF binding and cis-region-TF-gene â€œtripletâ€ regulatory associations using single-cell multi-omics data. The first component of REUNION, Unify, utilizes information theory-inspired complementary score functions that incorporate TF expression, chromatin accessibility, and target gene expression to identify regulatory associations. The second component, Rediscover, takes Unify estimates as input for pseudo semi-supervised learning to predict TF binding in accessible genomic regions that may or may not include detected TF motifs. Rediscover leverages latent chromatin accessibility and sequence feature spaces of the genomic regions, without requiring chromatin immunoprecipitation data for model training. Applied to peripheral blood mononuclear cell data, REUNION outperforms alternative methods in TF binding prediction on average performance. In particular, it recovers missing region-TF associations from regions lacking detected motifs, which circumvents the reliance on motif scanning and facilitates discovery of novel associations involving potential co-binding transcriptional regulators. Newly identified region-TF associations, even in regions lacking a detected motif, improve the prediction of target gene expression in regulatory triplets, and are thus likely to genuinely participate in the regulation._x000D_\nAvailability and implementation: All source code is available at https:\/\/github.com\/yangymargaret\/REUNION._x000D_\n"},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Reetika Ghag","Title":"scHOCMO: Higher Order Correlation Model for Single-cell Multi-omics","Abstract":"Single-cell technologies enable system level interrogations across several molecular layers at a single-cell resolution. Current single-cell technologies incorporate massive parallelism enabling high-throughput joint profiling of various modalities on a cell. Multi-modal single-cell data can be further integrated to understand the causal relationships among the several molecular layers driving regulatory mechanisms in disease progression. However, the heterogeneity introduced by various modalities and their feature spaces makes it challenging to unify data into a single inference framework. Here, we propose a novel method scHOCMO (Higher Order Correlation Model for Single cell multiomics), to address the scalability and generalizability challenges in the existing single-cell multimodal data integration methods. We extend the previously developed tensor-based HOCMO (Higher Order Correlation Model) to improve the scalability to analyze single cell data from 107 to 1013 scale using Trillion-Tensor framework. We illustrate our method, using the single-nucleus RNA seq (sn-RNA) and single-nucleus ATAC seq (sn-ATAC) data for Diabetic Kidney Disease. Using differentially expressed genes, we aim to elucidate the regulatory dynamics of disease progression based on these disease specific marker genes and cell types."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Pan-cell type continuous chromatin state annotation of all IHEC epigenomes","Abstract":"Understanding the mechanistic basis of genetic disease requires annotating the regulatory elements in the human genome. To this end, international consortia such as IHEC, ENCODE, and Roadmap Epigenomics have generated thousands of epigenomic datasets such as ChIP-seq, DNase-seq, and ATAC-seq that measure various biochemical activities in the genome, including transcription factor binding, histone modification, and DNA accessibility. Currently, the predominant methods for integrating these data sets to annotate regulatory elements are segmentation and genome annotation (SAGA) algorithms such as ChromHMM and Segway. SAGA algorithms partition the genome and assign a chromatin state label to each segment, indicating the epigenetic activity at that position. To alleviate the limitations of the discrete SAGA framework, we recently developed epigenome-ssm, a method that produces a vector of continuous chromatin state features at each position that summarizes epigenetic activity. Unlike discrete labels, these continuous features can easily represent varying strengths of a given element and can represent combinatorial elements with multiple types of activities. Here, we present a continuous chromatin state feature map generated using epigenome-ssm on 9,539 genome-wide signal tracks from six core histone modification assays across 1,698 epigenomes. We show that these feature maps constitute an intuitive and visualizable summary of epigenomic data and enable accurate identification of mechanisms of disease association."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Automated and genome-scale exploration of the cis-regulatory code involved in neuronal differentiation","Abstract":"Gene expression is controlled by proximal and distal cis-regulatory elements (CREs), containing DNA motifs bound by various transcription factors (TFs). Other sequence features, such as specific k-mers or low complexity regions, have also been implicated. _x000D_\nHowever, in a dynamic biological process such as cell differentiation, we lack an understanding of how the transcriptional activity of CREs progressively change and what sequence features underlie these transitions, which may reflect common and\/or coordinated regulatory processes. _x000D_\nHere, we use single-nucleus ATAC-seq with single-cell 5â€™ RNA-seq to follow, at a genome scale, CREs along differentiation of induced pluripotent stem cells into cortical neurons. We propose a guided clustering algorithm, STOIC (Statistical learning To Optimize Integrative Clustering) that jointly learns the different CRE clusters and their distinctive sequence-level features using an interpretable machine learning approach._x000D_\nThis procedure explores the expression space and delineates the CRE clusters iteratively in order to optimize the performance of a supervised classifier predicting CRE cluster membership based on DNA sequence features._x000D_\nWe show that STOIC provides more predictive sequence-level features than a standard k-means clustering. Furthermore, orthogonal chromatin and TF binding data collected in the same settings are used to validate the inferred CRE clusters and their sequence features, associate them to specific enhancer or promoter signatures and biological processes. Our results explore the complexity of the cis-regulatory code at the genome scale and provide an updated perspective on the transcriptional regulations at play during neuronal differentiation."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Expanding GTEx dataset with brain ontology-based graph neural networks to investigate genetic impacts on brain diseases","Abstract":"The human brain, with its intricate network of diverse regions, profoundly influences disease development. The Genotype-Tissue Expression (GTEx) program gathered transcriptome data and matched genotype data from over three hundred post-mortem donors, which allows us to understand how genetic variation can impact gene expression in diverse regions. However, the GTEx dataset included only 13 brain regions and only 10% of subjects had all brain regions measured. Improving the completeness of gene expression data within the GTEx project has the potential to elucidate the impact of disease risk variants on gene regulation in crucial tissues relevant to disease development. A possible resource to address this issue is the Allen Human Brain Atlas dataset. It collected transcriptome data from post-mortem brain tissue samples from 6 individuals, covering over a hundred distinct brain subregions. Leveraging the Allen dataset, we proposed a graph neural network model based on an expert ontology describing a hierarchy of increasingly fine-grained brain regions. This Graph Ontology model can predict 103 subordinate or previously uncollected brain regions for subjects within the GTEx dataset. We showed that our model outperformed several existing multi-tissue imputation models. Our model extended the initial 13 GTEx regions to 103 subordinate regions, enabling us to explore how genetic variation represented in GTEx can impact diverse disease-relevant regions that were not originally covered by the GTEx. Our prediction results can serve as a foundation for future investigations into how specific genetic variations influence diseases by altering gene expression patterns across a wide range of brain regions."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Interpretable single-cell factor decomposition using sciRED","Abstract":"Single-cell RNA sequencing (scRNA-seq) enables the exploration of gene expression heterogeneity within large cell populations, arising from biological and technical factors. Inferring gene expression programs from scRNA-seq data is challenging due to noise, sparsity, and high dimensionality, addressed by computational approaches like matrix factorization. Specialized factorization techniques such as glmPCA and cNMF have emerged in recent years to be tailored for scRNA-seq. However, the resulting factors must be manually interpreted. To address this gap, we developed sciRED as a tool to improve the interpretation of scRNA-seq factor analysis. sciRED implements a four-step approach to characterizing gene expression programs: (1) Removing confounding effects and using rotations to maximize factor interpretability (2) Calculating association statistics to map factors with known covariates, (3) Highlighting unexplained factors that may indicate hidden biological phenomena, and (4) Determining the genes and biological processes represented by unexplained factors. We apply our method, sciRED, across diverse datasets including the scMixology benchmark dataset and four biological single-cell atlases. Specifically, we showcase its application in identifying cell identity programs and sex-specific variations in a kidney map, discerning strong and weak stimulation signals in a PBMC dataset, eliminating ambient RNA contamination in a rat liver atlas to unveil strain variations, and revealing the hidden biology, represented by a rare cell type signature and anatomical zonation gene programs, in the healthy human liver map. These demonstrate the utility of our approach on real datasets for characterizing intricate biological signals within scRNA-seq maps."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Shaun Mahony","Title":"Accurate allocation of multi-mapped reads enables regulatory element analysis at repeats","Abstract":"Transposable elements (TEs) and other repetitive regions have been shown to contain gene regulatory elements, including transcription factor binding sites. Unfortunately, regulatory elements harbored by repeats have proven difficult to characterize using short-read sequencing assays such as ChIP-seq or ATAC-seq, as most regulatory genomics analysis pipelines discard â€œmulti-mappedâ€ reads. To address this shortcoming, we developed Allo, a new approach to allocate multi-mapped reads in an efficient, accurate, and user-friendly manner. Allo combines probabilistic mapping of multi-mapped reads with a convolutional neural network that recognizes the read distribution features of potential peaks, offering enhanced accuracy in multi-mapping read assignment. To demonstrate Alloâ€™s potential, we apply it to reanalyze almost 500 transcription factor ChIP-seq datasets from K562 cells. This analysis resulted in over 385,000 previously unidentified transcription factor binding sites in repetitive regions of the genome. We find that Allo is particularly beneficial in identifying ChIP-seq peaks at centromeres and in younger TEs. In particular, we find novel associations between particular TFs and the recently expanded SVA and ERVK transposon families. We also find that Allo has a striking ability to disambiguate multi-mapped reads at recently duplicated genes. Using Allo, we analyze how regulatory elements diverge at recently generated paralogous genes, enabling new regulatory insights at sites of recent evolutionary novelty that often get overlooked in regulatory genomics analyses. Finally, we demonstrate that TF binding sites harbored by repeats are particularly difficult for neural network-based methods to predict de novo, and we speculate on approaches that can offer improved performance."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":null,"Title":"Q&A for Flash Talks","Abstract":null},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Hirak Sarkar","Title":"A count-based model for delineating cell-cell interactions in spatial transcriptomics data","Abstract":"Motivation: Cell-cell interactions (CCIs) consist of cells exchanging signals with themselves and neighboring cells by expressing ligand and receptor molecules, and play a key role in cellular development, tissue homeostasis, and other critical biological functions. Since direct measurement of CCIs is challenging, multiple methods have been developed to infer CCIs by quantifying correlations between the gene expression of the ligands and receptors that mediate CCIs, originally from bulk RNA sequencing data and more recently from single-cell or spatial transcriptomics data. Spatial transcriptomics has a particular advantage over single-cell approaches since ligand-receptor correlations can be computed between cells or spots that are physically close in the tissue. However, the transcript counts of individual ligands and receptors in spatial transcriptomics data are generally low, complicating the inference of CCIs from expression correlations. _x000D_\n_x000D_\nResults: We introduce Copulacci, a count-based model for inferring CCIs from spatial transcriptomics data. Copulacci uses a Gaussian copula to model dependencies between the expression of ligands and receptors from nearby spatial locations even when the transcript counts are low.  On simulated data, Copulacci outperforms existing CCI inference methods based on the standard Spearman and Pearson correlation coefficients. Using several real spatial transcriptomics datasets, we show that Copulacci discovers biologically meaningful ligand-receptor interactions that are lowly expressed and undiscoverable by existing CCI inference methods. _x000D_\n_x000D_\nAvailability: Copulacci is implemented in Python and available at https:\/\/github.com\/raphael-group\/copulacci"},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Xinhai Pan","Title":"Mapping lineage-resolved scRNA-seq data with spatial transcriptomics using TemSOMap","Abstract":"Spatial transcriptomics (ST) has become a powerful technique that bridges the gap between traditional gene expression analysis and spatial information within tissues or organisms. While ST can obtain a snapshot of cellsâ€™ spatial gene expressions, the library size is relatively limited compared to scRNAseq datasets. This limitation can be overcome by integrating scRNAseq data with the ST data. By mapping the single cells onto the spatial data, we can also infer the spatial coordinates of the cells from the scRNAseq dataset. On the other hand, CRISPR\/Cas9-based lineage tracing technologies have enabled paired sequencing of cellsâ€™ gene expression and lineage barcodes. The reconstructed cell lineage tree from the barcodes represents cellsâ€™ clonal distances. With the availability of single-cell spatial and temporal information at the single-cell resolution, it is of great interest to look into the spatio-temporal dynamics of cells, which requires the inference of spatial coordinates of the lineage-traced cells. Therefore, we developed TemSOMap (Temporal and Spatial-Omics Mapping of single cells), which infers the spatial coordinates of cells by mapping a paired gene expression and lineage barcode dataset onto a spatial transcriptomics dataset using deep learning. The method aims to improve the accuracy of state-of-the-art mapping methods by utilizing the temporal and spatial information in the data. We show that TemSOMap can more accurately infer the spatial location of single cells, and can help us better understand the spatio-temporal dynamics of single cells using the spatially-resolved cell lineage and transcriptomic map."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Rian Pratama","Title":"Enhancing spatial transcriptomics analysis using deep learning-based batch effect mitigation","Abstract":"Spatial transcriptomics (ST) is a groundbreaking technique for studying the correlation between cellular organization within a tissue and their physiological and pathological properties. Every facet of spatial information, including cell\/spot proximity, distribution, and dimensionality, holds significance. Most methods lean heavily on proximity for ST analysis, each resulting in useful insights but still leaves other aspects untapped. In addition, samples procured at different times, different donors, and by different technologies introduce batch effects problem that hinders statistical approach employed by most analysis tools. Addressing these challenges, we have developed a deep learning method for analyzing integrated multiple ST data, focusing on distribution aspect. Additionally, our method leverages single-cell analysis tools._x000D_\n_x000D_\nOur study introduces Spatial Gene Net, a data integration pipeline utilizing representation learning approach to extract spatial distribution of genes into the same feature space as gene expression features. We employ an encoder network to extract spatial embedding, facilitating the projection of spatial features into gene expression feature space. Our approach allows seamless integration of multiple samples with minimum detriment, bolstering the statistical power of ST data analysis tool. We show application of our method on human DLPFC dataset. Our method consistently improves the performance of Seurat tools clustering, with the most significant increase observed in sample 151673, almost doubling the ARI score from 0.236 to 0.405. This result reveals the potential of gene distribution spatial aspect, encouraging the development of better spatial feature extractor which emphasizes the impact of integration and batch effect correction for understanding tissue characteristics."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Zhana Duren","Title":"Gene Regulatory Networks analysis from single cell multi-omics data","Abstract":"Existing methods for gene regulatory networks (GRN) inference rely on gene expression data alone or on lower resolution bulk data. Despite the recent integration of chromatin accessibility and RNA sequencing data, learning complex mechanisms from limited independent data points still presents a daunting challenge. Here we present LINGER (Lifelong neural network for gene regulation), a machine-learning method to infer GRNs from single-cell paired gene expression and chromatin accessibility data. LINGER incorporates atlas-scale external bulk data across diverse cellular contexts and prior knowledge of transcription factor motifs as a manifold regularization. LINGER achieves a fourfold to sevenfold relative increase in accuracy over existing methods and reveals a complex regulatory landscape of genome-wide association studies, enabling enhanced interpretation of disease-associated variants and genes. Following the GRN inference from reference single-cell multiome data, LINGER enables the estimation of transcription factor activity solely from bulk or single-cell gene expression data, leveraging the abundance of available gene expression data to identify driver regulators from case-control studies."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Maggie Beheler-Amass","Title":"Dynamic Gene Regulatory Network Inference with Interpretable, Biophysically-Motivated Neural ODEs","Abstract":"Gene Regulatory Networks (GRNs) are complex dynamical systems that modulate gene expression and drive transitions between phenotypic cell states. Determining these networks is crucial in understanding how gene dysregulation can lead to phenotypic variation and diseases. We present a novel biophysically-motivated neural ordinary differential equation (ODE) model framework with a biologically interpretable deep learning architecture that leverages dynamic single-cell data. This model framework infers GRNs by implicitly estimating underlying biophysical parameters such as RNA velocity, mRNA transcription rate, and mRNA decay rate._x000D_\n_x000D_\nTo test the accuracy of our model, we apply it to a simulated dataset with a known ground truth GRN. We demonstrate that the neural ODE can successfully predict gene expression at unseen time points, and decompose the inferred RNA velocity into the transcription and degradation driving the system, while inferring the underlying GRN. Next, we train a model on a single-cell Saccharomyces cerevisiae dataset dynamically responding to rapamycin treatment. The model learns regulatory responses to the rapamycin perturbation and reveals key genes involved in the cellular response in silico. Finally, we apply the model to a dynamic hematopoiesis dataset to test whether the model can capture bifurcations of hematopoietic stem cells progressing along the myeloid and lymphoid lineages. The applicability to real-world datasets highlights the utility of neural ODEs coupled with interpretable deep learning. This framework has the potential to advance our understanding of complex biological systems and aid in the discovery of regulatory mechanisms underlying cellular responses to perturbations."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:00","Format":"Live Stream","Speaker":"Noa Moriel","Title":"Optimal sequencing budget allocation for trajectory reconstruction of single cells","Abstract":"Charting cellular trajectories over gene expression is key to understanding dynamic cellular processes and their underlying mechanisms. While advances in single-cell RNA-sequencing technologies and computational methods have pushed forward the recovery of such trajectories, trajectory inference remains a challenge due to the noisy, sparse, and high-dimensional nature of single-cell data. This challenge can be alleviated by increasing either the number of cells sampled along the trajectory (breadth) or the sequencing depth, i.e. the number of reads captured per cell (depth). Generally, these two factors are coupled due to an inherent breadth-depth tradeoff that arises when the sequencing budget is constrained due to financial or technical limitations. Here we study the optimal allocation of a fixed sequencing budget to optimize the recovery of trajectory attributes. Empirical results reveal that reconstruction accuracy of internal cell structure in expression space scales with the logarithm of either the breadth or depth of sequencing. We additionally observe a power law relationship between the optimal number of sampled cells and the corresponding sequencing budget. For linear trajectories, non-monotonicity in trajectory reconstruction across the breadth-depth tradeoff can impact downstream inference, such as expression pattern analysis along the trajectory. We demonstrate these results for five single-cell RNA-sequencing datasets encompassing differentiation of embryonic stem cells, pancreatic Î² cells, hepatoblast and multipotent haematopoietic cells, as well as induced reprogramming of embryonic fibroblasts into neurons. By addressing the challenges of single-cell data, our study offers insights into maximizing the efficiency of cellular trajectory analysis through strategic allocation of sequencing resources."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Ayan Paul","Title":"Charting the role of RNA binding proteins in tissue-specific alternative splicing using machine explanations","Abstract":"The regulation of alternative splicing by RNA Binding Proteins (RBP) is an essential mechanism in determining tissue specificity. The nuances of the variation in the role of the RBPs, singly and collectively, in various tissues are not well understood. We present a study of two cell lines, HepG2 and K562 using eCLIP RBP binding data and shRNA RBP knockdown followed by RNA-seq data from the ENCODE project to chart the role of RBP cooperativity in regulating exon skipping, one of the primary modes of alternative splicing. We build RBP binding graphs from exon triplets and train machine learning models, both linear and non-linear, to map RBP bindings to exon-skipping quantification. We show significant non-linearities are expressed in both cell lines. We achieve state-of-the-art performance with Extreme Gradient Boosted Decision Trees and Deep Neural Networks with skip connections. We use Shapley values as post-hoc explanations of machine learning models to quantify the importance of individual RBPs and to identify instances of cooperative regulation between sets of RBPs. We explore RBP activity in close proximity to intron-exon junctions and in deep intronic regions. We show that RBPs have a subset of cell-line agnostic roles and a subset of cell-line-specific roles in regulating alternative splicing. Furthermore, we identify binding-region-specific roles of RBPs as splicing enhancers or silencers displaying the power of our analysis in elucidating the functional roles by which RBPs regulate alternative splicing."},{"Track":"RegSys","Room":"518","Weekday":"Sunday","Date":"14 July","Timespan":"17:20-18:00","Format":"In Person","Speaker":"Hae Kyung Im","Title":"Harnessing deep learning to amplify insights from GWAS","Abstract":"Genome-wide Association Studies (GWAS) have identified associations with thousands of complex traits across a significant portion of the genome. Transcriptome-wide Association Studies (TWAS) and similar methods (xWAS) aim to uncover causal mechanisms by leveraging genetic predictors of molecular traits. However, their effectiveness is constrained by the current limitations in predicting these traits from genotypes. In this talk, I will explore recent advancements in deep learning methods for predicting gene expression from DNA sequences and demonstrate how these techniques can enhance the power of TWAS. By fine-tuning pre-trained large-scale models, we can predict molecular traits on a much larger scale than is possible with traditional population-based approaches. This methodology holds promise for addressing challenges related to portability across ancestries and species, rare variations, linkage disequilibrium (LD) confounding, and single-cell expression analysis."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"09:00-09:15","Format":"In Person","Speaker":null,"Title":"Introduction and Welcome Words","Abstract":null},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"09:15-10:00","Format":"In Person","Speaker":"Dana Pe'er","Title":"Towards plasticity in the tissue context:  Characterizing niches","Abstract":null},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"10:00-10:15","Format":"In Person","Speaker":"Sofia Rodriguez","Title":"Deep analysis of regulatory networks based on single cell transcriptomics reveals a system of master regulators for Rett syndrome.","Abstract":"Rett syndrome is a mono-chromosomal disorder with a prevalence much higher in women (95% of all cases). This disease is characterized by difficulties to study its phenotype caused by an intrinsic heterogeneity of the brain tissues affected due to the stochastic silencing of the X chromosome. In addition, we are yet largely unaware of the cellular-autonomous and non-cellular autonomous alterations that occur in neuronal populations because of this pathology. To try to overcome these issues, we performed a gene regulatory analysis, from human organoid single cell transcriptomic data. We first performed a trajectory analysis to understand the data characteristics, followed by generation of pseudo-time-based gene regulatory networks to assess non-cellular autonomous processes. These approaches made possible to show differences in the distribution of cells and their developmental differences. From the regulatory networks, it was possible to identify the absence of a regulatory mechanism associated with non-coding transcription factors not previously described for Dopaminergic neurons and a System of Master Regulators associated to MECP2 in Gabaergic neurons. These results served to postulate the existence of systemic master regulators, which we further evaluated with dynamic Boolean models per cell type based on the proposed regulatory network."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"10:15-10:30","Format":"In Person","Speaker":"Wisdom A Akurugu","Title":"Genetic Determinants of Adrenocorticotropic Hormone Resistance in Children on Corticosteroid Treatment","Abstract":"Inhaled corticosteroids are crucial for managing asthma, but these may cause hypothalamic-pituitary-adrenal suppression (HPAS). Cortisol production is suppressed as it is in Addisonâ€™s Disease (AD), Clinical symptoms may be similar. Identifying genetic markers associated with ACTH resistance versus HPAS is crucial for precise patient stratification and personalized treatment. SNP data of ninety-six asthmatic children on inhaled corticosteroids and nasal steroids were studied. The participants underwent an overnight metyrapone test. Baseline adrenocorticotropic hormone and cortisol were measured as well as post-metyrapone adrenocorticotropic hormone (PMACTH). ACTH resistance was diagnosed if the PMACTH\/C ratio is greater than 0.35. Eight-two and seventy-six samples out of the 96 participants had data for Basal and PM ACTH\/C ratios respectively. SNP association was done using the PLINK analysis toolkit and statistical regression. Genetic models were assessed, and SNP functional annotation & prioritization were performed. Two significant SNPs emerged among others: rs6962 (G>A) on the SDHA gene and rs2303223 (G>A) on ZNF668 whereby rs6962 is likely resistant while rs2303223 is likely responsive to ACTH. Genotypic comparisons of rs6962 (AA vs GA & GG) and rs2303223 (GG vs GA & AA) showed statistical significance for âˆšBasalACTH\/C and âˆšPMACTH\/C ratios respectively. SDHA, integral to mitochondrial respiration, may affect metabolic pathways governing ACTH resistance. ZNF668, encodes a zinc finger protein and akin to other zinc proteins which are known to regular the glucocorticoid receptor (GR), may as well regulate the GR or other steroid receptors. Dysregulation of these genes could impact ACTH sensitivity. Further validatory and confirmatory investigations are recommended."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"10:30-10:45","Format":"In Person","Speaker":"Ornit Nahman","Title":"Cell specific priors rescue differential gene expression in spatial spot-based technologies","Abstract":"Spatial transcriptomics (ST), a breakthrough technology, captures the complex structure and state of tissues. Several ST technologies now exist, most prominently spot-based platforms such as Visium. Despite ST's widespread usage and distinct data characteristics, the vast majority of studies continue to analyze ST data using algorithms built for older technologies, such as single cell (SC) and bulk RNA-seq. This is particularly the case when identifying differentially expressed genes (DEGs), however, it remains unclear if the approaches  used are still valid for ST data. Here, we sought to characterize the performance of these methods by constructing an in-silico simulator of ST data with a controllable and known DEG ground truth. Surprisingly, our findings reveal little variation in the performance of classic DEG algorithms - all of which fail to accurately recapture known DEGs to significant levels. Importantly, we further demonstrate that cellular heterogeneity within spots is a primary cause of this poor performance and propose a simple gene-selection scheme, based on prior knowledge of cell-type specificity, to overcome this. Importantly, our approach outperforms existing data-driven methods and enhances DEG recovery and trustworthiness in ST data. Overall, our work details a conceptual framework that can be used upstream of any DEG algorithm to improve the accuracy of the results and of any subsequent downstream analysis."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"10:45-11:05","Format":"In Person","Speaker":null,"Title":"First Draft Assembly and Annotation of the Genome of the Cadmium-Resistant Fungus Talaromyces santanderensis using Oxford Nanopore sequencing: First Molecular Insights into its Cadmium Resistance.","Abstract":"Contamination of crops soils by cadmium (Cd) is a worldwide threat to ecosystems and human health. High concentrations of Cd damage the cell membrane, organelles, and generate overproduction of reactive oxygen species. Talaromyces santanderensis, a recently fungal strain isolated from cocoa soil, is a new species of Cd resistant fungus with a high tolerance rating between 100â€“400 mg\/kg. However, there is no molecular or genetic information about the mechanism used for tolerance. Thus, this study presents the first draft genome assembly and annotation of T. santanderensis. The genome of this strain was sequenced using Oxford Nanopore Technology with fungal DNA under stress conditions of Cd at 50 ppm, and under normal conditions of growth. The genome was assembled using Flye, Miniasm, NECAT, Wtdbg2 and Canu and produced assemblies exhibiting similarly high levels of BUSCO completeness (~96.5%) with a coverage of 20x on average. Flye presented the assembly with the highest contiguity featuring a N50 length of 8,443,216 bp and a maximum contig of 12,809,360 bp. The genome we present has a GC content of 45.16% and a size of 38,889,036 bp. Gene prediction yielded 13,791 genes, and through functional annotation we identified important homologous protein-coding genes reported to be associated with Cd resistance such as copper chaperone, arsenite and zinc transporter, arsenical resistance protein, and Cd2+-exporting ATPase. Additionally, we present T. santanderensis methylome for the modified bases 5mC and 5mhC during both conditions, which will give us information about the role of the epigenomic in adaptation to heavy metal environment."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"10:45-11:05","Format":"In Person","Speaker":null,"Title":"Constructing representative sequence models for evolutionary analysis of protein superfamilies","Abstract":"The ability to confidently infer evolutionary relationships at the scale of protein superfamilies would profoundly transform biology. While the advent of various machine learning-based models for sequence analysis have provided significant improvements in remote homology detection, it is unclear whether such methods can effectively reconstruct phylogenies at this scale. _x000D_\n_x000D_\nWhen analysing protein superfamilies, poor alignment accuracy presents a major hurdle to traditional phylogenetics. Additionally, the computational burden of performing these analyses on sufficiently large datasets may be prohibitive. These challenges are commonly exacerbated by input sequences which are poorly representative of the evolutionary space of interest. Curation approaches which rely on sequence similarity often produce unrepresentative sequences for alignment due to the diminishing correlation of sequence identity with evolutionary relatedness at high degrees of divergence.  _x000D_\n_x000D_\nWe developed an approach to minimise these issues which constructs representative profile Hidden Markov Models (pHMMs) for sequence curation from large evolutionary spaces. From pHMMs built with robust alignments of highly similar sequences, the tool iteratively expands the profilesâ€™ scopes. Their representativeness for the given space is optimised by systematic exclusion of sequence subsets and cross-validation over several iterations. Alignments and phylogenies constructed downstream of curation by this method demonstrate improvement across various metrics._x000D_\n_x000D_\nWe applied this approach to investigate the evolutionary origins of B3 metallo-beta-lactamases, gaining novel insights into their emergence from a diverse superfamily. Enhancements in the alignment and phylogeny conferred by this tool allowed for characterisation of deep ancestral variants and new hypotheses regarding molecular determinants of this familyâ€™s function."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"10:45-11:05","Format":"In Person","Speaker":null,"Title":"Characterization of Non-Equilibrium Phase-Separated Biomolecular Condensates","Abstract":"Biomolecular condensates, such as stress granules (SG), are understood to harbor protein aggregates implicated in neurodegenerative pathologies like Alzheimerâ€™s Disease (AD). Understanding biomolecular condensate dynamics may therefore provide insight into potential therapeutic interventions for these diseases. Biomolecular condensates are formed by liquid-liquid phase separation (LLPS). Under equilibrium conditions, multiple small droplets resulting from LLPS merge into a single large droplet. However, in living systems, phase-separated biomolecular condensates do not form large droplets and are instead maintained out-of-equilibrium as many small droplets. Molecular dynamics simulations run by our team suggested that this out-of-equilibrium behavior occurs due to an altered rate of switching between conformations of a biomolecule, with one state favoring phase separation and the other not. Based on this, we hypothesized that the faster a molecule switches between these two states, the further out-of-equilibrium and smaller the resulting biomolecular condensates will be. To investigate this, we used increasing cellular stress as a proxy for increasing swapping rate and explored the relationships between various stresses, ATP depletion, and SG sizes in a cell culture system. Additionally, we demonstrate that in vitro, phase-separated tau droplets remain smaller and more numerous when in the presence of the proline isomerase PPIA, compared to when PPIA is absent. By linking this PPIA to G3BP1, a core component of SGs, we showed that PPIA could control stress granule size. These findings together suggest that an out-of-equilibrium state can be maintained by an altered swapping rate of a biomolecule between two conformations."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"10:45-11:05","Format":"In Person","Speaker":null,"Title":"Multiomics analysis highlighted the role of senescence in regulating trophoblast differentiation: a promising target for early preeclampsia prediction.","Abstract":"Background: Preeclampsia (PE) is a common gestational disease affecting 2-5% of all pregnancies, with its aetiology associated with defective trophoblasts differentiation. Currently, there is no reliable approach for either prediction or treatment of PE. Recent research highlights the significance of premature trophoblast senescence as a new characteristic of PE. Nonetheless, the molecular interactions between trophoblast senescence and differentiation are poorly understood. _x000D_\n_x000D_\nMethods: We analyzed aging marker expression in trophoblast stem cells differentiating into extravillous trophoblasts (EVTs), identifying an aging-related gene set (ARGS). Using CRISPR knockout, we verified ARGS roles in trophoblast differentiation. Since miRNA is stable and allows easy PE diagnosis, we examined interactions between ARGS and miRNA using covalent ligation of endogenous argonaute-bound RNAs-crosslinking and immunoprecipitation (CLEAR-CLIP)._x000D_\n_x000D_\nResults: Our findings demonstrated that the EVT differentiation process encompasses a senescence process, characterized by elevated levels of ARGS. By employing CRISPR-screening, we demonstrated that 41 genes (TD-ARGs) in ARGS are involved in regulating trophoblast differentiation. 91 miRNAs (TD-ARG miRNA) that interact with TD-ARGs in trophoblast were then identified using our CLEAR-CLIP data. To test the relevance of these TD-ARG miRNA to PE, we examined their expressions in the serum of first-trimester pregnant women. The levels of 4 TD-ARG miRNA was significantly different in women who later developed PE when compared to those with normotensive pregnancy._x000D_\n_x000D_\nConclusion: The results of this study provide novel evidence that senescence process is associated with trophoblast differentiation. Clinically, they also indicate the possible use of serum miRNA that targeting TD-ARG in the early prediction of PE."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"11:25-11:40","Format":"In Person","Speaker":"Iria Pose Lagoa","Title":"Unraveling patient heterogeneity through explainable AI and network-based strategies","Abstract":"Complex diseases often present a wide landscape of molecular profiles, posing challenges in identifying biomarkers associated with disease progression and, consequently, efficient personalized therapies. This is the case of Chronic Obstructive Pulmonary Disease (COPD), a heterogeneous condition characterized by the development of severe airflow obstruction profiles. Here, we propose a comprehensive framework comprising three key components: feature selection, prediction performance, and SHAP values analysis. To achieve this, we used gene expression data from the Lung Tissue Research Consortium and employed various feature selection criteria to identify the most relevant discriminant genes. These filtering approaches include knowledge extracted from intrinsic data characteristics (data-driven), external information from DisGeNET of genes associated with COPD (curated COPD-related genes), and their respective biological expansions based on physical interaction partners (OmniPath) and network-based prioritization algorithms (GUILDify). Subsequently, we exhaustively evaluate the performance of several state-of-the-art classifiers: Random Forest, Support Vector Machines - polynomial and radial kernel, k-Nearest Neighbors, Generalized Linear Models, and XGBoost. SHAP values obtained from the most accurate settings were combined with clustering algorithms to explain the modelâ€™s predictions and perform a cluster-based analysis for COPD profiling. Our integrative approach successfully distinguished COPD patients with data-driven selection strategy achieving the highest performance. These classifiers demonstrated accuracies of up to 84.8%, surpassing previous approaches. The genes identified in this study serve as promising biomarkers for COPD subtyping, offering avenues for more personalized treatment modalities. This study underscores the potential of integrated analytical approaches in advancing the diagnosis and treatment of complex diseases such as COPD."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"11:40-11:55","Format":"Live Stream","Speaker":"Manoj M Wagle","Title":"Interpretable deep generative ensemble learning of cell identity paired with automated annotation for single-cell multi-omics","Abstract":"Single-cell omics technologies, with their recent advancement towards multi-modality, have achieved remarkable success in uncovering cellular heterogeneity at an unparalleled resolution. However, the high dimensionality, inherent noise, and sparsity render feature selection a crucial step in analyzing such data. Currently, the popular approach has been identifying highly variable genes, but this might not capture the complete spectrum of molecular variability and can miss out on certain informative genes. Moreover, the number of tools adept at capturing valuable information embedded in other single-cell modalities, such as chromatin accessibility and surface proteins, is currently limited. _x000D_\n_x000D_\nTo bridge this gap, we have developed Hydra, an interpretable deep generative framework based on variational autoencoders and data augmentation. Unlike traditional methods, Hydra is capable of effectively utilizing diverse single-cell omics data to capture cell-type specific molecular signatures, enabling a holistic examination of cells within the dataset. Additionally, as an integral component of this framework, we developed an ensemble classification module for the automated annotation of single-cell datasets. We extensively benchmarked Hydra across 23 datasets, including unimodal and multimodal single-cell omics datasets. Our results demonstrate that cell-type specific features selected by Hydra provide comparable to superior performance against several state-of-the-art methods in terms of stability, marker identification and cell-type annotation."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"11:55-12:10","Format":"In Person","Speaker":"Miriam Poley-Gil","Title":"Exploring the biophysical boundaries of protein families with deep learning methods","Abstract":"Recently, Deep Learning models have revolutionised the Molecular Biology field allowing us to explore the intricate interplay between protein sequence, structure and function faster. To understand what they are capturing and generating we have combined state-of-the-art protein models for inverse folding (such as ProstT5[1] and ProteinMPNN[2]) and for sequence generation (such as ProtGPT2[3] and ZymCTRL[4]) with biophysical analyses (Figure 1). _x000D_\nWe have studied conservation patterns of local energetic frustration in artificial datasets to shed light on the evolutionary processes leading to the diversification of some protein families, under the assumption that proteins are optimised for folding and stability, but also evolutionarily selected to function. We have developed a tool called FrustraEvo[5] that measures such conservation within and between protein families (available in full on the server https:\/\/frustraevo.qb.fcen.uba.ar\/)._x000D_\nWe found that most of the highly frustrated native residues are related to functional aspects. These functional residues are mostly recovered by sequence generation models, suggesting that there are alternative ways to design proteins instead of the way explored by evolution. In the case of catalytic sites, they are also recovered by inverse folding models. We therefore point out a selective memory concerning functionality (primary level of memory (local)). However, ProteinMPNN, also recovers the main network of frustrated contacts of the functional domains even suggesting a tertiary level of memory (contacts). Thus, our approach promises to effectively unravel the intricacies of protein family boundaries and explore design options for understanding protein evolution."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"12:10-12:15","Format":"In Person","Speaker":null,"Title":"Utilizing a Novel VAE Pipeline for Tau Inhibitor Screening Validated in Drosophila Melanogaster Alzheimerâ€™s Models","Abstract":"Alzheimer's disease (AD), affecting over 50 million worldwide, is a progressive disorder characterized by Tau protein aggregation, leading to significant impairments. Current FDA-approved AD drugs target amyloid-beta aggregation, highlighting the need for alternative pathway approaches. This study introduces a novel approach integrating computational predictions with validations to identify therapeutic molecules against Tau aggregation. A variational autoencoder (VAE), using a Keras architecture, included a recurrent neural network (RNN) encoder and decoder and a property prediction model, with a gated recurrent unit (GRU) layer added to the decoder. The model was trained using the ZINC 250k Molecules Database and tested on a unique database of 72 known Tau inhibitors. The outputs synthesized Methylene Blue (MB), with validation loss sub-0.2 post 20 epochs. MB has been shown to inhibit Tau aggregation by accelerating LLPS. Administered to Alzheimerâ€™s mutant D. melanogaster, MB increased RNA concentration in Tau-mutated fliesâ€™ brains. RNA quantification, a measure of transcriptional activity, validated MB's potential as a Tau inhibitor. This integrative approach highlights the efficacy of combining computational predictions with empirical testing in drug discovery."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"12:15-13:00","Format":"In Person","Speaker":null,"Title":"Keynote: TBD","Abstract":null},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"13:00-14:40","Format":"In Person","Speaker":null,"Title":"Lunch with Poster Session & Networking","Abstract":null},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"14:55-15:15","Format":"In Person","Speaker":null,"Title":"FinaleToolkit: Accelerating Cell-Free DNA Fragmentation Analysis with a High-Speed Computational Toolkit","Abstract":"The fragmentation pattern of cell-free DNA (cfDNA) represents a promising non-invasive biomarker for disease diagnosis and prognosis. Numerous fragmentation features, such as end motif and window protection score (WPS), have been characterized in cfDNA genomic sequencing. However, the analytical tools developed in these studies are often not released to the liquid biopsy community or are inefficient for processing large datasets. To address this gap, we have developed FinaleToolkit, a fast and memory-efficient Python package designed to generate comprehensive fragmentation features from large cfDNA genomic sequencing data. For instance, FinaleToolkit can generate genome-wide WPS features from a ~100X cfDNA whole-genome sequencing dataset in 74 minutes using 16 CPU cores and 49 GB of memory, offering up to a ~30-fold increase in processing speed compared to original implementations. We have benchmarked FinaleToolkit against existing studies or implementations where possible, confirming its efficacy. Furthermore, FinaleToolkit is open source and thoroughly documented with both command line interface and Python application programming interface (API) to facilitate its widespread adoption and use within the research community: https:\/\/github.com\/epifluidlab\/FinaleToolkit"},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"14:55-15:15","Format":"In Person","Speaker":null,"Title":"Development and Application of the MultiSEp R Package to Identify Multiple Myeloma Achilles' Heels for Drug Discovery","Abstract":"Almost all Multiple Myeloma (MM) patients relapse and ultimately succumb to therapy-resistant disease; there is an urgent need for more effective treatment. Achilles' heel relationships arise when the status of one gene exposes a cell's vulnerability to perturbation of a second gene, such as chemical inhibition, providing opportunities for precision oncology._x000D_\n_x000D_\nWe developed MultiSEp for integrative discovery of candidate gene dependency relationships in multiomics data. We predicted MM GDRs at genome-scale (27,232 genes, 370,777,296 candidate interactions) using transcriptomic data from the Multiple Myeloma Research Foundation (MMRF) CoMMpass study (n=928 patients). Filtering steps to derive a high-confidence synthetic lethal network (SynLethNet) included predicting characteristic mutual exclusive loss patterns (q<0.05). We predicted the population coverage achieved by drugging SynLethNet genes, and the impact of deleterious mutations. Our analysis only utilised deleterious mutations predicted by the variant effect prediction tools, SNPeff and SNPsift (annotated â€˜high-impactâ€™ mutations)._x000D_\n_x000D_\nWe characterised GDRs in the CoMMpass cohort and derived a high confidence predicted synthetic lethal network (1,466 genes, 5,245 edges; SynLethNet). Functional annotation of SynLethNet revealed many genes involved in the ubiquitin-proteasome system, which is dysregulated in MM and a target of current front-line therapy. Predictions were validated with the Cancer Dependency Map and the Cancer Therapeutics Response database._x000D_\n_x000D_\nWe present the MultiSEp R package, demonstrated with a case study in Multiple Myeloma where we predict candidate drug targets and provide mechanistic insights to advance precision oncology."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"14:55-15:15","Format":"In Person","Speaker":null,"Title":"Bridging Education and Research: Data Hunters Workshop Empowering Bioinformatics Education via Microbiome Studies","Abstract":"Ensuring access to the bioinformatics field shall be on the agenda of life sciences degrees. This especially applies to biology-related degrees, where future biologists often ignore its existence due to the scarcity of dedicated courses. \n\nWe present our contribution to bioinformatics education, using the Data Hunters Workshop as a case study. Kicked off on February 28th, Data Hunters constitutes an ongoing student-science activity for students of the University of Milano-Bicocca, particularly those in the Biotechnology and Biosciences Department. Combining educational engagement with scientific research, this initiative enables students to tackle the key issue of metadata standardizationâ€™s lack in metagenomics, while supporting their learning. We provided a 6-hour lecture and an autonomous hands-on phase, structured as a learn-and-play activity with in-house built online educational and command-line resources. Thus, 29 students gained the fundamentals of metagenomics and Python language. Thanks to these tools, they have now stepped into the role of bioinformaticians, actively curating metadata from 379 amplicon-based and shotgun sequencing projects of the human skin microbiome to collaboratively create a curated metadata collection. Upon the workshopâ€™s conclusion, we will assess the efficacy of our activity via surveys and standardized evaluation scales.\n\nOur workshop represents the effort to bridge educational and research aims, including students as bioinformaticians of the future. As a reflection of the synergy of our objectives, the workshop's outcomes will include significant educational impacts that will lead to the development of a collaborative curated collection of human skin microbiome metadata, alongside the advancement of bioinformatics dissemination."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"14:55-15:15","Format":"In Person","Speaker":null,"Title":"Seven Domain Topics in Bioinformatics Education - Refining the ISCB Core Competencies to Access Diversity in Training","Abstract":"The ISCB Regional Student Group of Brazil (RSG-Brazil) is at the forefront of promoting bioinformatics and computational biology education in Brazil. In 2019, RSG-Brazil launched its Educational Committee (EduComm) with a dual mission: to develop educational materials in Portuguese and to assess the efficacy of bioinformatics training in the country. Leveraging the ISCB core competency framework 3.0, the EduComm devised a novel training model to evaluate confidence in various technical aspects of bioinformatics. This model encompasses seven domain topics crucial for bioinformatics education: Biology, Statistics, Computer Science, Ethics, Bioinformatics Applications, Communication, and Professional Development. To gauge the educational needs of the Brazilian bioinformatics community, a survey was conducted in November 2023, collected 375 responses from across 21 states, predominantly from academia. Notably, the majority of respondents identified themselves as Bioinformatics Users, with a significant representation from Undergraduate and Graduate students. The survey revealed regional and profile-specific interests in Bioinformatics Topics, providing valuable insights for curriculum development. Through the efforts of EduComm, RSG-Brazil aims to tailor educational courses to meet the diverse needs of Brazil's computational biology student community. This work presents the findings from the survey conducted by RSG-Brazil's EduComm and highlights the importance of localized educational initiatives in advancing bioinformatics education on a global scale."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"15:15-16:30","Format":"In Person","Speaker":null,"Title":"The impact of Student Council in your personal and scientific trajectory","Abstract":null},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"16:30-17:30","Format":"In Person","Speaker":"Martin Steinegger","Title":"Metagenomic sequence analysis: from protein sequences to structures","Abstract":"In metagenomics, DNA is sequenced directly from the environment, allowing us to study the vast majority of microbes that cannot be cultivated in vitro. This approach enables the real-time capture of pathogens, environmental monitoring, and access to a treasure trove of protein sequence diversity. However, annotating these metagenomes is particularly challenging, with many open reading frames remaining unannotated._x000D_\n_x000D_\nAdvancements in protein structure prediction through methods like AlphaFold2 and ESMFold, have resulted in the AlphaFold databases and ESMatlas predicting over 214 and 620 million structures, respectively. In this talk, I will discuss how this avalanche of structural data can be used to improve genomic and proteomic annotation through rapid searches and clustering, and explore its potential to transform our understanding of microbial diversity."},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"17:30-17:35","Format":"In Person","Speaker":null,"Title":"Introducing ISCB Student Council activities","Abstract":null},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"17:35-17:55","Format":"In Person","Speaker":null,"Title":"Closing remarks","Abstract":null},{"Track":"Student Council Symposium","Room":"520a","Weekday":"Friday","Date":"12 July","Timespan":"17:55-18:00","Format":"In Person","Speaker":null,"Title":"All on stage for picture\/photo of the event","Abstract":null},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"08:30-08:50","Format":"In Person","Speaker":null,"Title":"Welcome and Introduction to SysMod!","Abstract":null},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"08:50-09:30","Format":"In Person","Speaker":null,"Title":"Keynote Speaker Talk","Abstract":null},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"09:30-09:50","Format":"In Person","Speaker":"Mingyang Lu","Title":"Building high quality dynamical models of gene regulatory circuits driving cellular state transitions using scRNA-seq data","Abstract":"A major question in systems biology is to elucidate the gene regulatory mechanisms of cellular state transitions during developmental processes like cell differentiation and disease progression such as tumorigenesis. The advances in single-cell RNA-sequencing (scRNA-seq) technology has enabled an enhanced understanding of the dynamics of genome-wide gene expression. Yet, establishing gene regulatory networks driving cellular state transitions using scRNA-seq data remains challenging for a mechanistic understanding of cellular state transitions. Here, we introduce NetDes, a combined top-down bioinformatics and bottom-up systems biology approach, aimed at computationally generating ODE-based nonlinear dynamical models of core transcription factor regulatory circuits that recapitulate observed gene expression time trajectories. Our in-silico benchmarking demonstrates the advantage of NetDes in inferring the ground-truth regulators and their combinations. We applied NetDes to build the core regulatory circuit driving the differentiation of human iPSC to definitive endoderm using time series scRNA-seq data. The constructed gene circuit captures the regulatory interactions between stemness and Epithelial-Mesenchymal Transition (EMT) during this cell differentiation. Compared to existing network construction methods, NetDes has the advantage in capturing the gene expression dynamics during cellular state transitions using a single dynamical circuit model. Additionally, we performed systems biology simulations on the established ODE model to identify possible regulators and their combinations to drive the observed gene expression dynamics of the system. Our approach paves the way for a high-quality mechanistic modeling of the gene regulation of cellular state transitions."},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"09:50-10:00","Format":"In Person","Speaker":"Humza Hemani","Title":"Deciphering epigenetic regulatory mechanisms of IFNg-induced Epithelial to Mesenchymal Transition in human breast cells using systems  approach","Abstract":"Epigenetics changes within the cellular microenvironment play a significant role in both normal tissue development and the initiation and advancement of breast cancer. The extracellular matrix, a crucial element of the cellular microenvironment, engages with growth factors to modulate cellular behaviors that contribute to growth, progression, and metastasis. Interferon-gamma (IFNÎ³) is a cytokine based growth factor known for its immunomodulatory effects, primarily in the context of the immune response against pathogens and cancer has been implicated in influencing cancer cell behavior, including epithelial to mesenchymal transition (EMT), a process involved in cancer progression and metastasis. However, the epigenetic regulation of IFNÎ³ (interferon gamma)-induced EMT particularly during breast cancer development, remain inadequately elucidated. Using a previously developed tensor-based HOCMO (Higher Order Correlation Model) on multi-omics data, we describe the modulatory mechanisms of EMT during breast cancer progression, focusing on epigenetic regulation and IFNÎ³ induction that target these epigenetic modifiers. Using HOC scores on proteomics data (mass spectrometry, reverse phase array) as well as RNAseq, ATACseq, and CycIF data we identified a  histone mark associated with IFNÎ³-induced EMT pathway of breast cells. We further validate our finding using CUT&RUN experiments. An expanding description of the epigenetic regulations that underlie the contribution of histone specific IFNÎ³-induced EMT to cancer progression will provide momentous insights for â€œimmunoepidrugâ€ to treat cancer progression and metastasis."},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:00","Format":"Live Stream","Speaker":"Tatum Liparulo","Title":"Mathematical Modeling suggests that Monocyte Activity may drive Sex Disparities during Influenza Infection","Abstract":"In humans, females of reproductive age are at greater risk than their male, age-matched counterparts for hospitalization and death from influenza infection. The innate immune response has been implicated as a factor of these sex differences in influenza pathogenesis. This study is based on the hypothesis that sex-specific outcomes emerge due to differences in rates\/speeds of select immune component responses.  We modified an existing mathematical model and fit the model to data from male and female mice infected with influenza to identify sex-specific rates of male and female immunoregulation. We implemented a large computational screen to rapidly identify immune rates that may be sex-specific. We used Bayesian information criteria (BIC) to guide scenario selection because the BIC balances the goodness of fit of the competing models against model complexity. Our results suggest that having sex-specific rates for proinflammatory monocyte induction by interferon and monocyte activity, provides the simplest (lowest BIC) explanation for the difference observed in the male and female responses. Markov-chain Monte Carlo (MCMC) analysis and global sensitivity analysis of the top model was performed to provide rigorous estimates of the sex-specific parameter distributions and provide insight into which parameters most effect innate immune responses.  Simulations using the top-performing model suggest that monocyte activity could be targeted to reduce influenza disease severity in females. Overall, our Bayesian statistical and dynamic modeling approach suggests that monocyte activity and induction parameters are sex-specific and may explain sex-differences in influenza disease immune dynamics."},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"Jennifer Riccio","Title":"Predictive Modeling and Experimental Control of Macrophage Pro-Inflammatory Dynamics","Abstract":"Macrophages are immune cells which play a key role in the reaction to biomaterials. They exhibit a functional phenotype (or state) induced by the stimulus received and conditions of the microenvironment. This polarization process is governed by specific cytokines that are released by the macrophage itself, as well as produced by other cellular activation mechanisms. Cytokines act as phenotype markers within a heterogeneous range whose extremes are historically identified as pro-inflammatory or M1 and anti-inflammatory or M2._x000D_\n_x000D_\nIn such a context, this work aims to propose a predictive modeling approach for the simulation of the response to a pro-inflammatory stimulus in macrophages. This will allow us to subsequently simulate the immune reaction induced by the presence of biomaterials at the cellular level, with the final goal to build a digital twin of the inflammatory response in a foreign body reaction. To do that, existing Ordinary Differential Equation (ODE) and Agent Based (AB) models have been considered and validated with in-vitro experimental data._x000D_\n_x000D_\nPreliminary results highlight a better agreement of the AB approach over the ODE models taken into account in this work._x000D_\nThis specific scheme is making simplified assumptions on spatial resolution and diffusion of inflammation (both cytokine and macrophages). However, the good agreement that we have observed in this simplified model encourages the use of a more advanced and comprehensive hybrid simulation platform based on AB modeling which implements a more thorough description of the intracellular pathways and the microenvironment."},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Imran Shah","Title":"Deciphering Cellular Fate Decisions: A Boolean Network Approach to Stress Response Network Tipping Points","Abstract":"Adaptive stress response networks (SRNs) are invoked when chemical exposures induce DNA damage, oxidative stress, unfolded proteins, hypoxia, or heat shock and are essential for maintaining cell health. With a highly conserved architecture for sensing and countering cellular stress, SRNs are also pivotal in activating senescence, apoptosis, and autophagy pathways if stress cannot be resolved. Perturbing SRNs beyond some threshold tips cells over from adaptive to adverse phenotypes. We are investigating these critical \"\"tipping points\"\" using a combined approach of literature mining, computational modeling, and high-throughput data analysis. We aim to elucidate how SRN dynamics dictate cellular phenotypes and propose Boolean Networks (BNs) to identify these tipping points by: 1) Constructing a biological knowledge graph (KG) of chemical stress inducers; 2) Utilizing the KG to build BNs and simulate SRNs; and, 3) Validating predictions with transcriptomic data from HepaRG cells. Herein, we describe the KG of 500 chemical stress inducers for which we developed BNs to simulate dynamic cell trajectories in DNA damage response for drugs and chemicals. By elucidating the molecular determinants of tipping points, this research furthers our understanding of cellular resilience in health and toxicity. _x000D_\n_x000D_\n_x000D_\nThis abstract does not reflect US EPA policy."},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Matteo Barberis","Title":"Integrative Systems and Synthetic Biology identifies a Yeast Minimal Cell Cycle network that coordinates cell proliferation dynamics","Abstract":"The eukaryotic cell cycle is driven by waves of cyclin-dependent kinase (cyclin\/Cdk) activities that rise and fall with a timely pattern called â€œwaves of cyclinsâ€. This pattern guarantees coordination and alternation of DNA synthesis with cell division. Through computational modelling, we have recently identified a minimal network underlying cyclin\/Cdk1 autonomous oscillations in budding yeast that we name the Yeast Minimal Cell Cycle. Here, we first explore by â€˜learning from buildingâ€™ whether these cell cycle oscillations may be achieved by synthesizing a functional genome consisting of a minimal set of cell cycle genes. We consider nine genes involved in the waves of cyclins: G1 cyclins (CLN1,2), mitotic cyclins (CLB1-6), and their positive and negative regulators (FKH1,2 and SIC1, respectively). Selected genes were pairwise deleted by CRISPR from their native loci in the yeast genome and simultaneously relocated with their native promoters into a synthetic gene cluster in the same cell. We then remove combinations of genes from this cluster. The frequency of gene loss and the growth rates of these strains were analysed and compared to kinetic models of the cyclin\/Cdk network that are verified against quantitative data of Clb dynamics. We unravel a novel molecular design that synchronizes Clb\/Cdk1 oscillations. Through integration of Synthetic and Systems Biology, this work shows that a minimal set of genes of the Yeast Minimal Cell Cycle can reproduce cell cycle oscillations, indicating that the genetic complexity of the yeast cell cycle can be reduced to identify a novel molecular network underlying cell proliferation dynamics."},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Vishvak Raghavan","Title":"Harnessing Agent-Based Modeling in CellAgentChat to Unravel Cell-Cell Interactions from Single-Cell Data","Abstract":"Understanding cell-cell interactions (CCIs) is essential yet challenging due to the inherent intricacy and diversity of cellular dynamics. Existing approaches often analyze global patterns of CCIs using statistical frameworks, missing the nuances of individual cell behavior due to their focus on aggregate data. This makes them insensitive in complex environments where the detailed dynamics of cell interactions matter. We introduce CellAgentChat, an agent-based model (ABM) designed to decipher CCIs from single-cell RNA sequencing and spatial transcriptomics data. This approach models biological systems as collections of autonomous agents governed by biologically inspired principles and rules. Validated against seven diverse single-cell datasets, CellAgentChat demonstrates its effectiveness in detecting intricate signaling events across different cell populations. Moreover, CellAgentChat offers the ability to generate animated visualizations of single-cell interactions and provides flexibility in modifying agent behavior rules, facilitating thorough exploration of both close and distant cellular communications. Furthermore, CellAgentChat leverages ABM features to enable intuitive in silico perturbations via agent rule modifications, pioneering new avenues for innovative intervention strategies. This ABM method empowers an in-depth understanding of cellular signaling interactions across various biological contexts, thereby enhancing in-silico studies for cellular communication-based therapies."},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Da-Wei Lin","Title":"Metabolic Objectives and Trade-offs in Single-cells during Cellular Transitions","Abstract":"Cell-type transitions, crucial for processes including cell quiescence, cell cycle, and embryogenesis, involve intricate metabolic rewiring to optimize competing biological objectives. The competition for cellular resources is often explored through the lens of Pareto optimality and metabolic trade-offs. Despite advancements in understanding these dynamics in unicellular organisms, the metabolic trade-offs in multicellular systems, especially during embryonic cell-state transitions, remain largely unexplored. Addressing this gap, we introduce the Single Cell Optimization Objective and Tradeoff Inference (SCOOTI) framework, a novel computational approach grounded in optimization theory, designed to infer cell-specific metabolic objectives from omics data. By integrating gene expression, protein abundance, and metabolite concentration data with genome-scale metabolic models, SCOOTI leveraged meta-learner regressors to quantitatively elucidate the metabolic objectives underlying cell quiescence, proliferation, and embryogenesis. Our analysis reveals distinct metabolic objectives across cell-cycle phases and embryonic development stages, highlighting the role of specific metabolites in driving these transitions. Notably, the framework uncovers a shift from a high entropy, multitasking metabolic system in early embryogenesis to a more deterministic metabolic focus on biomass production and cell growth in later stages. This shift is exemplified by the trade-offs between glutathione-mediated redox balance and biomass precursor synthesis, suggesting a Pareto optimality scenario where balancing redox status and growth-related objectives is crucial for embryonic development. Our findings challenge the traditional biomass maximization model, proposing instead that cellular metabolic objectives are highly context-dependent, varying significantly between quiescent and proliferative states, and across developmental stages. "},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"14:40-14:50","Format":"In Person","Speaker":"Ragothaman Yennamalli","Title":"Structural Systems Biology of Levan Biosynthesis in Bacillus subtilis","Abstract":"Bacillus subtilis is a key organism in biotechnology, with its metabolic capabilities offering potential for various valuable products like levan. Levan, a fructose polymer, has diverse applications such as the formulation of hydrogels, drug delivery, and wound healing, among others. But B. subtilisâ€™ metabolic models till date are not that well-annotated as compared to other model organismsâ€™ metabolic models and the exploitation of an organismâ€™s metabolic capability requires an improved model with expanded gene coverage. So, this study aims to enhance the metabolic model of B. subtilis for improving levan biosynthesis, addressing the need for accurate prediction of metabolic outcomes in industrial applications. We used      structural systems biology technique, which offers a promising approach to enhance predictive power, for the refinement of the levan biosynthesis pathway in B. subtilis. Using AlphaFold2, structural models were generated for critical genes lacking full-length PDB structures, which were then integrated into the model. Findings elucidate previously overlooked structural aspects of levan biosynthesis, while leveraging the STRING database to optimize product yield. We provide a detailed understanding of the levan biosynthesis in B. subtilis, shedding light on previously overlooked structural aspects of a pathway. This metabolic model acts as an input to further set of applications in advancing metabolic engineering efforts."},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"14:50-15:30","Format":"In Person","Speaker":"Melissa Kemp","Title":"Simple rules of intercellular communication for modeling emergent multicellular organization","Abstract":"Engineering multicellular systems is enhanced by understanding how collective organization arises during developmental processes through mechanical, biochemical and electrical communication. Which aspects of these processes can be circumvented, accelerated or modified according to specification to yield robust, reproducible organoids? Computational models that simulate the growth, division, and differentiation of pluripotent cells into emergent structures could accelerate experimental design, yet currently lag in their ability to inform organoid culture protocols. I will discuss my lab's computational results from developing agent-based models that capture heterogeneity and stochasticity within colonies and aggregates to both i) formulate hypotheses of intercellular communication during stem cell differentiation and ii) design new organoid structures using synthetic biology components. To address the challenges of agent-based model optimization, we have pursued new methods for analyzing microscopy images and simulation results by topological data analysis. Through a tight iteration between computation and experimentation, we established a critical role of intercellular transport, adhesion, and cell cycle asynchrony in the propagation of dynamic patterning in engineered iPSC systems. "},{"Track":"SysMod","Room":"525","Weekday":"Tuesday","Date":"16 July","Timespan":"15:30-15:40","Format":"In Person","Speaker":null,"Title":"Closing Words","Abstract":null},{"Track":"Technology Track","Room":"524c","Weekday":"Saturday","Date":"13 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Shoba Ranganathan","Title":"HPC-AI Support for Singaporeâ€™s Bioinformaticians and Computational Biologists ","Abstract":"NSCC Singapore was established in 2015 as a National Research Infrastructure and manages Singaporeâ€™s first national petascale facility with high-performance computing (HPC) resources. NSCC supports the HPC and Artificial Intelligence (AI) needs of the Singapore research community, accelerating innovative solutions, exemplified by select biological and biomedical case studies. "},{"Track":"Technology Track","Room":"524c","Weekday":"Saturday","Date":"13 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"Robyn Ball","Title":"Traversing the mouse-human interface with a knowledge graph of analytic and data services ","Abstract":"Functional genomics has generated a wealth of gene regulatory information across species and research has shown that variants can be identified within each species that have similar effects on orthologous targets. We developed a knowledge graph across data resources and analytical services for cross-species analysis that includes GenomeMUSter (https:\/\/muster.jax.org), the Mouse Phenome Database (https:\/\/phenome.jax.org\/), the meta-analysis server integrated with GenomeMUSter and the extensive collection of mouse phenotypic measurements in MPD, GeneWeaver (https:\/\/geneweaver.org), and VariantGraph db. _x000D_\n_x000D_\n     - GenomeMUSter is comprehensive and uniformly dense mouse variant resource comprised of imputed and measured allelic state data for 657 mouse strains at 106.8M sites    _x000D_\n     - The Mouse Phenome Database (MPD) is an NIH-recognized Biomedical Data Repository, curated and annotated with community standard ontologies, and integrated with a suite of analytical tools, including the meta-analysis server. _x000D_\n     - GeneWeaver is a curated repository of genomic experimental results and ontology resources with an analytical tool suite that enables users to perform cross-species integrated functional genomics   _x000D_\n     - VariantGraph database is a Neo4j graph comprised of 32B relations among mouse and human variants, transcripts, genes, and regulatory peaks that enables evidenced-based identification of variants and genes with similar effects on orthologous targets   _x000D_\n_x000D_\nWe will demo the knowledge graph and illustrate approaches to identify and characterize mouse-human effects with orthologous targets using motivating examples of cross-species multi-population multi-trait analytical approaches_x000D_\n_x000D_\n"},{"Track":"Technology Track","Room":"524c","Weekday":"Saturday","Date":"13 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Erik Sonnhammer","Title":"Network analyses for functional annotation with FunCoup tools","Abstract":"The FunCoup database (https:\/\/FunCoup.org) provides comprehensive functional association networks of genes\/proteins that were inferred by integrating massive amounts of multi-omics data, combined with orthology transfer. We will showcase how its unique online tools can be used to gain functional insights and answer scientific questions with network biology."},{"Track":"Technology Track","Room":"524c","Weekday":"Saturday","Date":"13 July","Timespan":"11:40-12:20","Format":"In Person","Speaker":"Rami Mehio","Title":"Advances towards comprehensive and accurate whole genome analysis at scale using DRAGEN accelerated algorithms","Abstract":"Research and medical genomics require comprehensive and scalable solutions to drive_x000D_\nthe discovery of novel disease targets, evolutionary drivers, and genetic markers with_x000D_\nclinical significance. This necessitates a framework to identify all types of variants_x000D_\nindependent of their size (e.g., SNV\/SV) or location (e.g., repeats). Here we present_x000D_\nDRAGEN that utilizes novel methods based on multigenomes mapping, hardware_x000D_\nacceleration, and machine learning based variant detection to provide novel insights_x000D_\ninto individual genomes with ~30min computation time (from raw reads to variant_x000D_\ndetection). DRAGEN outperforms all other state-of-the-art methods in speed and_x000D_\naccuracy across all variant types (SNV, indel, STR, SV, CNV) and further incorporates_x000D_\nspecialized methods to obtain key insights in medically relevant genes (e.g., HLA, SMN,_x000D_\nGBA). We showcase DRAGEN across 3,202 genomes and demonstrate its scalability,_x000D_\naccuracy, and innovations to further advance the integration of comprehensive_x000D_\ngenomics for research and medical applications."},{"Track":"Technology Track","Room":"524c","Weekday":"Saturday","Date":"13 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Etai Jacob","Title":"Enhancing Clinical Trial Outcomes with AI-based Predictive Biomarker Discovery via Contrastive Learning ","Abstract":"Modern clinical trials capture numerous clinicogenomic measurements. Manual discovery of predictive biomarkers is challenging. We introduce a framework which explores predictive biomarkers in a systematic and unbiased manner using contrastive learning. Applied to real data, our framework found biomarkers identifying IO-treated individuals who survive longer than those treated with chemotherapy. "},{"Track":"Technology Track","Room":"524c","Weekday":"Saturday","Date":"13 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":"Gwenn Berry","Title":"Miqa: Automating bioinformatics testing, evaluation and validation for real-time performance data & instant bug detection on every code change","Abstract":"Evaluation of bioinformatics pipeline performance (accuracy, robustness, and consistency) is critical both for developing and optimizing top-class algorithms, and for proving and maintaining the quality and reproducibility of these tools. Bioinformatics data is large, complex, and challenging to test, and many teams lack the time and resources to test effectively and efficiently, slowing development and introducing downstream risks and maintenance burdens.  _x000D_\n_x000D_\nMiqa is a biologist-friendly software quality assurance (QA) platform that can automate bioinformatic tool validation, benchmarking or troubleshooting as frequently as every code change.  It allows you to build custom tests & benchmarking metrics for any data type, and is agnostic to programming language (Python, R, C++, etc.), workflow & containerization technologies (Nextflow, Snakemake, Docker) and cloud\/execution platforms.  We will demonstrate how to easily set-up automated software tests, customize QA metrics and generate interactive reports for comprehensive bioinformatic evaluation within minutes, on common data types like BAM, FASTQ, VCF, BED, and CSV\/TSV\/JSON, as well as custom pipeline outputs, and how it can be applied to a variety of technology types and disciplines."},{"Track":"Technology Track","Room":"524c","Weekday":"Saturday","Date":"13 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Aurélien Luciani","Title":"UniProt: The Universal Protein resource: new features, access and tools for protein data ","Abstract":"The Universal Protein resource â€“ UniProt â€“ after its more than 20 years of existence, is now a_x000D_\nfundamental component in the bioinformatics and molecular biology community, providing a_x000D_\ncomprehensive, high-quality, and freely accessible resource of protein sequence and functional_x000D_\ninformation. Recognizing the continuous evolution in data analysis needs and technology, and the_x000D_\nexponential growth of biological data, UniProt has undergone a significant update to enhance its_x000D_\ninterface, functionalities, and overall user experience. This presentation aims to introduce these_x000D_\ntransformative changes to the participants of the conference._x000D_\nOur presentation will:_x000D_\n- Showcase the updated look and improved navigational functionalities of the new UniProt_x000D_\nwebsite._x000D_\n- Detail the advancements in the API that facilitate more efficient data retrieval and_x000D_\nintegration._x000D_\n- Highlight the enhanced data visualization tools that provide intuitive insights into protein_x000D_\nfunctions, interactions, and structures._x000D_\n- Demonstrate the optimized processes for data export and sharing, which cater to both_x000D_\nacademic and industrial research needs._x000D_\n- Engage with both new and veteran UniProt users to gather feedback and discuss potential_x000D_\nfuture enhancements, helping us define a development roadmap that is based on user feedback"},{"Track":"Technology Track","Room":"524c","Weekday":"Saturday","Date":"13 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":null,"Title":"TBD","Abstract":"TBD"},{"Track":"Technology Track","Room":"524c","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Kedar Patwardhan","Title":"Utilizing Pre-Treatment Lab Values & Whole-Lung Radiomics for Modeling Survival Risk for ICB in the mNSCLC Setting ","Abstract":"At AstraZeneca Oncology Data Science, we committed to unlock the potential of AI\/ML-driven data science. Here we demonstrate that pre-treatment clinical lab values and non-invasive CT imaging features can be used to model survival risk in the mNSCLC setting. This is an important step towards improving patient access to Immunotherapy. "},{"Track":"Technology Track","Room":"524c","Weekday":"Monday","Date":"15 July","Timespan":"11:00-11:20","Format":"In Person","Speaker":"Felipe Pérez-Jvostov","Title":"Enhancing Genomic Research through National Collaboration: The Role of Canada's National Data Platform","Abstract":"National data infrastructure is a critical enabler of Canadaâ€™s genomic research and community-driven collaboration. The success of such infrastructure is dependent on its capacity to address growing demands for data and its ability to enable a diverse range of resource-intensive computational activities. This presentation will focus on the challenges and opportunities of establishing such national data infrastructure in the Canadian context, and highlight the importance of data interoperability across domains and data types to fuel research and innovation in genomic science and beyond."},{"Track":"Technology Track","Room":"524c","Weekday":"Monday","Date":"15 July","Timespan":"11:20-11:40","Format":"Live Stream","Speaker":"Alessia David","Title":"The Missense3D portal: Structure-based evaluation of missense variants including protein complexes and transmembrane regions ","Abstract":"Missense3D (http:\/\/missense3d.bc.ic.ac.uk\/) predicts the impact of missense variants on protein structure and reports their structural impact e.g. burial of charged residues. A user can assess the impact of a variant on a monomeric structure, including its transmembrane region, or on a protein complex. Missense3D accepts any structure, including AlphaFold models. "},{"Track":"Technology Track","Room":"524c","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Suzanne Paley","Title":"Integrated Pathway\/Genome\/Omics Informatics in Pathway Tools and BioCyc ","Abstract":"An overview of the BioCyc website and Pathway Tools software suite, which features an extensive array of capabilities covering genome informatics, pathway informatics, regulatory informatics, and omics data analysis. Several new capabilities will be demonstrated, including multi-omics visualization tools, a new genome browser, and the comparative genome dashboard. "},{"Track":"Technology Track","Room":"524c","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Nicola Bordin","Title":"CATH and TED: Protein structure classification in the age of AI ","Abstract":"CATH, now up-to-date with the Protein Data Bank, created with the group of David Jones at UCL the TED resource, classifying over 200m domains from AFDBv4 within the CATH classification and identified over 7k novel folds. TED offers community access via a dedicated web resource, facilitating data visualization and downloads. "},{"Track":"Technology Track","Room":"524c","Weekday":"Monday","Date":"15 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Paulina Dragan","Title":"GPCRVS â€“ a machine learning system for GPCR drug discovery ","Abstract":"GPCRVS (https:\/\/gpcrvs.chem.uw.edu.pl) is an efficient machine learning system for the online assessment of the compound activity against several GPCR targets, including peptide and protein-binding GPCRs, the most difficult in virtual screening. GPCRVS evaluates compounds in terms of their activity range, pharmacological effects, and binding modes. "},{"Track":"Technology Track","Room":"524c","Weekday":"Monday","Date":"15 July","Timespan":"14:40-15:00","Format":"In Person","Speaker":null,"Title":"Modelling multi-omic, real-world data reveals immunogenomic drivers of resistance to cancer immunotherapy","Abstract":"At AstraZenecaâ€™s Oncology Data Science, we committed to unlock the potential of AI\/ML-driven data science. Here, we model clinical endpoints together with >10.000 of DNA and RNA profiled tumour samples from patients progressing on immune checkpoint blockade (ICB). We uncover that the post-ICB tumour microenvironment is fundamentally different in acquired vs primary resistance. At AstraZenecaâ€™s Oncology Data Science, we committed to unlock the potential of AI\/ML-driven data science. Here, we model clinical endpoints together with >10.000 of DNA and RNA profiled tumour samples from patients progressing on immune checkpoint blockade (ICB). We uncover that the post-ICB tumour microenvironment is fundamentally different in acquired vs primary resistance. "},{"Track":"Technology Track","Room":"524c","Weekday":"Monday","Date":"15 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":null,"Title":"Decoding the grammar of DNA using Natural Language Processing","Abstract":"DNA is the blueprint defining all living organisms. Therefore, understanding the nature and_x000D_\nfunction of DNA is at the core of all biological studies. Rapid advances in DNA sequencing_x000D_\nand computing technologies over the past few decades resulted in large quantities of DNA_x000D_\ngenerated for diverse experiments, exceeding the growth of all major social media platforms_x000D_\nand astronomy data combined. However, biological data is both complex and_x000D_\nhigh-dimensional, and is difficult to analyse with conventional methods._x000D_\nMachine learning is naturally well suited to problems with a large volume of data and_x000D_\ncomplexity. In particular, applying Natural Language Processing to the genome is_x000D_\nintuitive, since DNA is a natural language. Unique challenges exist in Genome-NLP over_x000D_\nnatural languages, including the difficulty of word segmentation or corpus comparison._x000D_\nTo tackle these challenges, we developed the first automated and open-source genomeNLP_x000D_\nworkflow that enables efficient and accurate knowledge extraction on biological data,_x000D_\nautomating and abstracting preprocessing steps unique to biology. This lowers the barrier to_x000D_\nperform knowledge extraction by both machine learning practitioners and computational_x000D_\nbiologists."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Arvind Mer","Title":"Combining computational pipelines and text mining to build a cell type knowledge graph resource","Abstract":"Advances in sequencing technologies are now allowing for the comprehensive analysis of whole genome structure (epigenomic) and expression (transcriptomic) characteristics of individual single cells. This is revealing the cellular complexity of healthy, diseased, and perturbed tissues at unprecedented granularity.  However, the knowledge derived from the analysis and interpretation of these experiments is currently only available as free text in scientific publications, making their exploration challenging and labor intensive.  To address this, we are developing a cell type knowledgebase resource to capture knowledge about cell phenotypes from these experiments.  Two streams of knowledge are being established.  First, using standard cell-by-gene expression matrices as input, validated analysis pipelines are used to produce information about cell type-specific marker genes and differential expression patterns, linked with experiment metadata about species and specimen sources, disease states, and perturbations.  Second, using open access peer-reviewed publications from PubMed Central (PMC) reporting results from single cell genomics experiments as input, AI-driven natural language processing (NLP) pipelines are being used to extract information about cell phenotypes and their associations with disease states and perturbation responses.  The outputs of these knowledge generating pipelines are then formulated as standardized semantically-structured assertions of subject-predicate-object triple statements compatible with storage using semantic web technologies and graph database platforms for search, analysis, and integration with other sources of knowledge about diseases and drugs, especially from NCBI and other NLM resources, for discovery of novel diagnostic biomarkers and therapeutic targets. "},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Maaly Nassar","Title":"Enhancing Machine Learning Based Drug Response Prediction Models via Text Mining-Driven Feature Selection Approach","Abstract":"Predicting anticancer treatment responses from baseline genomic data is a formidable challenge in personalized cancer medicine. Machine learning is increasingly employed to interpret drug responses from gene expression data, but a significant hurdle is the selection of relevant features. The large number of features can make training models time-consuming, computationally intensive, and less accurate. Hence, effective feature selection is essential to reduce complexity, and improve interpretability, leading to more efficient and accurate models. We propose a text-mining-based feature selection method that leverages peer-reviewed scientific literature to identify gene-drug interactions. By mining extensive literature from databases like PubMed, our method pinpoints genes associated with specific drugs, prioritizing them for pharmacogenomic analysis using a Bayesian linear classifier and Fisherâ€™s exact test. To assess the efficacy of our approach, we compared it against eight advanced feature selection methods using three different machine learning algorithms, Elastic Net, Random Forest, and deep learning across two independent cancer pharmacogenomic datasets. Our text-mining-based features showed a strong correlation with drug responses, outperforming other methods in both univariate and multivariate analyses. Specifically, models trained with our features excelled in within-domain and cross-domain validations, consistently achieving the highest prediction accuracy. These models were also effective in predicting drug responses in patient-derived xenograft pharmacogenomics datasets._x000D_\nOur text-mining-based feature selection method offers a robust, efficient, and scalable way to identify relevant genetic features for drug response prediction, enhancing the interpretability of predictive models by linking gene selection to documented biological relevance in scientific literature."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"11:20-11:40","Format":"Live Stream","Speaker":"Cathy Wu","Title":"Streamlining Drug Development with Conversational AI-Powered Knowledge Graphs: From Preclinical Discovery to Clinical Trials","Abstract":"The drug development industry faces an efficacy crisis, with a 90% clinical trial failure rate, an average of 9 years, and $1.5 billion spent on bringing a new drug to market. This is largely due the complex process of clinically translating and validating drug-target cellular machinery within extensive scientific literature. To tackle this challenge, we applied conversational AI-powered knowledge graphs (KGs) to various aspects of the drug development process, from preclinical drug discovery and repurposing to matching patients with clinical trials.  Our strategy includes: 1) creating FAIR (Findable, Accessible, Interoperable, and Reusable) knowledge graphs with ontology-validated Named Entity Recognition, 2) leveraging embeddings models and fine-tuned large language models (LLMs) for deciphering biomedical relationships, 3) identifying key therapeutic targets and adverse reactions using AI-driven graph analysis and 4) harnessing LLMs reasoning capabilities to match patients to clinical trials.   _x000D_\n_x000D_\nWe showcase how conversational AI-powered KGs can enhance microbiome repurposing and streamline patient matching with high accuracy. Using benchmark datasets like BIOASQ, we assessed various embedding models for capturing both structural and semantic relationships among biomedical entities. We further illustrate how fine-tuning LLMs, such as BioGPT, with synthetic datasets can accurately improve their understanding of biomedical contexts, classify the regulatory effects of intermingled biological entities and assess patients' eligibility for clinical trials based on matching criteria. Expanding on these methods, we employ AI-driven graph analysis techniques (e.g., graph centrality metrics) to precisely identify key therapeutic and diagnostic targets for specific diseases and drugs, ultimately improving the overall success rate in developing innovative treatments."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":null,"Title":"eMIND: Enabling automatic collection of protein variation impacts in Alzheimerâ€™s disease from the literature","Abstract":"Alzheimerâ€™s disease and related dementias (AD\/ADRDs) are among the most common forms of dementia, and yet no effective treatments have been developed. To gain insight into the disease mechanism, capturing the connection of genetic variations to their impacts, at the disease and molecular levels, is essential. The scientific literature continues to be a main source of information about the impact of variants and thus, the development of automatic methods to extract such information from literature would be useful to assist biocuration. We developed eMIND, a deep learning-based text mining system that supports the automatic extraction of annotations of variants and their impacts in AD\/ADRDs. In particular, we capture the impacts of protein-coding variants affecting a selected set of protein properties, such as protein activity\/function, structure and post-translational modifications. A major hypothesis we are testing is that the structure and words used in statements that describe the impact of one entity on another entity or event\/process are not specific to the two objects under consideration. Thus, a BERT model was fine-tuned using a training dataset with 8,245 positive and 11,496 negative impact relations derived from impact involving microRNAs. We conducted a preliminary evaluation on a small manually annotated corpus (60 abstracts) consisting of variant impact relations from AD\/ADRDs literature and obtained a recall of 0.84 and a precision of 0.94. The publications and extracted information by eMIND are integrated into the UniProtKB computationally mapped bibliography."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Peipei Ping","Title":"Poster Flash Presentations","Abstract":null},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"14:20-15:00","Format":"In Person","Speaker":"Inge Holtman","Title":"An informatic path to better understanding of cardiovascular biology and medicine","Abstract":"We will present an overview on our bioinformatics platforms as well as our use cases applying text mining approaches to better understand cardiovascular biology and medicine. "},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Zhiyong Lu","Title":"The Netherlands Neurogenetics Database: Reveiling clinical, neuropathological and genetic heterogeneity of brain-disorders","Abstract":"The brain is susceptible to a wide-range of neurodegenerative disorders, that share pathophysiological mechanisms, genetic risk factors, and are frequently clinically misdiagnosed[1]. Hence, there is a clear need for a data-driven delineation of the pathophysiological mechanisms of brain disorders, for improved diagnosis and prognosis. To this end, we established the Netherlands Neurogenetics Database (http:\/\/nnd.app.rug.nl\/) which aims to integrate the extensive clinical, neuropathological, genetic data of large collection of brain donors (+\/- 3000) from the Netherlands Brain Bank. We recently implemented Large Language Models (LLMs) to convert medical record summaries into clinical disease trajectories[2]. These trajectories included many known and novel disease specific symptoms, and were used for disease prediction, and disease subtyping and resulted in the identification of clinical subtypes of disease. Currently, weâ€™re implementing LLMs to process neuropathological examinations, giving us unprecedented insight into neuropathological state, which weâ€™ll relate back to the clinical heterogeneity. In addition, weâ€™re analysing common genetic variants (Illumina GSA-array), to refine current GWAS studies for neurodegenerative disorders, that typically include a considerable fraction of misdiagnosed individuals. Weâ€™re also calculating polygenic risk scores (PRS) to identify genetic features for clinical\/neuropathological subtypes and features. Together, these studies aim to give new data-driven insights into shared and unique features of neurodegenerative disorders. _x000D_\n_x000D_\n1. Revealing clinical heterogeneity in a large brain bank cohort. N.J. Mekkes, I.R. Holtman, Nat Med, 2024._x000D_\n2. Identification of clinical disease trajectories in neurodegenerative disorders with natural language processing. N. J. Mekkes, â€¦, B.J.L. Eggen, I.  Huitinga. I.R. Holtman, Nat Med, 2024."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Andrew Tran","Title":"GeneAgent: Self-verification Language Agent for Gene Set Knowledge Discovery","Abstract":"Genomics has been a research interest of molecular biologists for a long time. Recent studies have shown promising results by harnessing the instruction learning in Large Language Models (LLMs). Nonetheless, these methods did not explore LLMs in-depth to accurately identify biological functions of gene sets and are hindered by the issue of hallucinations. In response, we present GeneAgent, a first-of-its-kind language agent equipped with the self-verification capability to autonomously interact with domain-specific databases. GeneAgent contains four stages (i.e., generation, self-verification, modification, and summarization), which creates the process name and analytical narratives for the input gene set and activates the self-verification agent for verifying them respectively. Different stages of self-verification are cascaded through the modification module. After self-verification, GeneAgent produces the final response for the given gene set based on the verification report. Benchmarking on multiple gene sets in Gene Ontology, NeST, and MsigDB, GeneAgent achieves higher accuracies than the standard GPT-4 by a significant margin. Notably, for 15 gene sets (1.4%), GeneAgent accurately predicted the reference terms with 100% precision, compared with only 3 cases (0.3%) by GPT-4. Additionally, our enriched term tests demonstrate that GeneAgent can provide targeted gene synopsis for summarizing multiple biological terms in alignment with conventional enrichment analyses. Detailed case studies demonstrate that GeneAgent can effectively reduce hallucination issues in GPT-4 and generate reliable analytical narratives for gene functions. As such, GeneAgent stands as a robust solution for gene set knowledge discovery and can provide reliable insights for future research endeavors."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Xiangru Tang","Title":"MolLM: A Unified Language Model for Integrating Biomedical Text with 2D and 3D Molecular Representations","Abstract":"The current paradigm of deep learning models for the joint representation of molecules and text primarily relies on 1D or 2D molecular formats, neglecting significant 3D structural information that offers valuable physical insight. This narrow focus inhibits the modelsâ€™ versatility and adaptability across a wide range of modalities. Conversely, the limited research focusing on explicit 3D representation tends to overlook textual data within the biomedical domain. We present a unified pre-trained language model, MolLM, that concurrently captures 2D and 3D molecular information alongside biomedical text. MolLM consists of a text Transformer encoder and a molecular Transformer encoder, designed to encode both 2D and 3D molecular structures. To support MolLMâ€™s self-supervised pre-training, we constructed 160K molecule-text pairings. Employing contrastive learning as a supervisory signal for cross-modal information learning, MolLM demonstrates robust molecular representation capabilities across 4 downstream tasks, including cross-modality molecule and text matching, property prediction, captioning, and text-prompted molecular editing. Through ablation, we demonstrate that the inclusion of explicit 3D representations improves performance in these downstream tasks."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Minbyul Jeong","Title":"BioCoder: A Benchmark for Bioinformatics Code Generation with Large Language Models","Abstract":"Pre-trained large language models have significantly improved code generation. As these models scale up, there is an increasing need for the output to handle more intricate tasks and to be appropriately specialized to particular domains. Here, we target bioinformatics due to the amount of specialized domain knowledge, algorithms, and data operations this discipline requires. We present BioCoder, a benchmark developed to evaluate large language models (LLMs) in generating bioinformatics-specific code. BioCoder spans a broad spectrum of the field and covers cross-file dependencies, class declarations, and global variables. It incorporates 1026 Python functions and 1243 Java methods extracted from GitHub, along with 253 examples from the Rosalind Project, all pertaining to bioinformatics. Using topic modeling we show that overall coverage of the included code is representative of the full spectrum of bioinformatics calculations. BioCoder incorporates a fuzz-testing framework for evaluation. We have applied it to evaluate many models including InCoder, CodeGen, CodeGen2, SantaCoder, StarCoder, StarCoder+, InstructCodeT5+, GPT-3.5, and GPT-4. Furthermore, we finetuned StarCoder, demonstrating how our dataset can effectively enhance the performance of LLMs on our benchmark (by >15% in terms of Pass@K in certain prompt configurations and always >3%). The results highlight two key aspects of successful models: (1) Successful models accommodate a long prompt (> 2600 tokens) with full context, for functional dependencies. (2) They contain specific domain knowledge of bioinformatics, beyond just general coding knowledge. This is evident from the performance gain of GPT-3.5\/4 compared to the smaller models on the benchmark (50% vs up to 25%)."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":null,"Title":"Improving Medical Reasoning through Retrieval and Self-Reflection with Retrieval-Augmented Large Language Models","Abstract":"Recent proprietary large language models (LLMs), such as GPT-4, have achieved a milestone in tackling diverse challenges in the biomedical domain, ranging from multiple-choice questions to long-form generations._x000D_\nTo address challenges that still cannot be handled with the encoded knowledge of LLMs, various retrieval-augmented generation (RAG) methods have been developed by searching documents from the knowledge corpus and appending them unconditionally or selectively to the input of LLMs for generation._x000D_\nHowever, when applying existing methods to different domain-specific problems, poor generalization becomes apparent, leading to fetching incorrect documents or making inaccurate judgments. _x000D_\nIn this paper, we introduce Self-BioRAG, a framework reliable for biomedical text that specializes in generating explanations, retrieving domain-specific documents, and self-reflecting generated responses._x000D_\nWe utilize 84k filtered biomedical instruction sets to train Self-BioRAG that can assess its generated explanations with customized reflective tokens._x000D_\nOur work proves that domain-specific components, such as a retriever, domain-related document corpus, and instruction sets are necessary for adhering to domain-related instructions._x000D_\nUsing three major medical question-answering benchmark datasets, experimental results of Self-BioRAG demonstrate significant performance gains by achieving a 7.2% absolute improvement on average over the state-of-the-art open-foundation model with a parameter size of 7B or less._x000D_\nWe analyze that Self-BioRAG finds the clues in the question, retrieves relevant documents if needed, and understands how to answer with information from retrieved documents and encoded knowledge as a medical expert does._x000D_\nWe release our data and code for training our framework components and model weights _x000D_\n to enhance capabilities in biomedical and clinical domains."},{"Track":"Text Mining","Room":"524ab","Weekday":"Sunday","Date":"14 July","Timespan":"17:00-18:00","Format":"In Person","Speaker":null,"Title":"Leveraging AI, text mining and large language models to advance biology and medicine","Abstract":null},{"Track":"TransMed","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Ian Overton","Title":"Introduction","Abstract":null},{"Track":"TransMed","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Serdar Bozdag","Title":"Quality Assurance, Semantic Enrichment and Integration of Multimodal Health Data for Phenotype and Cohort Discovery with Deep Learning","Abstract":"Integration of data from multiple domains can greatly enhance the quality and applicability of knowledge generated in analysis workflows. However, working with health data is challenging, requiring careful preparation in order to support meaningful interpretation. We developed an R package for electronic health data preparation, â€œeHDPrepâ€ (Gigascience 2023;12:giad030, https:\/\/cran.r-project.org\/package=eHDPrep) demonstrated upon a multimodal colorectal cancer dataset (661 patients, 155 variables; Colo-661); a further demonstrator is taken from The Cancer Genome Atlas (459 patients, 94 variables; TCGA-COAD). eHDPrep offers user-friendly methods for quality control, including internal consistency checking and redundancy removal with information-theoretic variable merging (Figures 1, 2). eHDPrep also facilitates numerical encoding, variable extraction from free text, and completeness analysis. Semantic enrichment functionality can generate new informative â€œmeta-variablesâ€ according to ontological common ancestry, demonstrated with SNOMED CT and the Gene Ontology (Figure 3)._x000D_\nWe deployed variational autoencoders with a complex loss function evaluating reconstruction and clustering on the above data and whole-slide tumour images to discover phenotypes and candidate cohorts for more effective molecular stratification (Figures 4-6). Phenotypes represent novel combinations of features across tumour pathology, standard clinical parameters, lifestyle and demographic variables. Molecular stratification within these novel phenotypes seeks to develop new clinical tools for precision oncology."},{"Track":"TransMed","Room":"522","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Panagiotis Nikolaos Lalagkas","Title":"TA-RNN: an Attention-based Time-aware Recurrent Neural Network Architecture for Electronic Health Records","Abstract":"Motivation: Electronic Health Records (EHR) represent a comprehensive resource of a patient's medical history. EHR are essential for utilizing advanced technologies such as deep learning (DL), enabling healthcare providers to analyze extensive data, extract valuable insights, and make precise and data-driven clinical decisions. DL methods such as Recurrent Neural Networks (RNN) have been utilized to analyze EHR to model disease progression and predict diagnosis. However, these methods do not address some inherent irregularities in EHR data such as irregular time intervals between clinical visits. Furthermore, most DL models are not interpretable. In this study, we propose two interpretable DL architectures based on RNN, namely Time-Aware RNN (TA-RNN) and TA-RNN-Autoencoder (TA-RNN-AE) to predict patientâ€™s clinical outcome in EHR at next visit and multiple visits ahead, respectively. To mitigate the impact of irregular time intervals, we propose incorporating time embedding of the elapsed times between visits. For interpretability, we propose employing a dual-level attention mechanism that operates between visits and features within each visit._x000D_\nResults: The results of the experiments conducted on Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) and National Alzheimerâ€™s Coordinating Center (NACC) datasets indicated superior performance of proposed models for predicting Alzheimerâ€™s Disease (AD) compared to state-of-the-art and baseline approaches based on F2 and sensitivity. Additionally, TA-RNN showed superior performance on Medical Information Mart for Intensive Care (MIMIC-III) dataset for mortality prediction. In our ablation study, we observed enhanced predictive performance by incorporating time embedding and attention mechanisms. Finally, investigating attention weights helped identify influential visits and features in predictions."},{"Track":"TransMed","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Emily Hoskins","Title":"Share genetics between breast cancer and its predisposing diseases identifies candidate drugs for repurposing for breast cancer","Abstract":"The success of drugs targeting disease genes is widely acknowledged. However, identifying causal genes for common complex diseases remains a non-trivial task. This necessitates innovative approaches to accelerate complex disease drug discovery. We have previously shown that clinical associations between Mendelian and complex diseases can inform complex disease drug discovery due to pleiotropic effects of Mendelian genes. Here, we extend our approach to exploit clinical associations between pairs of complex diseases for drug discovery. We hypothesize that pleiotropic genes shared between a complex disease and its predisposing diseases can help us discover new uses for drugs currently approved only for the predisposing diseases. To test our hypothesis, we start with breast cancer, a well-studied and highly prevalent disease. We compile a list of six traits known to increase breast cancer risk (predisposing diseases), such as depression, high LDL, and type 2 diabetes. Using GWAS summary statistics and local genetic correlation analysis, we find a total of 84 genomic loci harboring mutations with positively correlated effects between breast cancer and each predisposing disease. These loci contain 202 protein-coding genes (shared genes). Using a network biology approach, for each disease pair, we connect drugs already indicated for the predisposing disease to its shared genes with breast cancer and identify drug repurposing candidates for breast cancer. Finally, we show that our list of candidate drugs is enriched for currently investigated and indicated drugs for breast cancer. Our findings suggest a novel way to accelerate drug discovery for complex diseases by leveraging shared genetics."},{"Track":"TransMed","Room":"519","Weekday":"Monday","Date":"15 July","Timespan":"17:20-17:40","Format":"In Person","Speaker":"Jian Liu","Title":"Prevalence and biological impact of clinically relevant gene fusions in head and neck cancer","Abstract":"Objective: Head and neck cancer (HNC) is the seventh most common cancer worldwide, with a 5-year survival rate of ~50%. The only existing genomic biomarker that guides targeted therapies in HNC is oncogenic HRAS mutations. Gene fusions are clinically targetable, genomic events that involve chromosomal rearrangement, resulting in aberrant function. Here we describe the biological and clinical impact of oncogenic fusions in a combined dataset of HNC. Methods: We evaluated RNA sequencing data from HNCs from the Oncology Research Information Exchange Network (ORIEN, n=1,540), The Cancer Genome Atlas (TCGA, n=528), and other published studies (n=588). We utilized STAR-Fusion and Arriba to detect gene fusions from RNAseq data. Results: Leveraging our combined cohort of 2,666 tumors with RNAseq, we identified 74 cases (2.8%) harboring a clinically relevant gene fusion. The most common fusions involved FGFR3 (N=19), EGFR (n=10), and FGFR2 (n=5). We observed significant gene overexpression in fusion-positive samples with respect to their gene fusion partner (p<0.001). Intrigued by the EGFR fusions that we uncovered, which have not previously been described in head and neck cancers, we further assessed the structure and breakpoints in these fusions. In ORIEN, 4\/5 gene fusions harbored the same breakpoint in EGFR with a gene fusion structure found to be successfully clinically targetable in lung cancer. Conclusions: Our results demonstrate that oncogenic gene fusions are prevalent in HNC, often lead to overexpression of the oncogene fusion partner, and are clinically relevant. Our results provide expanded therapeutic opportunities for patients with HNC."},{"Track":"TransMed","Room":"522","Weekday":"Monday","Date":"15 July","Timespan":"17:40-18:00","Format":"Live Stream","Speaker":"Heidi Rehm","Title":"PhiHER2: Phenotype-informed weakly supervised model for HER2 status prediction from pathological images","Abstract":"Motivation: HER2 status identification enables physicians to assess the prognosis risk and determine the treatment schedule for patients. In clinical practice, pathological slides serve as the gold standard, offering morphological information on cellular structure and tumoral regions. Computational analysis of pathological images has the potential to discover morphological patterns associated with HER2 molecular targets and achieve precise status prediction. However, pathological images are typically equipped with high-resolution attributes, and HER2 expression in breast cancer images often manifests the intratumoral heterogeneity._x000D_\nResults: We present a phenotype-informed weakly-supervised multiple instance learning architecture (PhiHER2) for the prediction of the HER2 status from pathological images of breast cancer. Specifically, a hierarchical prototype clustering module is designed to identify representative phenotypes across whole slide images. These phenotype embeddings are then integrated into a cross-attention module, enhancing feature interaction and aggregation on instances. This yields a prototype-based feature space that leverages the intratumoral morphological heterogeneity for HER2 status prediction. Extensive results demonstrate that PhiHER2 captures a better WSI-level representation by the typical phenotype guidance and significantly outperforms existing methods on real-world datasets. Additionally, interpretability analyses of both phenotypes and WSIs provide explicit insights into the heterogeneity of morphological patterns associated with molecular HER2 status."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"08:40-09:20","Format":"In Person","Speaker":"Irene Mei","Title":"Advancing Genomic Medicine through Clinical and Research Strategies","Abstract":"Supporting genomics in research and medicine requires infrastructure, including standards, knowledgebases and global data sharing, as well as a rich interface between research and clinical care as new discoveries are made. This talk will present strategies to identify novel causes of rare disease including the application of new technologies and analysis methods as well as building innovative approaches to global data sharing in collaboration with AnVIL and the Global Alliance for Genomics and Health. It will end on novel approaches to support genetics and genomics in medical practice."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Javad Rahimikollu","Title":"Transcriptional modulation unique to vulnerable motor neurons predict ALS across species and SOD1 gene mutations","Abstract":"Amyotrophic lateral sclerosis (ALS) is characterized by the progressive loss of somatic motor_x000D_\nneurons (MNs), which innervate skeletal muscles. However, certain MN groups including ocular MNs that regulate eye movement are relatively resilient to ALS. To reveal mechanisms of differential MN vulnerability, we investigate the transcriptional dynamics of two vulnerable and two resilient MN populations in SOD1G93A ALS mice. Differential gene expression analysis shows that each neuron type displays a largely unique spatial and temporal response to ALS. Resilient MNs regulate few genes in response to disease, but show clear divergence in baseline gene expression compared to vulnerable MNs, which in combination may hold the key to their resilience. EASE, fGSEA and ANUBIX enrichment analysis demonstrate that vulnerable MN groups share pathway activation, including regulation of neuronal death, ERK and MAPK cascades, inflammatory response and synaptic signaling. These pathways are largely driven by 11 upregulated genes, including Atf3, Cd44, Gadd45a, Ngfr, Ccl2, Ccl7, Gal, Timp1, Nupr1 and indicate that cell death occurs through similar mechanisms across vulnerable MNs albeit with distinct timing. Random Forest machine learning-based approach using DEGs upregulated in our SOD1G93A spinal MNs predict disease in human stem cell-derived MNs harboring the SOD1E100G mutation, and show that dysregulation of VGF, PENK, INA and NTS are strong disease-predictors across SOD1 mutations and species. A shared transcriptional vulnerability was also assessed through a meta-analysis across mouse SOD1 transcriptome datasets. In conclusion our study reveals vulnerability-specific gene regulation that may act to preserve neurons and can be used to predict disease."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"09:20-09:40","Format":"In Person","Speaker":"Juan Carvajal","Title":"Multi-dimensional Integration of PPI Network with Genetic and Molecular Data to Decipher the Genetic Underpinnings of RA Endotypes","Abstract":"Rheumatoid arthritis (RA) is a complex autoimmune disease with polyetiological genetic basis. Serum rheumatoid factor (RF) and anti-citrullinated peptide (CCP) antibodies are used to diagnose RA. However, it is unknown whether corresponding serological profiles map to distinct endotypes of RA. To address this, we first dissected differences across ~900 RA patients half of whom were serologically CCP+RF+ (i.e., double positive â€“ DP), and half that were RF+ alone (RF). Surprisingly, there was a significant difference in heritability across these groups (~30%), suggesting fundamental differences in genetic risk of these two kinds of RA. Next, we carried out a genome wide association analysis (GWAS) and identified the HLA locus as explaining part of but not the entire difference in heritability between DP and RF RA. To delve into the missing heritability, we implemented a network-based GWAS approach. We adapt Linkage Disequilibrium Adjusted Kinships (LDAK) to aggregate the impact of multiple regulatory SNPs associated with a gene into a single score, taking into account the underlying LD structure. Using network propagation, we then identify modules that explain significant the differences in heritability across DP and RF. These modules include HLA genes, but also capture other cytokines, chemokines and immune regulators and almost completely capture the entire difference in heritability. We were also able to further validate these modules by recapitulating some of the corresponding differences at the transcriptomic and proteomic level. Together, our results suggest that DP and RF RA are different disease endotypes with distinct genetic bases and pathophysiology."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"In Person","Speaker":"Utkarsh Rai","Title":"AI Epilepsy: Software solution to aid in the diagnosis of epilepsy using machine learning algorithms","Abstract":"Epilepsy is a chronic neurological disorder characterized by recurrent seizures, affecting approximately 50 million people worldwide. Different methods have been developed for efficient diagnosis, including prediction of cases requiring surgical intervention due to lack of effectiveness of drug-based treatments (known as refractory epilepsy). These methods include signal processing using electroencephalography (EEG), analysis of structural MRI, and expression of miRNA biomarkers in peripheral blood. Given the heterogeneity of this data, we developed a software solution to perform an integrated analysis of these data types, to aid diagnosis of epilepsy. Users can load the results of the different exams to generate a common report including the results of the different analyses. The analysis includes a machine learning approach for detection of seizures from EEG data. It also includes a classification  model for brain structural anomalies from MRI data. Finally, it includes a classification module based on the expression patterns of blood miRNA data. The software follows a distributed architecture with five main components orchestrated through docker compose. It facilitates the execution of asynchronous processes to run complex predictions by implementing Rabbit message queues. A visualzer of MRI scans was integrated for visualization and interaction with the data obtained from these images. Validation experiments show that the application is efficient and easy to use, taking into account the size and complexity of the data that needs to be analyzed together for epilepsy patients. We expect that this software makes a significant contribution towards the development of new tools and methods for epilepsy research."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"09:40-10:00","Format":"In Person","Speaker":"Quaid Morris","Title":"HTJ2K as a Default Storage Format for Medical Imagesâ€‹","Abstract":"Healthcare systems around the world store large volumes of medical images, like X-rays or scans. The largest public archive currently has 30.9 million radiology images. These images are high quality and use a lot of space making them difficult to store and share.â€‹_x000D_\nImage compression comes with two main challenges, loss in the quality of the image and additional resources needed to compress currently existing images. My project proposes using a recently introduced image format, high-throughput JPEG 2000 (HTJ2K), for lossless compression of these images and bringing their size down by a rough factor of 3.â€‹_x000D_\nThis new format allows you to see a blurry version first which gradually gets clearer. This is very handy when you are dealing with slow internet or huge files. A single image file holds several copies of gradually improving resolutions and medical researchers can pick from any of these, without having to duplicate their datasets.â€‹_x000D_\nMy project provides open-source tools to convert medical images to HTJ2K, methods for users of the images to decode, view and use them as necessary and pipelines that system architects can use to model medical image storage using HTJ2K format such that they are easy to maintain.â€‹"},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"10:40-11:20","Format":"In Person","Speaker":"Mariam Khanfar","Title":"The challenges of clinical deployment of automated cancer type classification for routine use","Abstract":"Accurate cancer type classifiers would have profound impact on the success of cancer treatment. Each year, in the US, more than 30,000 people present with new cancers of unknown primary (CUP), for which treatment options are very limited. Up to half of these patients could be matched with FDA-approved therapies if their cancer type were known. Cancer type classifiers can also distinguish new cancers from reoccurrences and resolve difficult diagnostic challenges. We recently deployed a highly accurate cancer type classifier, GDD-ENS, at Memorial Sloan Kettering Cancer Center (MSKCC) based on inputs derived from an FDA-approved, and routinely applied, targeted DNA sequencing panel called MSK-IMPACT. GDD-ENS, based on ENSembles of multilayer perceptrons, and replaced a pre-existing MSKCC system, GDD-RF. To make GDD-ENS well-suited to the clinical setting, based on lessons learned from GDD-RF, we made specific design choice in the classifier, in its training and evaluation, and how its outputs are integrated with other routinely available clinical data. I will present GDD-ENS, these choices and their impacts, as well as, GDD-ENSâ€™ successes and some areas of improvement. I will also discuss our efforts to generalize GDD-ENS to other targeted cancer gene panels._x000D_\nJoint work with Dr Michael Berger and our labs."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Sasha Blay","Title":"CIViC - an open-access knowledgebase for community driven curation of clinical variants in cancer","Abstract":"In the era of personalized oncology, identifying clinically relevant variants is critical due to the rapidly increasing variant data and need for consensus variant interpretation. The Clinical Interpretation of Variants in Cancer (CIViC-www.civicdb.org) knowledgebase is a free, open-access, open-source, and open-license public resource with an intuitive user interface and flexible public API for programmatic access to all content. _x000D_\n_x000D_\nCIViC supports variant interpretations with six evidence types: Predictive (Therapeutic response), Diagnostic, Prognostic, Predisposing, Oncogenic, and Functional. The model also supports curating Molecular Profiles, which allows users to logically associate one or more variants with evidence. This expansion into \"\"Complex\"\" multi-variant profiles enables the evaluation of clinical significance in contexts such as variant co-occurrence or mutual exclusivity, further enhancing the utility of CIViC in the field of oncology._x000D_\n_x000D_\nAll content in CIViC adheres to a structured data model which follows a published standard operating procedure for curation. This data model incorporates ontologies, standards and guidelines from across the field to promote interoperability and compatibility with other efforts. The CIViC community currently has >350 contributors that have generated >10,000 evidence items from >3,600 sources spanning >390 diseases and >530 therapies._x000D_\n_x000D_\nCIViC's key role in cancer variant interpretation was recognized with its inclusion in the list of 37 Global Core Biodata Resources, underscoring its value to the biological and life sciences community. As CIViC continues to adhere to rigorous standards in maintaining data quality, it remains an invaluable, freely accessible resource, advancing the field of personalized oncology."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"11:20-11:40","Format":"In Person","Speaker":"Jiaqi Li","Title":"Timing the development of chemoresistance in relapsed pediatric cancer","Abstract":"Survivors of pediatric cancer face lifelong battles with severe morbidities, including a significant risk of recurrence. Mutational signatures are patterns of somatic mutations in the cancer genome with specific etiologies. Recent cell line work links mutational signatures to chemotherapy response, signifying chemoresistance. The Shlien lab has identified therapy-associated mutational signatures in the genomes of relapsed pediatric patients, creating an opportunity to characterise when and where the effects of chemotherapy are felt in the pediatric cancer genome. I thus developed a pipeline that combines clonal evolution reconstruction with mutational signature extraction to elucidate changes in mutational processes. I used this pipeline to analyze 1,743 pediatric tumor genomes from 10 pediatric cancer datasets. I detected mutational signatures linked to 4 chemotherapy drugs: temozolomide, platinum-based agents, fluorouracil, and thiopurine. Of 235 samples with confirmed exposure, 37.9% displayed one or more therapy-associated signatures. Mutational signatures associated with alkylating agents like cisplatin were more prevalent and mutationally heavy than those linked to antimetabolites, suggesting the drug mechanism dictates its presentation in the genome. I identified specific subclones with chemotherapy signatures, demarking subclone-level resistance. In cases with multiple tumour samples, resistant subclones in recurrences were traced back to ancestors in the primary diagnostic tumors, suggesting certain lineages possessed the ability to withstand chemotherapy-induced pressures from an early stage and then expanded following treatment. Thus, investigating mutational signatures at the subclonal level unveils new insights into the clonal dynamics of pediatric cancers and the development of chemoresistance."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Hatice Osmanbeyoglu","Title":"PHENO-DEX: Phenotypic Mapping of Dexamethasone Response in Breast Cancer Cells using Single-cell Transcriptomics","Abstract":"Identifying tumor heterogeneity in response to treatment prior to clinical intervention is critical for long-term survival. Weâ€™ve developed an AI-based reference mapping strategy to profile tumor subpopulations in response to perturbations using single-cell transcriptomics. This strategy, known as PHENO-DEX, integrates two major algorithms: DSFMix and PHENOSTAMP. We use DSFMix, based on tree models to identify response\/non-response cell trajectories from a Dex-treated breast cancer cell dataset. Then, using a feed forward loop neural network algorithm, PHENOSTAMP, we next create a Dex-responding reference map, identifying 9 cell states (4 responsive and 5 non-responsive). Each cell state exhibits unique characteristics which correlates with cell plasticity response to Dex. We projected thirty breast cancer cell lines and three clinical breast cancer tumors onto the reference map, effectively revealing their cell state heterogeneity in response to Dex. In summary, weâ€™ve provided a framework to comprehensively characterize both cell lines and clinical samples, which better dissects the responsive states to Dex of tumors prior to any treatment, thereby providing clinical guidance for treatment decisions."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Yamil Damian Mahmoud","Title":"Spatial landscape of malignant pleural and peritoneal mesothelioma tumor immune microenvironment","Abstract":"Immunotherapies have shown modest clinical benefit thus far for malignant mesothelioma (MM). A deeper understanding of immune cell spatial distribution within the tumor immune microenvironment (TIME) is needed to identify interactions between tumor and different immune cell types that might impact the effectiveness of potential immunotherapies. We performed multiplex immunofluorescence (mIF) using tissue microarrays (TMAs, n=3) of samples from patients with malignant peritoneal (n=25) and pleural (n=88) mesothelioma (MPeM and MPM, respectively) to elucidate the spatial distributions of major immune cell populations and their association with LAG3, BAP1, NF2, and MTAP expression, the latter as a proxy for CDKN2A\/B. We also analyzed the relationship between the spatial distribution of major immune cell types with MM patient prognosis and clinical features. The distribution of immune cells within the TIME is similar between MPM and MPeM. However, there is a higher level of interaction between immune cells and tumor cells in MPM than MPeM. Within MPM tumors, there is increased amount of interaction between tumor cells and CD8+ T cells in BAP1-low than in BAP1-high expressing tumors. The cell-cell interactions identified in this investigation have potential implications for the immune response against MM tumors and could be a factor in the different behaviors of MPM and MPeM. Our findings provide a valuable resource for the MM cancer research community and exemplifies the utility of spatial resolution within single-cell analyses. Our mesothelioma spatial atlas mIF dataset is available at _x000D_\nhttps:\/\/mesotheliomaspatialatlas.streamlit.app\/."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Rawan Shraim","Title":"Integrative transcriptomic analysis and predictive modeling for immunotherapy response in melanoma","Abstract":"Despite significant advances in immunotherapies, a substantial subset of melanoma patients remains unresponsive, emphasizing the critical need for predictive biomarkers. Our study integrates transcriptomic analysis and predictive modeling to address this challenge._x000D_\nWe analyzed public single-cell RNASeq (scRNA-Seq) data from 48 melanoma biopsies (16,291 cells) and bulk RNASeq data from 514 patients treated with anti-PD1\/anti-CTLA4. Non-responders exhibited upregulated glycosylation-related genes in macrophages and CD8 T-cells, indicative of compromised immune function. Additionally, macrophages from non-responder biopsies displayed an immunosuppressive profile, coinciding with a treatment-resistant cell sub-group._x000D_\nFurthermore, we integrated scRNA-Seq data from various cancers (totaling 382,019 cells) and developed a signature for immune cell deconvolution in bulk datasets. Responders during treatment showed higher levels of CD8 T-cells, CD4 activated memory T-cells, and total immune infiltrate. Interestingly, responders also displayed increased levels of progenitor and terminally exhausted CD8 T cells compared to non-responders pre- and during treatment, respectively._x000D_\nTo create a robust predictive model of response to immunotherapies in melanoma, we combined the estimated immune cell composition with glycosylation-related genes, our previously published inflammasome pathway signature, and other known indicators, including tertiary lymphoid structures, cytolytic score, and PDL1 expression. Our XGBoost-based machine learning model was trained on the bulk RNA cohort data and achieved an accuracy of 0.79 and AUC of 0.87 with cross-validation._x000D_\nIn conclusion, our findings underscore the potential of integrating transcriptomic analysis and predictive modeling in translational medicine for predicting immunotherapy response in melanoma patients. This emphasizes the critical role of multi-omic approaches in precision medicine for cancer immunotherapy."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Shi Pan","Title":"A computational approach for the high-throughput identification of cancer-specific antigens for immunotherapeutic development","Abstract":"Cancer remains a major global health challenge, with current treatments such as chemotherapy and radiotherapy often limited by toxicity and late effects. This has prompted the development of targeted immunotherapies. An obstacle to the development of these therapies is the identification of cancer-specific antigens as therapeutic targets. _x000D_\n_x000D_\nTo address this challenge computationally, we developed a tool that prioritizes potential immunotherapeutic targets by integrating multi-source data, including user-supplied cancer expression data (e.g., proteomics or RNA-sequencing) and quantitative features from various databases selected to address a predefined criteria for ideal immunotherapeutic targets. Our tool can adjust for normalization, missing values, and applies feature weighting, producing a gene-specific score that reflects its suitability as a therapeutic target. We evaluated our toolâ€™s performance using mean-average-precision (MAP) score, which assesses the prioritization rankings of known therapeutic targets within the cancer phenotype. Utilizing twelve pediatric cancer cell line proteomics datasets for validation of our methodology, we generated optimized parameters leading to a 27-fold increase (p < 0.001) in the MAP score, highlighting our toolsâ€™ target prioritization capabilities. Using the generated optimized parameters, our tool was able to score known chimeric antigen receptor T-cell targets such as CD19, CD22, CD79b in the top 10 targets in B-cell non-Hodgkinâ€™s lymphoma, validating our methodology. Additionally, HLA-G was identified as a novel potential target across pediatric cancer phenotypes surveyed in the analysis.  _x000D_\n_x000D_\nWe have developed a tool to efficiently identify immunotherapeutic targets that can be used to accelerate the development of safer and more effective cancer immunotherapies."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Mozhgan Saeidi","Title":"Leveraging a Single-Cell Language Model for Precise EMT Status Prediction and Gene Signature Identification in Cancer","Abstract":"The epithelial-to-mesenchymal transition (EMT) is pivotal in tumour progression and resistance to treatment, yet its heterogeneity complicates the precise assessment of EMT status of individual tumour cells. While key epithelial and mesenchymal genes driving the transformation are well characterised, other regulators, especially at intermediate stages of the process, are less well understood._x000D_\nBy leveraging a pre-trained single-cell language model, we develop a generalisable classifier named EMT-language model (EMT-LM) to predict multiple  states within the EMT continuum at single cell resolution. Our training data use an RNA-seq dataset from Cook et al [1], which profile single cells from 0 hours to 7 days during EMT. EMT-LM demonstrates an average prediction accuracy of EMT state of 90% AUROC across various cancers. Our Attention-Driven Expression Significance Index (ADESI) combines attention scores from EMT-LM and the gene expression, to uncover genes that are critical in regulating the entire timeline of EMT. Our top regulators include genes involved in mitochondrial function (e.g., NDUFB10, MRPL51) and oxidative stress response (e.g., PRDX1) suggesting  a metabolic reprogramming during EMT. And patients exhibiting the 8h and 3d EMT signatures, as identified by genes with high attention scores in these categories, showed a notable decrease in survival rates in the METABRIC dataset.  _x000D_\nIn conclusion, EMT-LM exemplifies the effective application of language models in cancer biology research, offering a novel approach to EMT status prediction and identifying clinically relevant gene signatures reflecting the plasticity of the EMT programme."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"14:20-14:40","Format":"In Person","Speaker":"Roberto Bonelli","Title":"Streamlining Clinical Trial Matching Using a Two-Stage Zero-Shot LLM with Advanced Prompting","Abstract":"Identifying patients eligible for clinical trials is a critical bottleneck hindering medical research progress because many clinical trials allow only small, specific patient cohorts to be included in the clinical trial and require a certain number of participating patients to yield definitive results. Manually screening patients through unstructured medical records is time-consuming and expensive. This paper explores the potential of large language models (LLMs) enhanced with medical context to automate patient eligibility assessment for clinical trials. We first designed a two-stage zero-shot LLM approach to analyze a patientâ€™s medical history (presented as unstructured text) to determine their eligibility for a given trial. We use advanced prompting strategies to guide the LLM toward faster and more targeted assessments. Additionally, a two-stage retrieval pipeline pre-filters potential trials using efficient retrieval techniques, reducing the number of trials considered by the LLM. This substantially improves processing speed and efficiency. Our method holds promise for streamlining clinical trial patient matching."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Rachel Melamed","Title":"Multi-Omics Integration with High-Resolution AI-Derived Retinal Thickness: Unraveling Spatial Patterns of Retinal Susceptibility to Systemic Influences","Abstract":"Retinal thickness is a marker of retinal health and more broadly, a promising biomarker for many systemic diseases. We processed the UK Biobank retinal OCT images using a convolutional neural network on more than 40,000 individuals to produce fine-scale retinal thickness measurements on >29,000 points in the macula, the part of the retina responsible for human central vision. We then performed a multi-omics analysis and tested the association of common genomic variants, metabolomic, blood and immune biomarkers, ICD10 codes and polygenic risk scores with each of the fine-scale macular thickness points. Our analysis reveals high-resolution spatial retinal thickness association with hundreds of genetic loci, metabolites with spatially clustered effects, systemic disorders such as multiple sclerosis affecting specific areas as well as blood biomarkers such as reticulocyte count correlating with strong retinal thinning. Using enrichment analysis, we highlight that the parafoveal region of the macula is particularly susceptible to systemic insults and that metabolic correlations with its thickness magnify with age. Together, these results demonstrate not only the exquisite susceptibility of the retina to molecular and phenotypic changes but also the gains in spatial discovery power and resolution achievable by integrating multi-omics datasets with AI-generated data. All our results are accessible through a bespoke web interface."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":null,"Title":"New methods to discover drug combinations impacting cancer incidence","Abstract":"In this work we seek to mine health claims data to find combinations of drugs that may alter onset of cancer. This work has an ultimate goal of preventing cancer related to medical treatment, and of suggesting new treatments for the disease. Because drug combinations impacting cancer are unlikely to be discovered using randomized trials, we develop new methods using observational data to discover these effects. Our novel method based on the marginal structural model, but also includes a number of evaluations to identify robust signals."},{"Track":"TransMed","Room":"519","Weekday":"Tuesday","Date":"16 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Danielle Gutman","Title":"Closing Remarks","Abstract":null},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":null,"Title":"Exploring the landscape of regulatory uORFs in BMPR2  and their potential as therapeutic targets","Abstract":"About 50% of human genes harbor upstream open reading frames (uORFs), with a start codon in the 5â€™ untranslated region (5â€™UTR) occurring before the coding sequence (CDS) start, and an in-frame stop codon occurring before or after the CDS start. Previous work demonstrated functional roles for uORFs in regulating their CDS expression, specifically when altering uORF start\/stop codons. To detect functional uORFs, we created a database that combines experimentally detected uORFs and transcriptome based uORF predictions. As a test-case for our selection pipeline, the bone morphogenetic protein receptor type II (BMPR2) gene, is strongly associated with Pulmonary Arterial Hypertension (PAH). The 5â€™UTR of BMPR2 harbors over 30 uORFs, making it an excellent test case for functional validation. We cloned 921bp of the BMPR2 5â€™UTR into a novel bi-cistronic dual luciferase reporter vector, mutated start\/stop codons of selected uORFs, and demonstrated significant down- and up-regulation of reporter activity by a subset of regulatory variants compared to WT. We designed ASOs targeting two variant-identified regions of the WT 5â€™UTR and used those to treat a BMP-responsive reporter cell line. Both ASOs showed significant up-regulation of the BMP pathway compared to control. We are currently testing the ASOs in PAH patient-derived cells, and WT mice, and exploring additional uORFs in multiple genes implicated in different diseases. In conclusion, we generated an effective pipeline to assess uORFsâ€™ function and targetability. We aim to exploit this pipeline to enrich our understanding of 5â€™UTR variation and ultimately nominate potentially actionable therapeutic targets."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"10:40-11:00","Format":"In Person","Speaker":"Jun Cheng","Title":"Opening Remarks","Abstract":null},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"11:00-11:40","Format":"In Person","Speaker":"Wim Vranken","Title":"Beyond the sequence: interpreting missense variants with structure context","Abstract":"The vast majority of missense variants observed in the human genome are of unknown clinical significance. Machine learning approaches could close this variant interpretation gap by exploiting patterns in biological data to predict the pathogenicity of unannotated variants. I will discuss AlphaMissense, which combines advances of the highly-accurate structure prediction model, AlphaFold, and population variant data to predict missense variant pathogenicity. We demonstrate state-of-the-art predictions on clinically-ascertained labels and experimental benchmarks, without explicitly training on such data. Due to higher predictive performance, the fraction of ClinVar test variants that we can confidently classify with 90% precision has increased by 25.8 percentage points (from 67.1% to 92.9%) compared to the recent well-performing unsupervised model EVE. I will also cover aspects of model evaluation, interpretation and utility. For instance, we find that gene level AlphaMissense scores are predictive of genes essential to cell survival, and this property holds amongst the 22% of smaller genes, which methods based only on population cohort data lack statistical power to detect reliably."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"11:40-12:00","Format":"In Person","Speaker":"Burkhard Rost","Title":"Capturing biophysical and protein language model constraints for an improved assessment of the impact of mutations on protein function and stability","Abstract":"Our understanding of how proteins operate and how evolution shapes them is mainly based on their overall fold in relation to their amino acid sequence. The direct relation between these is now largely solved by methods such as AlphaFold2. However, capturing the subtle effect of mutations on protein behavior, especially in dynamic and structurally ambiguous regions, remains difficult. Protein language models (pLM) only require sequence information and are able to capture common patterns across protein families. pLMs, however, lack protein-specific nuances required for mutational analysis while producing an enormous amount of difficult to interpret features for each sequence position in a protein. We here introduce the D2D model, which captures protein-specific constraints on the pLM features through the use of evolutionary data in combination with a Gaussian mixture model (GMM). This combination can, out of the box, perform a diverse set of tasks such as single and multiple mutation pathogenicity, protein thermostability and the effect of mutations on binding. When further fine-tuned by supervised training, the D2D model significantly outperforms state-of-the art predictors, such as in the context of passenger and driver mutations in cancer. For interpretation of the effect of these mutations, we can similarly define â€˜biophysical constraintsâ€™ based on our suite of predictors of biophysical features of proteins, such as backbone dynamics or early folding regions. The combination of these approaches so provides high accuracy predictions for a variety of problems while still enabling physical interpretability, also for protein regions that do not have a single well-defined fold."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Oriol Gracia I Carmona","Title":"VespaG: Expert-guided protein Language Models enable accurate and blazingly fast fitness prediction","Abstract":"Exhaustively annotating the experimental effect of all known protein variants upon molecular protein function remains daunting and expensive. In response, we present VespaG, an effect prediction utilizing expert-guided protein Language Models (pLMs) to couple high accuracy with unprecedented speed in fitness prediction. We addressed the scarcity of experimental data, by â€œcuratingâ€ a dataset comprising 39 million Single Amino Acid Variants (SAVs) from Homo sapiens by explicitly modeling the evolutionary history of natural sequences based on multiple sequence alignments with the state-of-the-art effect prediction method, GEMME. This established a large training set._x000D_\n_x000D_\nVespaG is a minimalist deep learning model directly mapping pLM embeddings to comprehensive SAV mutational landscapes. Evaluation against the ProteinGym Substitution Benchmark with 2.5 million SAVs from 217 multiplex assays of variant effect (MAVE) demonstrated VespaG's efficacy (mean Spearman correlation 0.495Â±0.04, 95% confidence interval). This resembled methods such as VespaG's teacher GEMME, VESPA, and the much more sophisticated TranceptEVE and AlphaMissense. VespaG reached its top-level performance several orders of magnitude faster, predicting entire mutational landscapes for 20,000 proteins in under an hour on a desktop 32-core CPU, unlocking new possibilities for rapid variant assessment. VespaG performed much better for eukaryotes and prokaryotes than for viruses within ProteinGym (mean Spearman 0.508Â±0.02 vs. 0.414Â±0.02 for viral), underlining peculiarities of viral evolution and\/or potential limitations of current pLMs._x000D_\n_x000D_\nVespaG is available freely at https:\/\/github.com\/JSchlensok\/VespaG"},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"12:00-12:20","Format":"In Person","Speaker":"Heidi Rehm","Title":"Addressing biases in large language models for variant impact prediction in macro proteins","Abstract":"Large multi-domain proteins pose significant challenges for both experimental studies and computational modelling due to their extensive size, often spanning thousands of amino acids. While Large Language Models (LLMs) offer promise in studying variant effects, their token limit falls short of accommodating these macro proteins. One solution consists in dividing proteins into domain groups. However, since LLMs were trained on complete proteins, biases may arise when analysing protein fragments. _x000D_\nOur study delves deeper into these biases by assessing the performance of ESM2 across a diverse set of proteins containing prevalent domains found in macro proteins. Predictions performed on domain sequences displayed significantly lower confidence than those obtained using whole sequences as context, particularly near the N-terminal end positions. _x000D_\nTo mitigate this bias, we fine-tuned ESM2 using a dataset of domain sequences. The resulting model exhibited no significant differences between full-length and domain sequences, yielding a narrower distribution of discrepancies compared to the original ESM2. Moreover, the final predictions from the fine-tuned model aligned closely with those of ESM2 using the whole sequence as context, indicating successful bias reduction without compromising prediction quality. Furthermore, we evaluated the fine-tuned model's efficacy in testing variant effects on Titin, a macro protein with over 35,000 amino acids. Our model outperformed both the original ESM2 and other state-of-the-art variant effect prediction methods across all tested metrics. _x000D_\nIn summary, our findings highlight the effectiveness of fine-tuning LLMs on domain sequences to alleviate biases and improve accuracy in studying variant effects on large multi-domain proteins like Titin."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"14:20-15:00","Format":"In Person","Speaker":null,"Title":"Clinical classification of variation for disease causality","Abstract":null},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Dina Issakova","Title":"Ensemble Prediction of the Clinical Impact of Missense Variants Substantially Decreases VUS Rate in Genetic Testing","Abstract":null},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"15:00-15:20","Format":"In Person","Speaker":"Kivilcim Ozturk","Title":"MAJIQ-CLIN: A novel tool for the identification of Mendelian disease-causing variants from RNA-seq data","Abstract":"Exome sequencing (ES) is the current standard of care for patients with suspected Mendelian genetic disorders. However, the diagnostic rate is only 25 to 58%. One key regulatory process of gene expression that is not captured well by ES is RNA splicing. Changes in splicing, or alternative splicing, naturally occur in up to 95% of human genes, but 38-50% of human pathogenic variants are estimated to alter RNA splicing, including notable Mendelian disorders. It is therefore crucial to develop reliable tools to detect splicing aberrations from patient RNA-seq to improve current diagnostic rates. For a tool to be considered reliable for detecting splicing aberrations from RNA-Seq, a tool should reliably detect splicing aberrations in previously solved cases, be easy to use, and use resources realistic for a clinical setting. In recent years a few tools were developed to address this need, specifically LeafCutterMD and FRASER. While both served as good proof of concepts for enhancing clinical diagnostics using RNA-Seq, our analysis indicates that several challenges remain. To address these, we developed MAJIQ-CLIN, a pipeline for detecting splicing aberrations in a patientâ€™s RNA-Seq sample compared to a large cohort of controls. We evaluate existing tools compared to MAJIQ-CLIN using both synthetic data with spiked in splice variations as well as several datasets of solved test cases, demonstrating it compares favorably to both LeafCutterMD and FRASER with significant improvements in time, memory, usability, and accuracy. We hope to establish MAJIQ-CLIN as a tool routinely used in clinical practice to improve patient outcomes."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"15:20-15:40","Format":"In Person","Speaker":"Thomas Renne","Title":"Reclassifying variants of uncertain significance with transcriptional profiling","Abstract":"Understanding the functional impact of single amino acid substitutions in cancer driver genes remains an unmet need. Perturb-seq provides a tool to investigate the effects of individual mutations on cellular programs by measuring their transcriptional consequences. Here, we develop an approach to functionally assess variant impact in single cells. We deploy ScalablE fUnctional Screening by Sequencing (SEUSS), a Perturb-seq style technique, to generate and assay mutations of the Runt-related transcription factor 1 (RUNX1). We measured the impact of 115 mutations on RNA profiles in single myelogenous leukemia cells and used the profiles to categorize mutations into three functionally distinct groups: wild-type-like, loss-of-function-like and hypomorphic, that were validated in orthogonal assays. Using these profiles, we identified the functional impact of 16 documented variants of uncertain significance. Next, we trained a Random Forest classifier with our variant library serving as the training data and predicted functional effects of all remaining RUNX1 mutations (n=2582), resulting in predictions for a further 103 variants of uncertain significance and achieving an auROC of 0.79 and an auPR of 0.82 on a gold standard dataset. Overall, our work demonstrates the power of transcriptional profiling in single cells to assess the functional impact of missense mutations on cellular programs and provides a scalable method for coding variant impact phenotyping."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"15:40-16:00","Format":"In Person","Speaker":"Patrick Wall","Title":"Metacell burden: A method to quantify the effects on neurodevelopmental disorders of rare genomic variants aggregated across brain cells.","Abstract":"Neurodevelopmental disorders (NDs), such as autism or intellectual disabilities, affect up to 5% of the population. While the high heritability rate of these disorders, genetic contribution remains largely unidentified. WES has become pivotal in identifying structural variants and SNVs associated with NDs. However, their functional implications remain undetermined. Recent scRNAseq datasets give new insight in the relations between cell-types and NDs. Still, the majority of variants contributing to NDs are too rare and understudied, so their impact and the biological functions underlying remain unknown._x000D_\n_x000D_\nOur study introduces a novel metacell burden analysis to investigate the association between rare variants carried by 500k individuals, their phenotypes, and the brain cell-types underlying. We aggregate genes linked to biological functions (e.g. cell types or metacells) based on the expression extracted from large-scale single-cell RNA-seq data. The burden of each gene-set on a phenotype is then computed through a linear regression of the sum of metacellâ€™s genes disrupted by a variant._x000D_\n_x000D_\nWe identified that rare LoF variants disrupting genes associated to excitatory and inhibitory neurons are associated with cognitive ability. The analysis of structural variants highlighted other cell-types such as microglia, showing that cognitive ability results of multiple pathways. Some phenotypes have correlated effect sizes for specific cell types, while others donâ€™t suggest that some traits share developmental pathways. These findings, consistent with literature hypothesizes, give insight into the relation between rare variants, neurodevelopmental traits and cell profiles. This new approach offers a promising opportunity for further exploration of cell types and phenotypes."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"16:40-17:00","Format":"In Person","Speaker":"Marina Abakarova","Title":"Representing Mutations for Predicting Cancer Drug Response","Abstract":"Motivation. Predicting cancer drug response requires a comprehensive assessment of many mutations present across a tumor genome. While current drug response models generally use a binary mutated\/unmutated indicator for each gene, not all mutations in a gene are equivalent. _x000D_\n_x000D_\nResults. Here, we construct and evaluate a series of predictive models based on leading methods for quantitative mutation scoring. These methods include VEST4 and CADD, which score the likely impact of a mutation on normal gene function, and CHASMplus, which scores the likelihood the mutation drives cancer. These models capture cellular responses to dabrafenib, which specifically targets BRAF V600 mutations, whereas models based on binary mutation status do not. These performance improvements generalize to other drug responses, extending genetic indications for PIK3CA, ERBB2, EGFR, PARP1, and ABL1 inhibitors. _x000D_\n_x000D_\nConclusion. Introducing quantitative mutation features in drug response models increases predictive performance and mechanistic understanding. _x000D_\n_x000D_\nAvailability. Source code and a sample input dataset are available at https:\/\/github.com\/pgwall\/qms."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":"Thibault Latrille","Title":"Assessing lethal missense mutations and polymorphism in Drosophila melanogaster with an evolutionary-informed model","Abstract":"This study investigates the impact of missense mutations on the Drosophila melanogaster proteome and contributes to our understanding of the genotype-phenotype relationship, with implications for targeted protein editing. We applied a computationally efficient approach we developed previously to predict the impact of all possible mutations in the fly proteome. It leverages fast protein sequence search and alignment with evolutionary-informed mutational effect predictions. Leveraging resources such as FlyBase and the Drosophila Genetic Reference Panel (DGRP), we assessed the discriminative power of the predictions and investigated the interplay between polymorphism, including isoforms, evolutionary conservation, and pathogenicity at the organismal level. The approach accurately distinguishes benign from pathogenic mutations, achieving a balanced accuracy of 0.856. Beyond predictive capability, we found that invariant genes in the DGRP population demonstrate a greater variability across the kingdom of Life. Specifically, non-polymorphic and lethality-induced genes present 3.8-fold enrichment in the high fraction of observed substitutions in the protein homologs(>85%). Additionally, we showcase the importance of the context for variant effect prediction on the proteoforms of Mef2, a muscle-specific transcription factor.Â _x000D_\nWe provide the community with full variant effect predictions for the entire fly proteome, accessibleÂ at https:\/\/doi.org\/10.5281\/zenodo.10995110. Since the approach relies on the quality of the input MSA, we provide both global and local confidence metrics to guide users."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"17:00-17:20","Format":"In Person","Speaker":null,"Title":"A phylogenetic mutation-selection model predicts fitness effects of mutations in extant mammals","Abstract":"At the phylogenetic scale, sequence variation informs us on the selective effects of mutations. Indeed, mutations can be either beneficial, deleterious or neutral for their bearer, influencing the likelihood for a mutation to reach fixation. In this study, we first estimated the selective effects of point mutations inside mammalian protein coding sequences, assuming a nearly-neutral model of evolution at the phylogenetic scale. Confronting phylogenetic and population genomics dataset, we then confirmed that mutations predicted to be deleterious from the phylogenetic analysis are currently purified away in extant populations. Conversely, mutations predicted to repair previous deleterious changes are indeed shown to be beneficial in extant populations. This study confirms that deleterious substitutions are accumulating in mammals and are being reverted, generating a balance in which genomes are damaged and restored simultaneously at different loci. At the interface between population genomics and phylogenetic analysis, our work supports a nearly-neutral model of evolution at the phylogenetic scale, informing us on the effect of point mutations for extant populations and individuals. We observe that in 24 out of 28 populations analyzed, between 15% and 45% of all beneficial mutations that are currently segregating in the population are not due to an environmental change. Thus a substantial part of ongoing positive selection is not driven solely by adaptation to environmental change in mammals. Finally, we show that we can also use this nearly-neutral model of evolution as a null model above which we can detect adaptation in protein-coding DNA sequences."},{"Track":"VarI","Room":"521","Weekday":"Monday","Date":"15 July","Timespan":"17:20-18:00","Format":"In Person","Speaker":null,"Title":"Functional variomics for decoding variant effects","Abstract":null},{"Track":"WEB 2024: Experiential Learning on How to Implement AI in Bioinformatics Training","Room":"525","Weekday":"Monday","Date":"15 July","Timespan":"10:40-12:20","Format":"In Person","Speaker":null,"Title":"WEB 2024: Experiential Learning on How to Implement AI in Bioinformatics Training","Abstract":null},{"Track":"WEB 2024: Experiential Learning on How to Implement AI in Bioinformatics Training","Room":"525","Weekday":"Monday","Date":"15 July","Timespan":"14:20-16:00","Format":"In Person","Speaker":null,"Title":"WEB 2024: Experiential Learning on How to Implement AI in Bioinformatics Training","Abstract":null}]